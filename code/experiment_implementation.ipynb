{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjvGUcAmDCEA",
        "outputId": "7e69456b-4d75-4180-8ba0-b6ff46c08da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "ZU-W4aOuDCEC",
        "outputId": "fdedadc6-fb0b-4fec-e1d6-f1c81b9c1636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Cornell/CS5782\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 765 bytes | 23.00 KiB/s, done.\n",
            "From https://github.com/joshiarnav/5782_Final_Project\n",
            "   2bbc3f2..ed7f059  main       -> origin/main\n",
            "Updating 2bbc3f2..ed7f059\n",
            "Fast-forward\n",
            " code/model.py | 30 \u001b[32m+++++++++++++++++++++\u001b[m\u001b[31m---------\u001b[m\n",
            " 1 file changed, 21 insertions(+), 9 deletions(-)\n",
            "/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Cornell/CS5782\n",
        "!if [ ! -d \"5782_Final_Project\" ]; then git clone https://github.com/joshiarnav/5782_Final_Project.git && cd 5782_Final_Project; else cd \"5782_Final_Project\" && git pull; fi\n",
        "%cd 5782_Final_Project/code\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "U2pqN-fQDCEC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "# !pip install scipy  # Make sure scipy is installed for confidence intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8tVv4ZEXDCEC"
      },
      "outputs": [],
      "source": [
        "# Run a small test experiment (optional)\n",
        "# !python experiment_runner.py \\\n",
        "#     --output_dir=./experiment_results_test \\\n",
        "#     --orthographies 10ebased decimal \\\n",
        "#     --digit_lengths 2 5 \\\n",
        "#     --train_size=1000 \\\n",
        "#     --max_epochs=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Vdfk4Z2gDCEC"
      },
      "outputs": [],
      "source": [
        "# !python experiment_runner.py --output_dir=./experiment_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generalization_runner.py --output_dir=./generalized_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ0nd9tXuUdy",
        "outputId": "70984825-33cb-4515-cfc0-a50038cb8276"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: 10ebased\n",
            "================================================================================\n",
            "\n",
            "Experiment already completed for 10ebased, 30 digits, seed 42. Skipping training.\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Already tested 10ebased on 2 digits with seed 42\n",
            "Already tested 10ebased on 5 digits with seed 42\n",
            "Already tested 10ebased on 10 digits with seed 42\n",
            "Already tested 10ebased on 15 digits with seed 42\n",
            "Already tested 10ebased on 20 digits with seed 42\n",
            "Already tested 10ebased on 25 digits with seed 42\n",
            "Already tested 10ebased on 30 digits with seed 42\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: 10based\n",
            "================================================================================\n",
            "\n",
            "Experiment already completed for 10based, 30 digits, seed 42. Skipping training.\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Already tested 10based on 2 digits with seed 42\n",
            "Already tested 10based on 5 digits with seed 42\n",
            "Already tested 10based on 10 digits with seed 42\n",
            "Already tested 10based on 15 digits with seed 42\n",
            "Already tested 10based on 20 digits with seed 42\n",
            "Already tested 10based on 25 digits with seed 42\n",
            "Already tested 10based on 30 digits with seed 42\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: words\n",
            "================================================================================\n",
            "\n",
            "Experiment already completed for words, 30 digits, seed 42. Skipping training.\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Already tested words on 2 digits with seed 42\n",
            "Already tested words on 5 digits with seed 42\n",
            "Already tested words on 10 digits with seed 42\n",
            "Already tested words on 15 digits with seed 42\n",
            "Already tested words on 20 digits with seed 42\n",
            "Already tested words on 25 digits with seed 42\n",
            "Already tested words on 30 digits with seed 42\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: underscore\n",
            "================================================================================\n",
            "\n",
            "Experiment already completed for underscore, 30 digits, seed 42. Skipping training.\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Already tested underscore on 2 digits with seed 42\n",
            "Already tested underscore on 5 digits with seed 42\n",
            "Already tested underscore on 10 digits with seed 42\n",
            "Already tested underscore on 15 digits with seed 42\n",
            "Already tested underscore on 20 digits with seed 42\n",
            "Already tested underscore on 25 digits with seed 42\n",
            "Already tested underscore on 30 digits with seed 42\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: character_fixed\n",
            "================================================================================\n",
            "\n",
            "Experiment already completed for character_fixed, 30 digits, seed 42. Skipping training.\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Already tested character_fixed on 2 digits with seed 42\n",
            "Already tested character_fixed on 5 digits with seed 42\n",
            "Already tested character_fixed on 10 digits with seed 42\n",
            "Already tested character_fixed on 15 digits with seed 42\n",
            "Already tested character_fixed on 20 digits with seed 42\n",
            "Already tested character_fixed on 25 digits with seed 42\n",
            "Already tested character_fixed on 30 digits with seed 42\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: character\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Training model: character, 30 digits, seed 42\n",
            "================================================================================\n",
            "\n",
            "Configuration: Namespace(operation='addition', model_name_or_path='t5-base', train_size=10000, val_size=1000, test_size=1000, min_digits_train=2, min_digits_test=30, base_number=10, train_batch_size=128, val_batch_size=256, max_seq_length=512, num_workers=4, max_epochs=25, check_val_every_n_epoch=10, precision=32, gradient_clip_val=1.0, accumulate_grad_batches=4, optimizer='AdamW', lr=0.0004, weight_decay=5e-05, scheduler='StepLR', gamma=1.0, step_size=1000, balance_train=True, balance_val=True, balance_test=False, invert_question=False, invert_answer=False, orthography='character', max_digits_train=30, max_digits_test=30, seed=42, output_dir='./generalized_results/character_trained_on_30_digits_seed42')\n",
            "Configuration: Namespace(operation='addition', model_name_or_path='t5-base', train_size=10000, val_size=1000, test_size=1000, min_digits_train=2, min_digits_test=30, base_number=10, train_batch_size=128, val_batch_size=256, max_seq_length=512, num_workers=4, max_epochs=25, check_val_every_n_epoch=10, precision=32, gradient_clip_val=1.0, accumulate_grad_batches=4, optimizer='AdamW', lr=0.0004, weight_decay=5e-05, scheduler='StepLR', gamma=1.0, step_size=1000, balance_train=True, balance_val=True, balance_test=False, invert_question=False, invert_answer=False, orthography='character', max_digits_train=30, max_digits_test=30, seed=42, output_dir='./generalized_results/character_trained_on_30_digits_seed42')\n",
            "Seed set to 42\n",
            "/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/gpu_optimizations.py:48: UserWarning: Enabled Tensor Core optimizations for NVIDIA A100-SXM4-40GB\n",
            "  warnings.warn(f\"Enabled Tensor Core optimizations for {device_name}\")\n",
            "/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/gpu_optimizations.py:71: UserWarning: Configured TF32 settings for NVIDIA A100-SXM4-40GB (keeping 32-bit precision for stability)\n",
            "  warnings.warn(f\"Configured TF32 settings for {device_name} (keeping 32-bit precision for stability)\")\n",
            "Applied GPU optimization settings\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Creating trainer...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Creating model...\n",
            "2025-05-06 08:08:58.276493: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-06 08:08:58.293800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746518938.315988   41420 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746518938.322771   41420 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-06 08:08:58.345125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting training...\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42 exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                       | Params | Mode \n",
            "-------------------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 222 M  | train\n",
            "-------------------------------------------------------------\n",
            "222 M     Trainable params\n",
            "0         Non-trainable params\n",
            "222 M     Total params\n",
            "891.614   Total estimated model params size (MB)\n",
            "541       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 5 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 0 7 5 2 9 2 4 0 0 1 1 9 6 0 0 5 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 6 0 0 5 4 5 9 3 5 3 3 0 8 5 9 8 3 5 3 3 0 8 5 9 8 3 7 2 7 plus\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:05<00:00,  2.64s/it]\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "Epoch 0:   0% 0/79 [00:00<?, ?it/s] \n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 1 6 1 3 6 5 7 2 7 0 6 0 0 6 4 9 plus 6 9 1 9 7 7 4 3 9 6 5 5 8 2 6 1 8?\n",
            "Sample answer: 1 1 0 8 1 1 4 0 1 2 3 6 1 8 3 2 6 7\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 4 7 8 4 9 7 2 2 7 8 plus 9 6 3 1 7 4 1 2 5 3?\n",
            "Sample answer: 1 4 4 1 6 7 1 3 5 3 1\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 1 1 1 2 2 4 4 7 0 9 4 6 5 2 2 0 plus 4 2 4 9 6 6 2 5 9 6 4 1 9 3 5 7 9?\n",
            "Sample answer: 1 0 3 6 0 8 8 7 0 6 7 3 6 5 8 7 9 9\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 0 9 plus 4 9 7?\n",
            "Sample answer: 6 0 6\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 2 4 2 6 0 7 0 9 0 5 4 1 1 4 2 9 0 6 8 6 4 2 1 2 plus 5 9 3 2 8 3 8 4 4 3 3 3 1 4 4 8 8 9 3 5 5 1 8 7 4?\n",
            "Sample answer: 7 1 7 5 4 4 5 5 3 3 8 7 2 5 9 1 8 0 0 4 1 6 0 8 6\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 2 4 0 8 3 0 9 8 4 2 7 8 4 5 2 4 2 5 3 7 6 3 plus 2 5 1 5 9 7 4 7 9 1 3 6 2 6 0 2 5 1 3 3 3 4?\n",
            "Sample answer: 4 9 2 4 2 8 4 6 3 4 1 4 7 1 2 6 7 6 7 0 9 7\n",
            "Epoch 0:  25% 20/79 [00:05<00:15,  3.88it/s, v_num=2, train_loss=1.530]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 1 1 2 5 4 4 3 9 0 0 2 6 1 5 3 5 3 6 3 4 7 8 9 5 3 8 plus 4 0 4 1 2 3 2 0 6 1 3 6 3 2 0 5 5 2 9 5 3 1 7 7 6 4?\n",
            "Sample answer: 5 1 6 6 6 7 5 9 6 1 6 2 4 7 4 0 8 9 3 0 1 0 7 3 0 2\n",
            "Epoch 0:  76% 60/79 [00:14<00:04,  4.23it/s, v_num=2, train_loss=1.170]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 4 7 5 9 0 8 6 9 0 7 3 7 7 5 7 0 8 0 0 8 1 8 0 7 plus 1 0 5 0 6 2 2 2 7 1 5 4 9 4 7 5 1 8 2 3 2 5 6 4?\n",
            "Sample answer: 5 8 0 9 7 0 9 1 7 8 9 2 7 0 4 5 9 8 3 1 4 3 7 1\n",
            "Epoch 0: 100% 79/79 [00:18<00:00,  4.30it/s, v_num=2, train_loss=1.130]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:07<00:00,  1.80s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 0: 100% 79/79 [00:25<00:00,  3.09it/s, v_num=2, train_loss=1.130, val_exact_match=0.000]Epoch 0, global step 20: 'val_exact_match' reached 0.00000 (best 0.00000), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt' as top 1\n",
            "Epoch 1:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.130, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 7 7 2 9 5 7 7 9 0 0 1 3 1 1 5 0 3 5 8 2 plus 6 0 3 8 4 7 7 5 5 1 8 0 5 5 7 4 1 6 8 8 8?\n",
            "Sample answer: 1 1 8 1 1 4 3 5 3 4 1 8 1 8 6 8 9 2 0 4 7 0\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 9 9 7 4 2 0 9 6 5 5 2 6 1 3 7 3 7 9 3 1 4 0 5 0 4 4 4 plus 7 9 0 7 4 2 2 4 0 7 5 7 2 4 6 6 3 6 7 0 5 9 3 4 3 9 9 6?\n",
            "Sample answer: 1 5 9 0 4 8 4 3 3 7 3 0 9 8 6 0 3 7 4 6 3 7 3 3 9 4 4 4 0\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 4 4 2 8 4 6 8 7 5 4 5 5 6 2 5 6 2 1 6 5 7 7 1 5 4 5 9 plus 9 1 9 1 3 0 2 4 1 8 5 8 9 9 2 5 5 9 4 6 4 7 4 5 9 9 6 0?\n",
            "Sample answer: 1 0 6 3 4 1 4 9 2 9 4 0 4 5 5 5 1 2 1 6 3 0 5 1 7 5 4 1 9\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 7 9 1 7 plus 4 9 2 4?\n",
            "Sample answer: 1 2 8 4 1\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 4 9 9 9 7 7 3 8 5 9 0 7 3 0 1 3 9 2 0 4 plus 4 5 4 9 1 1 2 5 5 5 6 4 3 5 3 4 6 2 7 9?\n",
            "Sample answer: 9 5 4 8 8 8 6 4 1 4 7 1 6 5 4 8 5 4 8 3\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 8 5 4 0 8 4 0 3 plus 2 1 8 1 1 8 5 2?\n",
            "Sample answer: 1 0 7 2 2 0 2 5 5\n",
            "Epoch 1:  25% 20/79 [00:07<00:23,  2.55it/s, v_num=2, train_loss=1.050, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 1 8 6 5 5 plus 6 5 2 4 1?\n",
            "Sample answer: 8 3 8 9 6\n",
            "Epoch 1:  76% 60/79 [00:16<00:05,  3.57it/s, v_num=2, train_loss=1.060, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8 0 3 1 2 8 3 2 2 6 4 3 7 3 7 4 5 0 8 7 0 2 2 4 plus 2 9 1 2 3 5 0 0 6 5 3 4 2 0 3 6 2 3 5 7 2 1 7 1?\n",
            "Sample answer: 1 0 9 4 3 6 3 3 2 9 1 7 7 9 4 1 0 7 4 4 4 2 3 9 5\n",
            "Epoch 1: 100% 79/79 [00:20<00:00,  3.77it/s, v_num=2, train_loss=1.150, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:09<00:00,  2.43s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 1: 100% 79/79 [00:30<00:00,  2.57it/s, v_num=2, train_loss=1.150, val_exact_match=0.000]Epoch 1, global step 40: 'val_exact_match' was not in top 1\n",
            "Epoch 2:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.150, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 1 1 plus 4 7 9?\n",
            "Sample answer: 1 2 9 0\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 3 0 9 plus 7 3 8 2?\n",
            "Sample answer: 8 6 9 1\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 5 3 7 3 9 2 6 1 9 4 7 6 9 9 6 9 1 5 plus 3 1 6 0 5 9 8 5 3 2 8 8 1 4 3 2 5 6 0 4?\n",
            "Sample answer: 7 8 1 4 3 3 7 7 9 4 8 2 9 1 3 2 2 5 1 9\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 0 4 6 3 1 8 2 2 2 2 4 7 2 4 7 0 4 7 8 3 2 9 9 plus 9 2 8 8 4 5 2 6 0 2 8 4 2 9 0 3 7 2 4 0 8 4 7 9?\n",
            "Sample answer: 1 0 3 3 4 7 7 0 8 2 5 0 9 0 1 5 0 7 7 1 9 1 7 7 8\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 9 4 1 1 6 plus 7 1 6 4 9 3?\n",
            "Sample answer: 1 4 1 0 6 0 9\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 4 1 0 9 3 4 7 2 9 8 7 7 1 3 5 9 plus 2 5 1 9 1 2 9 4 6 1 2 7 1 6 6 1?\n",
            "Sample answer: 6 6 2 8 4 7 6 7 6 0 0 4 3 0 2 0\n",
            "Epoch 2:  25% 20/79 [00:04<00:13,  4.41it/s, v_num=2, train_loss=1.170, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 8 9 3 9 2 3 2 7 1 3 3 9 4 1 7 0 3 0 8 9 0 7 7 4 1 3 7 plus 5 9 9 2 6 4 3 8 6 2 5 9 8 2 7 3 4 6 4 4 4 9 9 6 8 8 7?\n",
            "Sample answer: 1 4 9 3 1 8 7 6 5 7 5 9 9 2 4 4 3 7 7 3 3 5 7 7 1 0 2 4\n",
            "Epoch 2:  76% 60/79 [00:13<00:04,  4.45it/s, v_num=2, train_loss=1.040, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8 8 6 8 0 5 4 8 9 8 plus 5 6 4 6 7 5 1 8 8 5?\n",
            "Sample answer: 1 4 5 1 4 8 0 6 7 8 3\n",
            "Epoch 2: 100% 79/79 [00:17<00:00,  4.47it/s, v_num=2, train_loss=1.340, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:09<00:00,  2.36s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 2: 100% 79/79 [00:27<00:00,  2.91it/s, v_num=2, train_loss=1.340, val_exact_match=0.000]Epoch 2, global step 60: 'val_exact_match' was not in top 1\n",
            "Epoch 3:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.340, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 0 7 3 7 5 9 7 6 7 4 0 0 3 6 plus 1 4 8 1 2 1 0 1 1 8 9 5 8 1 6?\n",
            "Sample answer: 4 5 5 4 9 6 9 8 8 6 3 5 8 5 2\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 2 5 3 1 5 9 1 2 1 8 0 1 9 7 4 0 6 7 2 2 9 4 0 8 plus 1 6 9 7 5 0 7 1 7 5 5 1 9 3 6 6 9 1 9 1 4 7 5 4 0?\n",
            "Sample answer: 8 9 5 0 6 6 6 2 9 7 3 2 1 3 4 0 9 8 6 3 7 6 9 4 8\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 4 1 0 5 3 3 0 7 5 5 3 9 6 8 5 2 0 plus 6 7 1 6 5 8 7 5 0 5 9 7 0 2 0 2 3 1?\n",
            "Sample answer: 1 3 1 2 7 1 2 0 5 8 1 5 0 9 8 8 7 5 1\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 7 0 3 2 5 plus 5 7 2 8 7?\n",
            "Sample answer: 1 2 7 6 1 2\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 3 9 9 2 1 2 5 3 4 5 3 4 6 2 1 3 1 6 7 3 6 5 1 8 2 9 3 8 5 plus 1 2 6 5 5 1 1 5 4 1 3 3 6 7 7 3 0 6 2 2 1 8 6 1 3 1 5 7 9?\n",
            "Sample answer: 5 2 5 7 6 3 6 8 8 6 6 8 2 9 8 6 2 2 9 5 8 3 7 9 6 0 9 6 4\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 6 0 0 6 5 0 6 3 5 2 7 3 8 8 6 5 9 9 8 6 9 8 3 8 7 0 5 6 plus 6 0 6 7 8 9 3 1 7 5 6 9 2 1 4 0 3 1 3 0 6 5 3 9 8 0 4 4?\n",
            "Sample answer: 1 2 0 7 4 3 9 9 5 2 8 4 3 1 0 0 6 3 1 1 7 6 3 7 8 5 1 0 0\n",
            "Epoch 3:  25% 20/79 [00:04<00:13,  4.49it/s, v_num=2, train_loss=1.150, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 3 8 8 1 1 5 plus 3 8 0 0 2 5?\n",
            "Sample answer: 7 6 8 1 4 0\n",
            "Epoch 3:  76% 60/79 [00:13<00:04,  4.46it/s, v_num=2, train_loss=1.070, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 3 0 7 6 3 9 plus 6 1 0 1 0 2?\n",
            "Sample answer: 9 1 7 7 4 1\n",
            "Epoch 3: 100% 79/79 [00:17<00:00,  4.50it/s, v_num=2, train_loss=1.080, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 1 4 4 4 4 4 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 0 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.11it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 3: 100% 79/79 [00:21<00:00,  3.73it/s, v_num=2, train_loss=1.080, val_exact_match=0.000]Epoch 3, global step 80: 'val_exact_match' was not in top 1\n",
            "Epoch 4:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.080, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 3 3 9 2 1 7 3 1 6 5 5 2 7 2 3 1 5 1 7 9 4 1 8 5 3 plus 2 9 7 3 0 6 5 8 5 1 1 5 0 8 9 0 5 8 6 7 7 3 6 5 8 4?\n",
            "Sample answer: 6 3 1 2 2 8 3 1 6 7 7 0 3 6 1 3 7 3 8 5 6 7 8 4 3 7\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 2 7 9 5 9 0 1 2 6 9 2 4 5 1 1 7 9 8 7 7 4 7 3 4 5 6 plus 9 0 2 1 4 7 9 1 0 1 7 0 4 9 5 9 7 5 0 4 2 5 7 1 3 7 8?\n",
            "Sample answer: 1 0 3 0 1 0 6 9 2 2 8 6 2 9 4 7 1 5 4 9 2 0 0 4 4 8 3 4\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 1 5 6 plus 1 7 1 1?\n",
            "Sample answer: 4 8 6 7\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 0 8 8 1 6 6 7 7 1 2 6 2 8 7 9 5 6 8 3 1 2 6 7 4 5 6 7 plus 1 3 1 2 6 1 6 2 0 6 8 8 6 2 6 9 6 2 5 2 8 5 5 1 3 6 5 8?\n",
            "Sample answer: 3 4 0 0 7 8 2 9 7 8 1 4 9 1 4 9 1 9 3 5 9 8 1 8 8 2 2 5\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 1 9 2 8 1 7 4 2 4 5 plus 6 0 4 0 7 4 0 2 6 7 2?\n",
            "Sample answer: 1 2 2 3 3 5 5 7 6 9 1 7\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 5 3 plus 3 4?\n",
            "Sample answer: 8 7\n",
            "Epoch 4:  25% 20/79 [00:04<00:13,  4.40it/s, v_num=2, train_loss=1.110, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 9 2 9 8 8 3 3 1 6 7 2 5 4 4 4 9 7 0 1 7 5 0 plus 7 1 5 1 5 7 3 1 4 7 1 2 7 1 5 3 9 1 4 7 8 4?\n",
            "Sample answer: 1 6 4 5 0 4 0 6 3 1 4 3 8 1 6 0 3 6 1 6 5 3 4\n",
            "Epoch 4:  76% 60/79 [00:13<00:04,  4.41it/s, v_num=2, train_loss=1.070, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2 4 5 6 9 8 0 plus 5 6 2 9 4 5 2?\n",
            "Sample answer: 8 0 8 6 4 3 2\n",
            "Epoch 4: 100% 79/79 [00:17<00:00,  4.47it/s, v_num=2, train_loss=1.420, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:09<00:00,  2.37s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 4: 100% 79/79 [00:27<00:00,  2.90it/s, v_num=2, train_loss=1.420, val_exact_match=0.000]Epoch 4, global step 100: 'val_exact_match' was not in top 1\n",
            "Epoch 5:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.420, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 5 6 7 9 4 1 3 9 6 0 0 0 4 9 1 8 6 6 2 8 plus 1 3 9 9 1 7 4 0 1 4 7 4 3 4 3 2 7 2 1 0 8?\n",
            "Sample answer: 6 9 6 7 1 1 5 4 1 0 7 4 3 9 2 4 5 8 7 3 6\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 8 1 0 3 9 2 5 1 plus 6 4 5 6 4 3 2 3?\n",
            "Sample answer: 1 4 5 6 0 3 5 7 4\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 2 8 8 plus 8 0 0?\n",
            "Sample answer: 1 0 8 8\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 4 2 2 0 6 3 2 0 9 2 1 9 2 7 4 plus 5 4 6 5 4 4 6 7 9 3 5 1 9 1 2 9?\n",
            "Sample answer: 1 0 8 8 7 5 1 0 0 0 2 7 3 8 4 0 3\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 9 6 3 5 1 8 5 2 8 1 0 9 1 8 2 1 9 7 0 plus 9 9 6 6 2 5 0 6 8 1 2 8 1 8 2 5 0 0 8 3?\n",
            "Sample answer: 1 7 9 2 9 7 6 9 2 0 9 3 9 1 0 0 7 2 0 5 3\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 5 9 3 4 3 1 0 9 0 6 0 5 9 3 1 7 7 2 7 8 2 8 9 7 3 6 5 7 9 2 plus 7 9 3 4 4 9 1 6 5 3 0 9 9 6 8 8 8 9 2 1 8 7 9 4 5 4 2 8 6 2?\n",
            "Sample answer: 1 3 8 6 8 8 0 2 5 5 9 1 5 9 0 0 6 6 2 0 0 1 6 9 1 9 0 8 6 5 4\n",
            "Epoch 5:  25% 20/79 [00:04<00:13,  4.48it/s, v_num=2, train_loss=1.170, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 3 7 4 9 5 5 2 9 2 plus 9 5 6 0 6 1 9 9 5?\n",
            "Sample answer: 1 3 3 1 0 1 7 2 8 7\n",
            "Epoch 5:  76% 60/79 [00:13<00:04,  4.49it/s, v_num=2, train_loss=1.230, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 7 6 1 2 4 2 9 plus 3 5 5 4 3 1 2?\n",
            "Sample answer: 1 1 1 6 6 7 4 1\n",
            "Epoch 5: 100% 79/79 [00:17<00:00,  4.51it/s, v_num=2, train_loss=1.290, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 1 5 5 6 6 6 6 6 6 6 6 6 6 6 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 1 0 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 1 0 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:04<00:00,  1.14s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 5: 100% 79/79 [00:22<00:00,  3.58it/s, v_num=2, train_loss=1.290, val_exact_match=0.000]Epoch 5, global step 120: 'val_exact_match' was not in top 1\n",
            "Epoch 6:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.290, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 9 2 3 3 1 plus 4 2 3 6 6 8?\n",
            "Sample answer: 1 0 1 5 9 9 9\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 6 8 1 4 3 1 4 0 6 plus 5 9 9 0 2 1 2 3 7 0?\n",
            "Sample answer: 1 1 6 7 1 6 4 3 7 7 6\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 1 7 3 8 6 1 8 3 0 4 6 2 1 plus 8 2 8 8 6 1 9 8 3 6 3 3 6 2?\n",
            "Sample answer: 9 4 6 2 4 8 1 6 6 6 7 9 8 3\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 6 0 9 9 6 5 plus 2 1 2 5 5 0 5?\n",
            "Sample answer: 5 7 3 5 4 7 0\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 4 6 0 4 8 9 2 3 4 7 6 8 0 4 2 0 4 1 6 6 0 4 5 plus 6 6 4 3 3 3 6 2 4 8 1 1 0 8 4 9 3 3 9 6 4 5 3 6?\n",
            "Sample answer: 9 1 0 3 8 2 5 4 8 2 8 7 8 8 9 1 3 8 1 3 0 5 8 1\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 8 6 9 0 0 2 8 8 7 3 5 2 2 9 0 0 8 7 6 5 plus 3 1 6 3 0 9 0 1 4 0 5 9 4 2 6 6 4 1 0 2?\n",
            "Sample answer: 1 1 8 5 3 1 1 9 0 1 4 1 1 7 1 6 7 2 8 6 7\n",
            "Epoch 6:  25% 20/79 [00:04<00:13,  4.40it/s, v_num=2, train_loss=1.050, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 9 6 3 7 4 plus 6 6 4 9 8?\n",
            "Sample answer: 1 6 2 8 7 2\n",
            "Epoch 6:  76% 60/79 [00:13<00:04,  4.53it/s, v_num=2, train_loss=1.190, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 4 7 4 8 3 1 4 3 0 4 0 6 0 plus 8 7 8 5 4 2 9 5 1 9 6 1 2?\n",
            "Sample answer: 1 3 5 3 3 7 4 3 8 2 3 6 7 2\n",
            "Epoch 6: 100% 79/79 [00:17<00:00,  4.54it/s, v_num=2, train_loss=0.977, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 9 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:09<00:00,  2.33s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0030\n",
            "\n",
            "Epoch 6: 100% 79/79 [00:26<00:00,  2.95it/s, v_num=2, train_loss=0.977, val_exact_match=0.003]Epoch 6, global step 140: 'val_exact_match' reached 0.00300 (best 0.00300), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=06-val_exact_match=0.0030.ckpt' as top 1\n",
            "Epoch 7:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.977, val_exact_match=0.003]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 4 2 9 1 2 9 9 0 3 5 2 5 4 6 6 7 5 8 4 0 3 plus 2 4 7 7 7 8 3 9 4 5 4 2 2 5 2 8 3 9 1 4 0 3?\n",
            "Sample answer: 8 9 0 6 9 1 3 8 4 8 9 4 7 9 9 5 1 4 9 8 0 6\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 0 3 3 9 9 5 9 1 3 7 0 4 6 6 4 5 9 7 8 3 2 2 8 7 3 plus 9 0 7 7 3 2 8 4 8 8 7 4 9 4 0 0 3 4 2 9 1 7 5 1 3 2?\n",
            "Sample answer: 1 0 1 1 1 3 2 4 4 0 2 4 5 4 0 6 4 9 4 0 7 4 9 8 0 0 5\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 9 5 2 1 0 7 8 2 4 2 7 1 8 2 7 plus 6 3 9 3 6 3 8 7 4 3 9 1 1 4 8 4?\n",
            "Sample answer: 1 6 3 4 5 7 4 6 5 6 8 1 8 3 3 1 1\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9 2 3 6 0 7 1 5 0 2 6 4 6 7 6 5 9 7 7 5 1 3 8 4 6 2 0 8 2 plus 2 3 6 6 5 9 9 9 0 8 1 7 0 4 1 9 2 6 3 3 4 3 6 9 3 7 4 4 5?\n",
            "Sample answer: 1 1 6 0 2 6 7 1 4 1 0 8 1 7 1 8 5 2 4 0 8 5 7 5 3 9 9 5 2 7\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 4 1 0 2 8 0 7 0 0 5 5 6 1 0 7 3 9 0 3 2 0 5 5 6 plus 9 7 0 9 2 8 1 2 0 8 3 8 6 1 8 9 0 9 1 2 3 7 5 7?\n",
            "Sample answer: 1 3 8 1 2 0 8 8 2 1 3 9 4 7 2 6 2 9 9 4 4 4 3 1 3\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3 6 8 3 0 0 5 0 7 1 2 9 plus 9 1 5 4 7 6 9 7 9 8 3 1?\n",
            "Sample answer: 1 2 8 3 7 7 7 4 8 6 9 6 0\n",
            "Epoch 7:  25% 20/79 [00:04<00:14,  4.03it/s, v_num=2, train_loss=0.960, val_exact_match=0.003]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 8 2 9 3 4 6 5 4 6 8 7 7 4 0 0 8 1 8 4 8 2 1 9 plus 2 4 6 7 0 0 4 0 8 4 4 5 7 6 0 9 7 8 3 7 3 6 3?\n",
            "Sample answer: 1 0 7 6 0 4 6 9 5 5 3 2 3 1 6 1 7 9 6 8 5 5 8 2\n",
            "Epoch 7:  76% 60/79 [00:13<00:04,  4.31it/s, v_num=2, train_loss=1.070, val_exact_match=0.003]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 9 6 8 3 2 0 4 4 2 4 6 2 5 5 0 1 3 2 6 9 8 1 0 7 7 1 5 2 5 plus 2 2 8 7 5 6 5 2 2 0 1 9 6 2 4 3 5 3 0 7 6 7 0 7 1 1 1 1 0?\n",
            "Sample answer: 1 1 9 7 0 7 6 9 6 4 4 8 2 1 7 4 4 8 5 7 7 4 8 1 4 8 2 6 3 5\n",
            "Epoch 7: 100% 79/79 [00:17<00:00,  4.39it/s, v_num=2, train_loss=1.060, val_exact_match=0.003]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 6 8 8 8 8 8 8\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.03it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 7: 100% 79/79 [00:21<00:00,  3.61it/s, v_num=2, train_loss=1.060, val_exact_match=0.002]Epoch 7, global step 160: 'val_exact_match' was not in top 1\n",
            "Epoch 8:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.060, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 4 6 8 0 4 8 0 5 8 8 1 7 9 2 7 3 3 0 2 7 2 4 0 8 9 6 plus 9 7 6 9 9 8 5 5 2 4 9 8 0 1 5 9 2 6 0 9 0 4 1 8 2 7 8?\n",
            "Sample answer: 1 5 2 3 8 0 3 3 5 8 3 7 9 8 0 8 6 5 9 1 1 7 6 5 9 1 7 4\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 9 3 4 9 0 5 1 1 6 9 7 6 3 plus 4 6 3 3 9 3 3 0 7 4 3 2 3 2?\n",
            "Sample answer: 1 0 5 6 8 8 3 8 1 9 1 2 9 9 5\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 3 3 1 4 5 5 1 9 0 6 1 5 plus 6 3 8 5 1 8 1 7 9 8 6 8 6?\n",
            "Sample answer: 1 2 7 1 6 6 3 6 9 8 9 3 0 1\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 9 6 7 0 7 7 6 5 7 4 7 8 5 3 3 3 7 3 4 5 0 5 0 2 8 1 plus 9 1 1 6 7 1 8 0 7 4 1 6 9 4 7 6 3 4 6 7 0 1 5 7 1 8 3?\n",
            "Sample answer: 1 8 0 8 3 7 9 5 7 3 1 6 4 8 0 0 9 7 2 0 1 5 2 0 7 4 6 4\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 3 4 1 8 1 7 6 2 2 8 7 6 0 8 6 9 9 6 7 plus 9 0 4 8 4 4 7 3 4 5 6 0 3 3 9 5 8 2 4 4?\n",
            "Sample answer: 1 5 3 9 0 2 6 4 9 6 8 4 7 9 4 8 2 8 2 1 1\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 4 2 3 2 5 5 7 6 6 8 plus 4 1 0 3 8 0 1 4 7 8?\n",
            "Sample answer: 8 3 3 6 3 5 9 1 4 6\n",
            "Epoch 8:  25% 20/79 [00:04<00:13,  4.51it/s, v_num=2, train_loss=1.070, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 9 3 6 6 9 4 1 0 1 2 2 8 plus 7 6 5 1 2 8 6 1 2 0 7 0?\n",
            "Sample answer: 1 7 0 1 8 2 2 7 1 3 2 9 8\n",
            "Epoch 8:  76% 60/79 [00:13<00:04,  4.49it/s, v_num=2, train_loss=1.090, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 5 5 4 4 6 4 6 1 6 3 4 6 1 9 1 8 0 6 4 7 4 6 3 3 plus 5 3 9 1 9 8 5 8 0 8 5 9 6 9 5 1 2 9 4 1 7 3 6 0?\n",
            "Sample answer: 1 0 9 3 6 6 3 1 9 7 2 0 5 8 8 6 9 3 5 8 9 1 9 9 3\n",
            "Epoch 8: 100% 79/79 [00:17<00:00,  4.50it/s, v_num=2, train_loss=1.000, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 6 9 9 9 9 9 9 9 9 9 9 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.02it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0050\n",
            "\n",
            "Epoch 8: 100% 79/79 [00:21<00:00,  3.67it/s, v_num=2, train_loss=1.000, val_exact_match=0.005]Epoch 8, global step 180: 'val_exact_match' reached 0.00500 (best 0.00500), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=08-val_exact_match=0.0050.ckpt' as top 1\n",
            "Epoch 9:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.000, val_exact_match=0.005]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 7 2 5 5 0 2 1 6 7 7 3 7 8 8 3 1 plus 6 8 6 2 1 9 3 1 2 5 1 8 2 9 4 0?\n",
            "Sample answer: 1 4 1 1 7 2 1 4 8 0 2 5 6 1 7 7 1\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 8 5 4 8 2 1 6 6 5 3 6 1 6 6 2 0 8 plus 3 4 0 5 7 8 8 5 7 4 0 3 4 1 2 3 8?\n",
            "Sample answer: 1 1 9 5 4 0 0 5 2 2 7 6 5 0 7 4 4 6\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 0 5 4 0 3 9 3 7 1 plus 5 1 7 3 0 1 0 4 5 4?\n",
            "Sample answer: 1 1 2 2 7 0 4 9 8 2 5\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 7 1 2 8 3 5 7 2 3 6 1 5 3 3 9 9 5 6 4 5 2 6 5 9 1 0 4 8 plus 6 8 5 3 6 0 3 7 1 9 8 6 1 1 2 2 3 7 5 4 0 8 4 3 9 1 8 9?\n",
            "Sample answer: 1 3 9 8 1 9 6 0 9 5 6 0 1 4 5 2 1 9 3 9 9 3 5 0 3 0 2 3 7\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 6 1 plus 3 3 4?\n",
            "Sample answer: 5 9 5\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 4 4 5 5 3 2 2 8 3 plus 3 6 6 5 7 0 5 6 3?\n",
            "Sample answer: 8 1 2 1 0 2 8 4 6\n",
            "Epoch 9:  25% 20/79 [00:04<00:13,  4.44it/s, v_num=2, train_loss=1.090, val_exact_match=0.005]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 5 0 2 9 9 6 2 2 1 6 8 2 5 4 3 3 2 7 1 plus 6 7 6 7 6 8 7 5 9 5 8 8 0 3 4 7 8 0 6?\n",
            "Sample answer: 1 1 7 9 7 6 4 9 8 1 2 7 0 5 7 8 1 0 7 7\n",
            "Epoch 9:  76% 60/79 [00:13<00:04,  4.43it/s, v_num=2, train_loss=0.963, val_exact_match=0.005]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 9 8 3 3 7 7 6 6 8 6 4 8 9 9 3 0 2 8 0 7 9 plus 8 2 6 0 7 4 2 2 4 0 8 5 9 1 3 7 4 1 3 2 2?\n",
            "Sample answer: 1 8 0 9 4 5 1 8 9 2 7 3 4 9 0 6 7 6 9 4 0 1\n",
            "Epoch 9: 100% 79/79 [00:17<00:00,  4.48it/s, v_num=2, train_loss=1.170, val_exact_match=0.005]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 5 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:04<00:00,  1.22s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0080\n",
            "\n",
            "Epoch 9: 100% 79/79 [00:22<00:00,  3.51it/s, v_num=2, train_loss=1.170, val_exact_match=0.008]Epoch 9, global step 200: 'val_exact_match' reached 0.00800 (best 0.00800), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=09-val_exact_match=0.0080.ckpt' as top 1\n",
            "Epoch 10:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.170, val_exact_match=0.008]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 1 8 9 0 8 3 4 0 1 6 5 1 3 8 plus 7 9 8 4 8 0 6 6 1 9 1 1 4 4 1?\n",
            "Sample answer: 1 2 1 7 3 8 9 0 0 2 0 7 6 5 7 9\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 4 1 8 7 8 2 8 5 8 5 6 8 0 7 2 2 1 1 2 6 1 8 7 6 9 9 plus 7 6 4 5 3 2 3 5 2 3 1 0 5 9 5 5 7 7 9 2 1 0 5 9 0 3?\n",
            "Sample answer: 1 1 8 3 3 1 5 2 1 0 8 7 8 6 6 7 7 8 9 1 8 2 9 3 6 0 2\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 8 8 1 9 3 7 3 7 4 7 9 plus 4 1 8 4 5 6 0 2 6 1 6?\n",
            "Sample answer: 1 3 0 0 3 9 3 4 0 0 9 5\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9 9 8 5 plus 4 9 6 9?\n",
            "Sample answer: 1 4 9 5 4\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 4 4 3 3 2 6 5 2 1 3 1 4 8 2 6 3 7 2 5 8 plus 6 5 3 2 9 5 2 3 8 7 1 7 8 3 6 2 7 8 0 9 8?\n",
            "Sample answer: 7 9 7 6 2 7 8 9 0 8 4 9 3 1 8 9 1 5 3 5 6\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 6 3 5 9 8 9 9 plus 6 7 8 3 1 1 5?\n",
            "Sample answer: 1 3 1 4 3 0 1 4\n",
            "Epoch 10:  25% 20/79 [00:06<00:18,  3.16it/s, v_num=2, train_loss=1.050, val_exact_match=0.008]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 4 0 9 7 7 8 0 8 6 4 4 3 4 3 1 4 2 5 1 4 5 4 6 7 6 8 0 0 2 9 plus 4 7 1 7 2 5 6 0 8 7 9 5 9 4 2 3 4 3 3 1 1 2 0 3 2 0 1 7 8 4?\n",
            "Sample answer: 8 8 1 5 0 3 6 9 5 2 3 9 3 7 3 7 6 8 4 5 6 6 7 0 8 8 1 8 1 3\n",
            "Epoch 10:  76% 60/79 [00:15<00:04,  3.93it/s, v_num=2, train_loss=0.926, val_exact_match=0.008]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 3 8 0 4 3 0 7 2 plus 5 7 6 5 5 8 1 1?\n",
            "Sample answer: 9 5 6 9 8 8 8 3\n",
            "Epoch 10: 100% 79/79 [00:19<00:00,  4.07it/s, v_num=2, train_loss=1.220, val_exact_match=0.008]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 8 8 8 8\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:04<00:00,  1.07s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0080\n",
            "\n",
            "Epoch 10: 100% 79/79 [00:23<00:00,  3.33it/s, v_num=2, train_loss=1.220, val_exact_match=0.008]Epoch 10, global step 220: 'val_exact_match' was not in top 1\n",
            "Epoch 11:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.220, val_exact_match=0.008]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 5 9 1 6 8 4 4 2 7 8 9 3 4 0 3 3 3 7 5 7 9 1 plus 2 0 1 3 3 4 8 5 0 2 8 7 9 5 6 9 1 9 2 4 5 4 9?\n",
            "Sample answer: 1 0 6 0 5 0 3 2 9 3 0 7 7 2 9 7 2 5 3 0 0 3 4 0\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 4 1 0 9 1 plus 5 9 9 8 6?\n",
            "Sample answer: 1 0 1 0 7 7\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 5 5 4 4 7 5 1 3 3 2 0 3 6 2 6 4 plus 7 3 4 4 2 1 3 9 4 4 3 8 3 2 4 4?\n",
            "Sample answer: 1 2 8 8 8 9 6 5 2 7 6 4 1 9 5 0 8\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 1 4 2 6 0 8 0 2 1 8 3 6 6 8 4 1 6 5 plus 2 6 4 8 3 9 7 3 6 9 2 6 1 3 5 9 5 6 7?\n",
            "Sample answer: 8 7 9 1 0 0 5 3 9 1 0 9 8 0 4 3 7 3 2\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 4 1 1 7 7 1 9 0 5 5 2 7 0 0 6 1 3 1 plus 1 4 5 0 4 7 7 5 1 3 8 6 9 5 3 6 9 7?\n",
            "Sample answer: 5 5 6 8 1 9 6 5 6 9 1 3 9 5 9 8 2 8\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1 1 plus 7 4?\n",
            "Sample answer: 8 5\n",
            "Epoch 11:  25% 20/79 [00:04<00:13,  4.39it/s, v_num=2, train_loss=1.010, val_exact_match=0.008]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 3 6 7 6 plus 1 0 0 7?\n",
            "Sample answer: 4 6 8 3\n",
            "Epoch 11:  76% 60/79 [00:13<00:04,  4.43it/s, v_num=2, train_loss=1.070, val_exact_match=0.008]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2 9 7 7 4 7 9 3 9 1 5 1 3 4 1 5 0 1 7 plus 8 1 8 5 8 4 0 6 6 5 1 8 8 9 3 5 9 0 2?\n",
            "Sample answer: 1 1 1 6 3 3 2 0 0 5 6 7 0 2 3 5 0 9 1 9\n",
            "Epoch 11: 100% 79/79 [00:18<00:00,  4.36it/s, v_num=2, train_loss=0.925, val_exact_match=0.008]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.14it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0090\n",
            "\n",
            "Epoch 11: 100% 79/79 [00:21<00:00,  3.65it/s, v_num=2, train_loss=0.925, val_exact_match=0.009]Epoch 11, global step 240: 'val_exact_match' reached 0.00900 (best 0.00900), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=11-val_exact_match=0.0090.ckpt' as top 1\n",
            "Epoch 12:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.925, val_exact_match=0.009]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 5 6 2 9 5 5 5 3 5 6 8 5 7 8 3 4 6 plus 4 7 9 4 5 7 1 6 0 5 6 9 7 9 4 3 0 8?\n",
            "Sample answer: 1 4 3 5 7 5 2 7 1 4 1 3 8 3 7 2 6 5 4\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 3 7 1 6 7 1 8 3 1 7 5 7 3 4 9 0 4 8 5 5 3 6 0 6 plus 2 0 0 3 8 3 3 1 0 2 3 6 0 6 8 9 5 4 7 9 0 4 9 1 8?\n",
            "Sample answer: 3 3 7 5 5 0 4 9 3 4 1 1 8 0 3 8 5 9 6 4 5 8 5 2 4\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 5 1 7 8 2 3 7 6 9 6 7 4 7 5 0 1 2 5 1 8 plus 7 3 5 6 6 4 1 9 1 6 3 7 7 3 1 3 9 0 4 1 6?\n",
            "Sample answer: 8 8 7 4 4 6 5 6 8 6 0 5 2 0 6 4 0 2 9 3 4\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 3 4 1 4 5 5 7 2 9 9 4 5 plus 1 8 4 5 8 3 1 3 1 0 3 3 6?\n",
            "Sample answer: 5 1 8 7 2 8 7 0 4 0 2 8 1\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 5 6 4 0 8 5 4 6 0 plus 5 3 5 0 6 7 9 2 2?\n",
            "Sample answer: 1 0 9 9 1 5 3 3 8 2\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1 3 7 7 9 2 1 4 9 0 3 4 8 0 8 6 7 8 0 1 4 7 1 6 9 4 9 7 plus 2 2 5 6 8 5 2 2 8 9 7 3 3 1 1 7 0 0 2 4 9 8 1 0 5 7 5 8?\n",
            "Sample answer: 3 6 3 4 7 7 3 7 8 0 0 8 1 2 0 3 7 8 2 6 4 5 2 7 5 2 5 5\n",
            "Epoch 12:  25% 20/79 [00:05<00:15,  3.69it/s, v_num=2, train_loss=0.848, val_exact_match=0.009]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 2 1 7 5 1 3 3 1 2 7 0 0 plus 1 0 4 6 8 8 0 4 4 5 7 5?\n",
            "Sample answer: 3 2 2 2 0 1 3 5 7 2 7 5\n",
            "Epoch 12:  76% 60/79 [00:14<00:04,  4.15it/s, v_num=2, train_loss=1.010, val_exact_match=0.009]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2 5 9 0 7 2 6 6 6 1 2 8 2 9 5 7 4 plus 8 8 7 2 8 0 0 1 5 7 0 1 4 3 4 1 8?\n",
            "Sample answer: 1 1 4 6 3 5 2 6 8 1 8 2 9 7 2 9 9 2\n",
            "Epoch 12: 100% 79/79 [00:18<00:00,  4.25it/s, v_num=2, train_loss=1.170, val_exact_match=0.009]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 7 7\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:05<00:00,  1.49s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0080\n",
            "\n",
            "Epoch 12: 100% 79/79 [00:24<00:00,  3.22it/s, v_num=2, train_loss=1.170, val_exact_match=0.008]Epoch 12, global step 260: 'val_exact_match' was not in top 1\n",
            "Epoch 13:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.170, val_exact_match=0.008]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 8 6 1 8 1 8 0 9 8 9 5 plus 5 4 4 2 5 9 2 2 9 6 3 7?\n",
            "Sample answer: 1 5 3 0 4 4 1 0 3 9 5 3 2\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 6 3 0 8 2 1 4 0 5 7 6 6 1 5 6 6 5 2 0 2 1 9 2 7 9 0 plus 5 7 4 1 8 4 8 5 0 6 9 4 5 3 7 9 5 2 4 3 9 0 4 7 9 1 4?\n",
            "Sample answer: 1 3 3 7 2 6 6 9 9 1 2 7 1 1 5 3 6 1 7 6 4 1 2 4 0 7 0 4\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 8 9 0 1 1 2 6 8 1 7 7 5 2 0 2 1 8 7 5 0 0 1 2 3 1 9 9 3 9 plus 6 2 9 1 1 7 1 9 3 8 4 2 9 1 0 7 0 2 2 3 8 9 8 9 1 2 8 2 4?\n",
            "Sample answer: 1 5 1 9 2 2 9 8 7 5 6 1 8 1 1 2 8 8 9 7 3 9 1 1 2 3 2 7 6 3\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 5 5 4 0 5 0 4 1 4 0 4 8 plus 1 2 4 9 8 9 0 5 2 7 9 8 0?\n",
            "Sample answer: 4 8 0 3 9 4 0 9 4 2 0 2 8\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 9 6 0 1 3 0 4 5 0 8 1 7 1 2 1 2 8 8 8 6 1 2 5 3 6 8 9 plus 9 0 0 8 3 3 8 8 6 7 8 1 9 4 5 1 6 0 7 0 9 7 0 4 3 5 9 6?\n",
            "Sample answer: 1 1 9 6 8 4 6 9 3 1 8 6 3 6 5 7 2 8 9 5 9 5 8 2 9 7 2 8 5\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 9 9 1 6 5 8 5 9 6 4 7 4 5 3 4 4 0 6 4 plus 4 2 2 3 7 3 5 5 2 9 7 9 8 6 9 8 1 1 0?\n",
            "Sample answer: 1 4 1 4 0 3 2 1 4 9 4 5 4 4 0 4 2 1 7 4\n",
            "Epoch 13:  25% 20/79 [00:04<00:13,  4.40it/s, v_num=2, train_loss=1.100, val_exact_match=0.008]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 7 6 5 7 7 1 3 0 8 7 2 2 9 4 6 2 9 1 7 0 2 6 6 plus 4 2 7 2 7 8 0 6 4 0 4 6 9 4 9 3 2 9 5 2 7 8 0?\n",
            "Sample answer: 1 1 9 3 0 4 9 3 7 2 7 6 9 8 9 5 6 2 1 2 3 0 4 6\n",
            "Epoch 13:  76% 60/79 [00:13<00:04,  4.47it/s, v_num=2, train_loss=1.110, val_exact_match=0.008]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 5 6 6 0 3 7 8 7 5 5 plus 7 4 5 0 2 8 1 1 5 7?\n",
            "Sample answer: 1 3 1 1 0 6 5 9 9 1 2\n",
            "Epoch 13: 100% 79/79 [00:17<00:00,  4.49it/s, v_num=2, train_loss=1.050, val_exact_match=0.008]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 7 1 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.10it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0090\n",
            "\n",
            "Epoch 13: 100% 79/79 [00:21<00:00,  3.71it/s, v_num=2, train_loss=1.050, val_exact_match=0.009]Epoch 13, global step 280: 'val_exact_match' was not in top 1\n",
            "Epoch 14:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.050, val_exact_match=0.009]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 1 1 6 4 0 3 4 9 6 1 3 1 2 1 6 plus 6 0 3 3 6 5 7 0 0 8 8 9 3 3 6?\n",
            "Sample answer: 7 1 9 7 6 9 1 9 7 0 2 0 5 5 2\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 5 6 3 4 1 3 9 5 0 7 3 2 plus 6 6 3 9 9 1 7 8 1 7 8 9 6?\n",
            "Sample answer: 1 4 2 0 3 3 3 1 7 6 8 6 2 8\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 8 7 6 6 1 9 7 3 7 0 1 7 2 4 3 plus 9 1 8 8 7 8 1 1 2 3 4 9 7 0 5 2?\n",
            "Sample answer: 1 1 0 6 5 4 0 0 8 6 0 5 1 4 2 9 5\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 2 0 2 0 3 4 7 7 6 1 4 5 9 3 0 6 1 0 0 1 4 6 6 9 3 5 6 plus 8 5 7 8 3 2 3 1 8 0 4 4 7 5 1 5 5 2 8 0 7 4 9 3 4 6 1 5?\n",
            "Sample answer: 1 3 7 8 0 3 5 7 9 5 6 5 9 3 4 4 6 1 3 8 0 8 9 6 0 3 9 7 1\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 0 6 7 7 8 4 3 9 plus 6 2 1 3 7 3 4 1 4?\n",
            "Sample answer: 7 2 8 1 5 1 8 5 3\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 6 5 0 6 3 1 5 7 0 6 4 9 8 0 5 8 0 3 8 7 5 9 0 5 4 1 plus 6 1 2 6 9 0 0 0 7 0 6 9 1 3 7 7 6 8 4 3 6 3 6 6 8 6?\n",
            "Sample answer: 1 2 6 3 3 2 1 5 7 7 7 1 8 9 4 3 5 7 2 3 1 2 2 7 2 2 7\n",
            "Epoch 14:  25% 20/79 [00:04<00:13,  4.40it/s, v_num=2, train_loss=1.080, val_exact_match=0.009]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 2 0 2 4 7 5 0 9 8 6 5 0 7 8 2 9 2 9 8 0 4 2 0 8 8 4 9 9 3 plus 4 6 5 7 7 2 4 3 0 3 4 9 7 2 2 8 8 4 3 5 8 3 2 8 0 0 4 4 2?\n",
            "Sample answer: 6 6 8 2 4 7 5 2 9 0 0 0 5 0 5 8 1 4 1 6 2 5 3 6 8 5 4 3 5\n",
            "Epoch 14:  76% 60/79 [00:13<00:04,  4.48it/s, v_num=2, train_loss=1.050, val_exact_match=0.009]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8 9 9 6 6 5 plus 6 1 0 3 2 6?\n",
            "Sample answer: 1 5 0 9 9 9 1\n",
            "Epoch 14: 100% 79/79 [00:17<00:00,  4.51it/s, v_num=2, train_loss=1.050, val_exact_match=0.009]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 7\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.25it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0140\n",
            "\n",
            "Epoch 14: 100% 79/79 [00:20<00:00,  3.81it/s, v_num=2, train_loss=1.050, val_exact_match=0.014]Epoch 14, global step 300: 'val_exact_match' reached 0.01400 (best 0.01400), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=14-val_exact_match=0.0140.ckpt' as top 1\n",
            "Epoch 15:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.050, val_exact_match=0.014]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 2 0 6 1 8 7 7 4 7 3 8 3 5 5 6 9 4 3 3 9 0 3 9 2 2 0 4 1 plus 2 7 1 8 3 5 6 4 4 8 5 0 9 6 6 0 2 8 9 4 3 5 8 8 2 4 0 3?\n",
            "Sample answer: 4 7 8 0 2 3 3 9 2 2 3 4 5 2 2 9 7 2 3 3 3 9 8 0 4 4 4 4\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 6 1 plus 7 1?\n",
            "Sample answer: 1 3 2\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 2 7 4 6 6 2 4 6 plus 6 5 2 4 6 4 1 0?\n",
            "Sample answer: 9 2 7 1 2 6 5 6\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 1 3 7 9 8 1 5 9 9 2 5 9 8 4 4 2 0 0 plus 2 1 6 6 0 8 3 8 7 3 4 0 3 5 3 6 7 8 0?\n",
            "Sample answer: 5 3 0 4 0 6 5 4 7 2 6 6 3 3 8 0 9 8 0\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 3 0 2 7 7 8 8 plus 5 1 5 5 2 9 1?\n",
            "Sample answer: 8 1 8 3 0 7 9\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3 0 2 7 6 2 6 1 3 2 3 9 1 0 9 2 7 7 2 4 3 1 4 4 0 2 4 plus 2 7 2 7 7 6 4 1 8 4 9 2 5 3 9 2 4 9 8 8 2 8 4 8 2 5 8?\n",
            "Sample answer: 5 7 5 5 3 9 0 3 1 7 3 1 6 4 8 5 2 7 1 2 5 9 9 2 2 8 2\n",
            "Epoch 15:  25% 20/79 [00:08<00:23,  2.48it/s, v_num=2, train_loss=1.160, val_exact_match=0.014]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 2 8 1 7 2 4 8 2 2 7 1 0 3 0 plus 6 8 8 8 6 3 0 5 6 7 6 3 4 9?\n",
            "Sample answer: 9 7 0 5 8 7 8 7 9 4 7 3 7 9\n",
            "Epoch 15:  76% 60/79 [00:17<00:05,  3.51it/s, v_num=2, train_loss=0.921, val_exact_match=0.014]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 3 8 8 9 8 0 1 9 7 plus 9 5 1 9 6 2 0 5 7?\n",
            "Sample answer: 1 3 4 0 9 4 2 2 5 4\n",
            "Epoch 15: 100% 79/79 [00:21<00:00,  3.73it/s, v_num=2, train_loss=1.110, val_exact_match=0.014]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 4 8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 1 1\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.17it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0070\n",
            "\n",
            "Epoch 15: 100% 79/79 [00:24<00:00,  3.21it/s, v_num=2, train_loss=1.110, val_exact_match=0.007]Epoch 15, global step 320: 'val_exact_match' was not in top 1\n",
            "Epoch 16:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.110, val_exact_match=0.007]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 8 9 3 1 6 4 9 2 9 plus 8 6 9 2 5 5 1 3 5 9?\n",
            "Sample answer: 1 8 5 8 5 7 1 6 2 8 8\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 0 8 6 4 6 8 9 4 plus 3 7 0 9 4 0 0 4 4?\n",
            "Sample answer: 1 0 7 9 5 8 6 9 3 8\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 7 5 7 4 2 8 plus 8 5 7 6 4 9?\n",
            "Sample answer: 1 6 1 5 0 7 7\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 8 7 9 1 plus 6 1 7 4 8?\n",
            "Sample answer: 9 0 5 3 9\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 7 0 7 4 9 9 4 1 0 9 9 7 4 5 plus 6 8 9 6 8 5 0 7 7 6 7 3 0 0 4?\n",
            "Sample answer: 8 6 0 4 3 5 0 1 8 7 7 2 7 4 9\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1 4 1 5 9 3 3 9 3 1 1 4 7 plus 7 1 4 0 8 3 5 1 2 9 3 6 0?\n",
            "Sample answer: 8 5 5 6 7 6 9 0 6 0 5 0 7\n",
            "Epoch 16:  25% 20/79 [00:04<00:13,  4.44it/s, v_num=2, train_loss=1.120, val_exact_match=0.007]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 8 3 7 6 7 4 1 3 0 7 8 2 1 8 0 1 6 0 7 7 4 9 3 8 2 plus 3 0 4 0 9 0 3 9 6 2 7 9 8 6 9 8 6 7 0 4 2 9 8 3 7?\n",
            "Sample answer: 1 1 4 1 7 6 4 5 2 7 0 6 2 0 5 0 0 2 7 8 1 7 9 2 1 9\n",
            "Epoch 16:  76% 60/79 [00:13<00:04,  4.49it/s, v_num=2, train_loss=1.070, val_exact_match=0.007]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2 2 0 5 0 2 6 6 5 3 2 8 plus 1 7 1 6 6 0 5 1 0 7 1 6?\n",
            "Sample answer: 3 9 2 1 6 3 1 7 6 0 4 4\n",
            "Epoch 16: 100% 79/79 [00:17<00:00,  4.50it/s, v_num=2, train_loss=0.953, val_exact_match=0.007]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 4 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:05<00:00,  1.44s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0130\n",
            "\n",
            "Epoch 16: 100% 79/79 [00:23<00:00,  3.38it/s, v_num=2, train_loss=0.953, val_exact_match=0.013]Epoch 16, global step 340: 'val_exact_match' was not in top 1\n",
            "Epoch 17:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.953, val_exact_match=0.013]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 7 5 2 2 5 4 6 8 4 5 4 7 4 7 3 6 5 7 2 9 9 7 4 7 6 7 2 plus 7 5 1 6 5 7 8 8 5 5 1 1 0 6 6 5 8 6 9 4 3 9 6 6 7 6 0 7?\n",
            "Sample answer: 1 3 2 6 8 8 3 3 5 3 9 6 5 8 1 3 9 5 2 6 7 3 9 4 1 5 2 7 9\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 8 5 7 6 plus 9 6 3 6 2?\n",
            "Sample answer: 1 7 4 9 3 8\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 8 7 6 2 3 5 7 plus 7 9 3 7 8 0 6?\n",
            "Sample answer: 1 6 7 0 0 1 6 3\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 9 1 2 4 8 plus 2 3 9 4 3 0?\n",
            "Sample answer: 6 3 0 6 7 8\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 4 4 2 6 9 0 3 0 6 0 9 8 4 8 3 9 1 4 0 0 plus 1 2 8 7 9 1 5 5 7 7 2 3 8 1 7 5 7 6 5 2 5?\n",
            "Sample answer: 3 7 3 0 6 0 5 8 8 3 3 3 6 6 5 9 6 7 9 2 5\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3 3 7 6 6 8 8 3 5 5 3 7 6 5 6 3 8 3 3 7 9 1 3 2 2 2 1 0 plus 2 7 9 9 5 7 6 2 5 9 1 4 9 4 5 9 4 1 9 3 6 4 2 3 7 9 7 1?\n",
            "Sample answer: 6 1 7 6 2 6 4 6 1 4 5 2 6 0 2 3 2 5 3 1 5 5 5 6 0 1 8 1\n",
            "Epoch 17:  25% 20/79 [00:04<00:13,  4.45it/s, v_num=2, train_loss=0.992, val_exact_match=0.013]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 4 3 0 1 5 9 7 2 7 1 6 3 4 0 0 0 6 6 5 6 9 7 plus 4 6 4 7 2 7 8 0 8 0 4 9 8 9 2 3 3 1 9 6 5 8?\n",
            "Sample answer: 8 9 4 8 8 7 5 3 5 2 1 3 2 9 2 3 9 8 5 3 5 5\n",
            "Epoch 17:  76% 60/79 [00:13<00:04,  4.48it/s, v_num=2, train_loss=0.934, val_exact_match=0.013]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8 3 5 0 3 3 8 2 4 9 9 0 4 6 6 4 1 3 6 1 9 2 9 5 8 7 7 9 plus 5 3 3 3 1 3 5 5 0 4 4 3 6 1 8 8 8 1 5 1 5 5 8 2 5 9 8 9?\n",
            "Sample answer: 1 3 6 8 3 4 7 3 7 5 4 3 4 0 8 5 2 9 5 1 3 4 8 7 8 4 7 6 8\n",
            "Epoch 17: 100% 79/79 [00:17<00:00,  4.48it/s, v_num=2, train_loss=1.230, val_exact_match=0.013]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 9 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 3 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:02<00:00,  1.36it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0150\n",
            "\n",
            "Epoch 17: 100% 79/79 [00:20<00:00,  3.84it/s, v_num=2, train_loss=1.230, val_exact_match=0.015]Epoch 17, global step 360: 'val_exact_match' reached 0.01500 (best 0.01500), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0150.ckpt' as top 1\n",
            "Epoch 18:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.230, val_exact_match=0.015]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 0 8 3 5 9 3 1 6 1 5 1 0 0 5 plus 5 1 6 8 4 6 8 6 2 7 1 7 7 5 7?\n",
            "Sample answer: 1 3 2 5 2 0 6 1 7 8 8 6 8 7 6 2\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 0 3 8 6 3 9 2 5 6 1 7 4 4 2 8 3 5 9 4 0 4 5 1 plus 5 5 1 9 6 4 1 9 2 8 6 3 6 0 7 5 0 4 6 2 3 6 7 5?\n",
            "Sample answer: 1 2 5 5 8 2 8 1 1 8 4 8 1 0 5 0 3 4 0 5 6 4 1 2 6\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 7 6 8 4 9 1 7 1 3 5 1 9 4 2 8 1 4 9 4 9 9 9 2 4 1 4 plus 8 8 2 3 6 1 1 0 3 8 9 7 9 3 6 2 7 8 9 3 4 6 9 5 1 3?\n",
            "Sample answer: 1 6 5 0 8 5 2 8 1 7 4 1 7 3 6 4 4 2 8 4 3 4 6 1 9 2 7\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 3 4 1 1 1 0 3 1 9 6 3 8 5 5 7 3 9 7 1 7 8 0 0 7 1 plus 2 2 6 0 9 5 8 0 6 0 6 8 6 3 8 1 1 1 8 1 6 3 5 9 3 9?\n",
            "Sample answer: 8 6 0 2 0 6 8 3 8 0 3 2 4 9 3 8 5 1 5 3 4 1 6 0 1 0\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 9 3 0 9 4 7 3 3 0 6 1 1 7 5 3 2 plus 9 5 3 5 5 8 7 0 3 5 4 2 2 3 9 9 4?\n",
            "Sample answer: 1 2 4 6 6 5 3 4 3 6 6 0 3 4 1 5 2 6\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1 0 1 9 4 5 5 5 2 8 1 4 7 2 7 5 0 2 4 6 2 6 5 plus 4 9 9 0 0 4 6 0 0 3 3 2 4 1 9 0 8 7 8 9 8 5 9?\n",
            "Sample answer: 6 0 0 9 5 0 1 5 3 1 4 7 1 4 6 5 9 0 3 6 1 2 4\n",
            "Epoch 18:  25% 20/79 [00:07<00:20,  2.81it/s, v_num=2, train_loss=1.180, val_exact_match=0.015]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 8 7 8 7 5 3 5 3 plus 8 8 0 2 8 0 5 6?\n",
            "Sample answer: 1 7 5 9 0 3 4 0 9\n",
            "Epoch 18:  76% 60/79 [00:16<00:05,  3.74it/s, v_num=2, train_loss=0.978, val_exact_match=0.015]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8 6 4 4 6 4 2 9 4 9 6 4 3 6 plus 3 5 5 5 7 0 7 4 6 7 6 2 3 1?\n",
            "Sample answer: 1 2 2 0 0 3 5 0 4 1 7 2 6 6 7\n",
            "Epoch 18: 100% 79/79 [00:20<00:00,  3.92it/s, v_num=2, train_loss=0.810, val_exact_match=0.015]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 7 7 7 7 4 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:06<00:00,  1.73s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0160\n",
            "\n",
            "Epoch 18: 100% 79/79 [00:27<00:00,  2.91it/s, v_num=2, train_loss=0.810, val_exact_match=0.016]Epoch 18, global step 380: 'val_exact_match' reached 0.01600 (best 0.01600), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=18-val_exact_match=0.0160.ckpt' as top 1\n",
            "Epoch 19:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.810, val_exact_match=0.016]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 6 1 7 1 0 0 6 6 9 6 plus 3 9 2 2 7 2 9 2 7 5 7?\n",
            "Sample answer: 1 2 5 3 9 8 2 9 9 4 5 3\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 2 4 0 7 9 5 4 2 0 7 4 9 1 2 1 5 3 8 0 9 1 3 4 4 8 6 2 3 0 plus 7 4 0 6 6 9 4 6 6 1 3 3 0 1 7 6 2 6 8 9 9 5 1 9 2 3 4 9 9 4?\n",
            "Sample answer: 1 4 6 4 7 4 9 0 0 8 2 0 7 9 2 9 7 8 0 7 0 8 6 5 3 7 2 1 2 2 4\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 3 0 plus 8 2 4?\n",
            "Sample answer: 1 7 5 4\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3 6 7 8 8 3 4 3 7 0 9 4 5 9 2 9 6 7 1 plus 5 2 2 3 9 8 3 2 9 9 5 5 8 1 0 1 1 4 6?\n",
            "Sample answer: 8 9 0 2 8 1 7 6 7 0 5 0 4 0 3 0 8 1 7\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 5 6 6 9 4 4 plus 2 8 6 6 2 7?\n",
            "Sample answer: 8 5 3 5 7 1\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3 1 5 7 4 4 2 4 1 1 3 3 4 7 8 6 9 7 9 9 6 0 3 2 plus 4 3 2 9 8 1 8 5 0 7 6 6 1 1 7 2 0 4 0 0 8 5 4 4?\n",
            "Sample answer: 7 4 8 7 2 6 0 9 1 8 9 9 5 9 5 9 0 2 0 0 4 5 7 6\n",
            "Epoch 19:  25% 20/79 [00:05<00:15,  3.81it/s, v_num=2, train_loss=0.990, val_exact_match=0.016]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 7 6 5 3 8 3 7 8 7 8 2 6 plus 8 2 4 6 2 6 2 6 2 7 3 5?\n",
            "Sample answer: 1 5 9 0 0 1 0 0 5 0 5 6 1\n",
            "Epoch 19:  76% 60/79 [00:14<00:04,  4.24it/s, v_num=2, train_loss=1.040, val_exact_match=0.016]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 4 6 9 7 1 8 1 2 7 2 2 2 8 0 2 2 6 3 2 5 9 plus 1 4 5 5 4 2 6 9 4 1 1 0 0 7 2 3 6 1 7 5 3?\n",
            "Sample answer: 6 1 5 2 6 0 8 2 1 3 3 2 8 7 4 6 2 5 0 1 2\n",
            "Epoch 19: 100% 79/79 [00:18<00:00,  4.30it/s, v_num=2, train_loss=1.130, val_exact_match=0.016]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 7 7 9 9 9 9 4\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 6 8 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.03it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0250\n",
            "\n",
            "Epoch 19: 100% 79/79 [00:22<00:00,  3.55it/s, v_num=2, train_loss=1.130, val_exact_match=0.025]Epoch 19, global step 400: 'val_exact_match' reached 0.02500 (best 0.02500), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.0250.ckpt' as top 1\n",
            "Epoch 20:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.130, val_exact_match=0.025]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 1 2 0 5 2 0 plus 4 4 3 2 7 9?\n",
            "Sample answer: 5 6 3 7 9 9\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 8 6 3 4 0 4 0 7 3 plus 5 0 6 3 7 0 2 4 6?\n",
            "Sample answer: 1 3 6 9 7 7 4 3 1 9\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 7 7 8 5 plus 9 2 9 4?\n",
            "Sample answer: 1 7 0 7 9\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9 0 2 7 5 3 8 2 0 7 3 4 1 5 5 2 2 plus 9 7 0 8 7 2 9 3 1 3 7 9 7 2 8 6 1?\n",
            "Sample answer: 1 8 7 3 6 2 6 7 5 2 1 1 3 8 8 3 8 3\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 3 4 8 plus 8 1 8?\n",
            "Sample answer: 1 1 6 6\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 2 1 0 0 1 8 6 2 8 2 7 0 7 3 6 2 9 2 2 9 6 0 2 1 7 9 4 8 8 plus 3 9 1 9 0 0 2 3 7 8 7 1 8 6 1 2 8 9 7 2 3 9 2 3 8 1 2 9 8?\n",
            "Sample answer: 6 0 1 9 1 8 8 6 6 1 4 2 5 9 7 5 8 2 0 1 9 9 4 5 6 0 7 8 6\n",
            "Epoch 20:  25% 20/79 [00:04<00:13,  4.41it/s, v_num=2, train_loss=1.020, val_exact_match=0.025]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 5 4 8 2 1 2 4 2 6 6 5 1 3 0 plus 9 8 5 4 6 1 8 0 1 0 0 7 0 1?\n",
            "Sample answer: 1 5 3 3 6 7 4 2 2 7 6 5 8 3 1\n",
            "Epoch 20:  76% 60/79 [00:13<00:04,  4.43it/s, v_num=2, train_loss=0.955, val_exact_match=0.025]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2 9 6 5 1 plus 5 4 5 5 0?\n",
            "Sample answer: 8 4 2 0 1\n",
            "Epoch 20: 100% 79/79 [00:17<00:00,  4.47it/s, v_num=2, train_loss=0.886, val_exact_match=0.025]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 6 0 7 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 7 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:07<00:00,  1.80s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0420\n",
            "\n",
            "Epoch 20: 100% 79/79 [00:24<00:00,  3.17it/s, v_num=2, train_loss=0.886, val_exact_match=0.042]Epoch 20, global step 420: 'val_exact_match' reached 0.04200 (best 0.04200), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=20-val_exact_match=0.0420.ckpt' as top 1\n",
            "Epoch 21:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.886, val_exact_match=0.042]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 3 4 6 1 0 0 0 9 6 8 3 6 8 4 4 2 9 plus 4 6 8 9 3 4 8 8 4 4 2 2 1 4 4 2 5 1?\n",
            "Sample answer: 1 4 0 3 5 4 4 8 9 4 1 0 5 8 2 8 6 8 0\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 6 0 5 0 0 5 plus 7 0 3 5 4 3 5?\n",
            "Sample answer: 8 6 4 0 4 4 0\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 2 2 0 6 3 5 6 1 3 9 0 5 6 9 0 8 0 0 4 2 4 9 3 1 6 7 0 5 7 plus 3 7 8 8 7 9 4 3 0 7 8 1 6 5 1 5 2 1 6 3 4 8 3 0 2 1 9 9 8?\n",
            "Sample answer: 5 9 9 5 1 5 0 4 4 6 8 7 3 4 2 3 2 2 0 5 9 7 6 1 8 9 0 5 5\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 5 7 1 1 7 plus 7 0 9 4 7 9?\n",
            "Sample answer: 1 5 6 6 5 9 6\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 2 8 2 8 6 5 8 8 9 6 2 4 1 7 0 6 plus 7 4 7 8 2 1 2 4 9 9 4 7 6 4 4 6 5?\n",
            "Sample answer: 8 7 6 1 0 7 8 3 8 9 1 0 0 6 1 7 1\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1 2 1 3 9 7 5 1 8 plus 6 6 5 2 6 9 7 9 1?\n",
            "Sample answer: 7 8 6 6 6 7 3 0 9\n",
            "Epoch 21:  25% 20/79 [00:05<00:15,  3.76it/s, v_num=2, train_loss=0.897, val_exact_match=0.042]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 3 6 0 0 4 6 8 4 8 plus 2 6 8 6 0 9 8 6 8?\n",
            "Sample answer: 6 2 8 6 5 6 7 1 6\n",
            "Epoch 21:  76% 60/79 [00:14<00:04,  4.18it/s, v_num=2, train_loss=0.863, val_exact_match=0.042]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 9 1 9 5 4 4 9 1 8 8 7 4 9 8 1 8 8 1 5 9 8 1 9 0 plus 3 0 6 5 8 0 0 3 2 2 8 1 9 3 5 8 7 0 8 8 7 4 2 3?\n",
            "Sample answer: 1 2 2 6 1 2 4 9 5 1 1 5 6 9 1 7 7 5 2 4 8 5 6 1 3\n",
            "Epoch 21: 100% 79/79 [00:18<00:00,  4.27it/s, v_num=2, train_loss=1.120, val_exact_match=0.042]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 4 0 5 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 9 9 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 8 0 9 1 1 1 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 6 6 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 4\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:06<00:00,  1.59s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0480\n",
            "\n",
            "Epoch 21: 100% 79/79 [00:24<00:00,  3.17it/s, v_num=2, train_loss=1.120, val_exact_match=0.048]Epoch 21, global step 440: 'val_exact_match' reached 0.04800 (best 0.04800), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=21-val_exact_match=0.0480.ckpt' as top 1\n",
            "Epoch 22:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.120, val_exact_match=0.048]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 1 2 6 7 1 7 2 6 8 7 5 9 9 0 4 plus 4 2 6 6 7 8 5 8 6 4 2 3 9 2 7 3?\n",
            "Sample answer: 7 3 9 3 5 0 3 1 3 2 9 9 9 1 7 7\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 8 1 2 4 0 6 4 6 5 7 3 3 9 0 7 8 5 8 4 3 0 1 plus 3 8 3 3 4 4 1 3 5 6 4 0 9 6 5 4 9 1 1 7 6 5?\n",
            "Sample answer: 1 1 9 5 7 5 0 6 0 1 3 7 4 8 7 3 3 4 9 6 0 6 6\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 6 6 6 9 5 0 0 plus 2 8 0 2 8 2 4 6?\n",
            "Sample answer: 9 4 6 9 7 7 4 6\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 5 6 5 9 8 7 3 plus 9 1 4 2 4 5 6 2?\n",
            "Sample answer: 1 4 7 0 8 4 4 3 5\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 3 2 1 8 0 1 4 9 9 plus 9 3 4 7 5 4 4 6 7?\n",
            "Sample answer: 1 2 5 6 5 5 5 9 6 6\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3 6 5 3 6 9 5 plus 2 6 7 9 6 5 5?\n",
            "Sample answer: 6 3 3 3 3 5 0\n",
            "Epoch 22:  25% 20/79 [00:07<00:21,  2.78it/s, v_num=2, train_loss=0.980, val_exact_match=0.048]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 1 0 6 5 5 3 0 4 2 3 4 5 6 8 plus 3 4 6 3 2 7 5 8 8 9 0 7 7 0?\n",
            "Sample answer: 4 5 2 8 8 0 6 3 1 2 5 3 3 8\n",
            "Epoch 22:  76% 60/79 [00:16<00:05,  3.74it/s, v_num=2, train_loss=0.890, val_exact_match=0.048]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 4 9 9 8 1 4 3 2 2 4 0 5 9 3 3 6 2 2 5 5 8 4 2 0 7 7 3 1 1 plus 7 3 1 3 0 6 4 5 1 7 5 7 4 8 9 1 2 7 1 6 9 9 4 8 0 7 1 9 4?\n",
            "Sample answer: 1 2 3 1 1 2 0 7 7 4 1 6 3 4 2 2 7 4 9 7 2 8 3 6 8 8 4 5 0 5\n",
            "Epoch 22: 100% 79/79 [00:20<00:00,  3.91it/s, v_num=2, train_loss=0.933, val_exact_match=0.048]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 4 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 0 9 8 8 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 6 6 6 6 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:04<00:00,  1.10s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0630\n",
            "\n",
            "Epoch 22: 100% 79/79 [00:24<00:00,  3.21it/s, v_num=2, train_loss=0.933, val_exact_match=0.063]Epoch 22, global step 460: 'val_exact_match' reached 0.06300 (best 0.06300), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt' as top 1\n",
            "Epoch 23:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=0.933, val_exact_match=0.063]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 7 5 9 9 0 5 5 8 plus 2 9 9 1 5 8 1 6?\n",
            "Sample answer: 1 0 5 9 0 6 3 7 4\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 1 9 9 6 6 9 2 5 9 4 7 8 8 9 9 2 2 9 0 plus 5 0 1 0 8 9 2 7 5 1 9 5 4 3 6 0 5 7 6 3?\n",
            "Sample answer: 8 2 1 0 5 6 2 0 1 1 4 3 3 2 5 9 8 0 5 3\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 2 7 4 6 6 2 4 6 plus 6 5 2 4 6 4 1 0?\n",
            "Sample answer: 9 2 7 1 2 6 5 6\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 6 8 3 7 5 5 0 1 8 4 0 8 5 0 0 8 8 5 3 7 0 5 3 plus 2 7 8 6 2 3 4 1 4 4 2 7 5 9 1 7 6 1 0 7 5 7 9 3?\n",
            "Sample answer: 5 4 6 9 9 8 9 1 6 2 6 8 4 4 1 8 4 9 6 1 2 8 4 6\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 9 5 8 2 5 1 6 7 3 5 6 4 5 0 6 3 2 5 0 6 0 3 7 plus 3 1 6 4 3 6 0 3 8 3 3 4 1 9 4 4 2 4 4 0 1 5 0 6?\n",
            "Sample answer: 1 1 1 2 2 6 1 2 0 5 6 9 0 6 4 5 0 5 6 9 0 7 5 4 3\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 9 9 5 5 plus 6 6 1 8?\n",
            "Sample answer: 1 6 5 7 3\n",
            "Epoch 23:  25% 20/79 [00:06<00:19,  3.03it/s, v_num=2, train_loss=0.803, val_exact_match=0.063]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 4 3 4 0 2 7 8 0 6 1 5 0 6 7 7 3 1 6 8 8 plus 5 3 0 2 6 9 5 8 1 1 1 9 7 5 2 4 1 8 3 5?\n",
            "Sample answer: 9 6 4 2 9 7 3 8 7 2 7 0 4 2 9 7 3 5 2 3\n",
            "Epoch 23:  76% 60/79 [00:15<00:04,  3.81it/s, v_num=2, train_loss=0.824, val_exact_match=0.063]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 5 2 3 2 5 9 7 2 0 3 3 2 8 4 6 0 3 7 3 5 7 1 7 8 7 5 6 4 7 4 plus 8 5 3 2 6 6 4 5 5 8 6 5 6 3 7 6 1 1 6 3 3 5 7 7 4 0 2 7 7 5?\n",
            "Sample answer: 1 3 7 6 5 2 6 1 7 6 1 9 8 4 8 3 6 4 8 9 9 0 7 5 6 1 5 9 2 4 9\n",
            "Epoch 23: 100% 79/79 [00:20<00:00,  3.94it/s, v_num=2, train_loss=1.250, val_exact_match=0.063]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 8 0 0 0 0 7 7 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 7 6 6 5 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:04<00:00,  1.05s/it]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0370\n",
            "\n",
            "Epoch 23: 100% 79/79 [00:24<00:00,  3.26it/s, v_num=2, train_loss=1.250, val_exact_match=0.037]Epoch 23, global step 480: 'val_exact_match' was not in top 1\n",
            "Epoch 24:   0% 0/79 [00:00<?, ?it/s, v_num=2, train_loss=1.250, val_exact_match=0.037]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 5 9 2 2 4 2 2 9 2 5 0 2 8 8 0 0 2 3 0 7 8 0 5 plus 8 6 7 2 1 2 5 1 8 8 8 3 1 6 2 1 3 3 9 0 7 4 4 2?\n",
            "Sample answer: 1 3 2 6 4 3 6 7 4 8 1 3 3 4 5 0 1 3 6 2 1 5 2 4 7\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 3 0 2 5 2 4 1 0 2 6 8 5 7 6 8 6 4 1 7 plus 2 0 4 2 8 6 8 0 0 1 4 9 0 4 0 3 7 6 7 9?\n",
            "Sample answer: 5 3 4 5 3 9 2 1 0 4 1 7 6 1 7 2 4 0 9 6\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 5 2 5 0 5 8 0 9 1 9 7 plus 2 6 6 1 1 8 9 4 7 8 2?\n",
            "Sample answer: 7 9 1 1 7 7 0 3 9 7 9\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9 4 3 2 4 5 9 plus 2 8 4 9 4 9 0?\n",
            "Sample answer: 1 2 2 8 1 9 4 9\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 2 4 1 1 7 6 3 0 0 5 3 2 6 4 4 2 6 8 5 5 6 9 8 7 9 0 5 plus 7 3 6 1 4 1 3 2 8 7 4 5 8 4 0 1 2 9 1 2 0 2 3 2 2 6 3 4?\n",
            "Sample answer: 9 6 0 2 5 8 9 5 8 7 9 9 1 0 4 5 5 5 9 7 5 9 3 1 0 5 3 9\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 7 0 7 1 plus 6 4 0 0?\n",
            "Sample answer: 1 3 4 7 1\n",
            "Epoch 24:  25% 20/79 [00:04<00:14,  4.17it/s, v_num=2, train_loss=0.994, val_exact_match=0.037]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 7 8 3 9 4 5 0 7 3 8 plus 2 8 4 6 2 2 3 1 8 3?\n",
            "Sample answer: 1 0 6 8 5 6 7 3 9 2 1\n",
            "Epoch 24:  76% 60/79 [00:13<00:04,  4.39it/s, v_num=2, train_loss=0.974, val_exact_match=0.037]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 9 4 5 2 1 8 8 9 7 6 0 1 3 0 4 6 0 6 4 5 9 0 plus 9 0 3 3 9 5 0 6 4 7 4 5 3 7 7 5 0 2 7 3 1 0?\n",
            "Sample answer: 1 8 4 8 6 1 3 9 6 2 3 4 6 6 8 2 1 0 9 1 9 0 0\n",
            "Epoch 24: 100% 79/79 [00:17<00:00,  4.43it/s, v_num=2, train_loss=1.080, val_exact_match=0.037]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 9 6 0 1 3 6 4 5 9 3 5 3 3 0 8 5 9 8 3 7 2 7 plus 8 1 7 0 7 5 2 9 2 4 0 0 1 1 9 9 0 0 5 4 6 5 3?\n",
            "Sample correct answer: 1 5 1 3 0 8 8 9 3 8 3 3 5 4 5 0 7 6 0 3 8 3 8 0\n",
            "Sample predicted answer: 1 5 1 4 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 9 2 2\n",
            "Exact match: False\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 5 5 4 5 9 5 1 0 3 plus 2 4 5 1 3 9 7 9 3?\n",
            "Sample correct answer: 7 9 9 7 3 4 8 9 6\n",
            "Sample predicted answer: 7 9 8 8 8 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 6 9 6 3 5 7 1 8 5 3 8 3 4 9 8 0 4 5 1 0 6 1 7 plus 1 0 6 0 1 1 5 9 2 2 7 8 0 1 1 3 7 1 5 6 6 7 3 7?\n",
            "Sample correct answer: 5 7 5 6 4 7 3 1 0 8 1 6 3 6 1 1 7 6 0 7 7 3 5 4\n",
            "Sample predicted answer: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 37, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:03<00:00,  1.06it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0600\n",
            "\n",
            "Epoch 24: 100% 79/79 [00:21<00:00,  3.65it/s, v_num=2, train_loss=1.080, val_exact_match=0.060]Epoch 24, global step 500: 'val_exact_match' was not in top 1\n",
            "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
            "Epoch 24: 100% 79/79 [00:21<00:00,  3.65it/s, v_num=2, train_loss=1.080, val_exact_match=0.060]\n",
            "\n",
            "Evaluating model...\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Testing DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 1 0 8 1 8 6 0 7 2 6 5 1 7 2 0 7 5 2 1 3 7 3 8 9 6 0 3 9 5 7 plus 4 3 1 5 8 1 4 1 5 1 0 5 5 8 8 4 3 3 7 2 0 4 2 0 0 3 4 5 0 8?\n",
            "Sample correct answer: 5 3 9 7 6 7 4 8 7 7 5 7 3 0 9 1 8 5 8 5 7 8 0 9 6 3 8 4 6 5\n",
            "Sample predicted answer: 5 4 0 9 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 9 9 9 9 9 9 9 9 9 9 9 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 40, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 6 2 1 8 5 5 1 3 5 2 6 3 7 4 8 3 2 6 8 7 8 8 0 2 2 1 1 4 8 plus 8 1 0 9 4 3 5 7 7 6 2 3 3 6 6 8 5 5 9 3 1 6 6 6 7 5 0 0 2 9?\n",
            "Sample correct answer: 1 1 7 3 1 2 9 0 9 1 1 4 9 7 4 1 6 8 8 6 1 9 5 4 6 9 7 1 1 7 7\n",
            "Sample predicted answer: 1 1 7 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 6 7 7 7 7 7 7\n",
            "Exact match: False\n",
            "Max answer length: 40, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 0 2 4 4 3 2 8 8 3 2 7 4 6 4 2 8 7 1 4 3 5 1 1 8 7 8 6 0 4 plus 7 5 5 2 9 8 9 8 7 5 9 8 4 4 4 4 9 1 6 1 9 9 7 6 6 4 5 1 5 8?\n",
            "Sample correct answer: 1 6 5 7 7 4 2 2 7 5 9 2 5 9 0 8 7 7 8 7 6 3 4 8 8 5 2 3 7 6 2\n",
            "Sample predicted answer: 1 6 5 7 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 40, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 4/4 [00:05<00:00,  1.27s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 4/4 [00:05<00:00,  1.28s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/character_trained_on_30_digits_seed42/results.json\n",
            "Done!\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.3300.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "================================================================================\n",
            "Running generalization experiment for orthography: decimal\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Training model: decimal, 30 digits, seed 42\n",
            "================================================================================\n",
            "\n",
            "Configuration: Namespace(operation='addition', model_name_or_path='t5-base', train_size=10000, val_size=1000, test_size=1000, min_digits_train=2, min_digits_test=30, base_number=10, train_batch_size=128, val_batch_size=256, max_seq_length=512, num_workers=4, max_epochs=25, check_val_every_n_epoch=10, precision=32, gradient_clip_val=1.0, accumulate_grad_batches=4, optimizer='AdamW', lr=0.0004, weight_decay=5e-05, scheduler='StepLR', gamma=1.0, step_size=1000, balance_train=True, balance_val=True, balance_test=False, invert_question=False, invert_answer=False, orthography='decimal', max_digits_train=30, max_digits_test=30, seed=42, output_dir='./generalized_results/decimal_trained_on_30_digits_seed42')\n",
            "Configuration: Namespace(operation='addition', model_name_or_path='t5-base', train_size=10000, val_size=1000, test_size=1000, min_digits_train=2, min_digits_test=30, base_number=10, train_batch_size=128, val_batch_size=256, max_seq_length=512, num_workers=4, max_epochs=25, check_val_every_n_epoch=10, precision=32, gradient_clip_val=1.0, accumulate_grad_batches=4, optimizer='AdamW', lr=0.0004, weight_decay=5e-05, scheduler='StepLR', gamma=1.0, step_size=1000, balance_train=True, balance_val=True, balance_test=False, invert_question=False, invert_answer=False, orthography='decimal', max_digits_train=30, max_digits_test=30, seed=42, output_dir='./generalized_results/decimal_trained_on_30_digits_seed42')\n",
            "Seed set to 42\n",
            "/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/gpu_optimizations.py:48: UserWarning: Enabled Tensor Core optimizations for NVIDIA A100-SXM4-40GB\n",
            "  warnings.warn(f\"Enabled Tensor Core optimizations for {device_name}\")\n",
            "/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/gpu_optimizations.py:71: UserWarning: Configured TF32 settings for NVIDIA A100-SXM4-40GB (keeping 32-bit precision for stability)\n",
            "  warnings.warn(f\"Configured TF32 settings for {device_name} (keeping 32-bit precision for stability)\")\n",
            "Applied GPU optimization settings\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Creating trainer...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Creating model...\n",
            "Starting training...\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42 exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                       | Params | Mode \n",
            "-------------------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 222 M  | train\n",
            "-------------------------------------------------------------\n",
            "222 M     Trainable params\n",
            "0         Non-trainable params\n",
            "222 M     Total params\n",
            "891.614   Total estimated model params size (MB)\n",
            "541       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 554595103 plus 245139793? What is 554595103 plus 245139793?\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:04<00:00,  2.47s/it]\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "Epoch 0:   0% 0/79 [00:00<?, ?it/s] \n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 41613657270600649 plus 69197743965582618?\n",
            "Sample answer: 110811401236183267\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 4784972278 plus 9631741253?\n",
            "Sample answer: 14416713531\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 61112244709465220 plus 42496625964193579?\n",
            "Sample answer: 103608870673658799\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 109 plus 497?\n",
            "Sample answer: 606\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 1242607090541142906864212 plus 5932838443331448893551874?\n",
            "Sample answer: 7175445533872591800416086\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 2408309842784524253763 plus 2515974791362602513334?\n",
            "Sample answer: 4924284634147126767097\n",
            "Epoch 0:  25% 20/79 [00:02<00:07,  7.47it/s, v_num=0, train_loss=3.210]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 11254439002615353634789538 plus 40412320613632055295317764?\n",
            "Sample answer: 51666759616247408930107302\n",
            "Epoch 0:  76% 60/79 [00:07<00:02,  7.73it/s, v_num=0, train_loss=2.790]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 475908690737757080081807 plus 105062227154947518232564?\n",
            "Sample answer: 580970917892704598314371\n",
            "Epoch 0: 100% 79/79 [00:10<00:00,  7.74it/s, v_num=0, train_loss=2.900]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 11698989898\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 1069898\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 10898989898\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:00<00:00,  4.25it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0010\n",
            "\n",
            "Epoch 0: 100% 79/79 [00:11<00:00,  7.06it/s, v_num=0, train_loss=2.900, val_exact_match=0.001]Epoch 0, global step 20: 'val_exact_match' reached 0.00100 (best 0.00100), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0010.ckpt' as top 1\n",
            "Epoch 1:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.900, val_exact_match=0.001]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 577295779001311503582 plus 603847755180557416888?\n",
            "Sample answer: 1181143534181868920470\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7997420965526137379314050444 plus 7907422407572466367059343996?\n",
            "Sample answer: 15904843373098603746373394440\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1442846875455625621657715459 plus 9191302418589925594647459960?\n",
            "Sample answer: 10634149294045551216305175419\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 7917 plus 4924?\n",
            "Sample answer: 12841\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 49997738590730139204 plus 45491125556435346279?\n",
            "Sample answer: 95488864147165485483\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 85408403 plus 21811852?\n",
            "Sample answer: 107220255\n",
            "Epoch 1:  25% 20/79 [00:02<00:08,  6.77it/s, v_num=0, train_loss=2.550, val_exact_match=0.001]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 18655 plus 65241?\n",
            "Sample answer: 83896\n",
            "Epoch 1:  76% 60/79 [00:08<00:02,  6.87it/s, v_num=0, train_loss=2.450, val_exact_match=0.001]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 803128322643737450870224 plus 291235006534203623572171?\n",
            "Sample answer: 1094363329177941074442395\n",
            "Epoch 1: 100% 79/79 [00:11<00:00,  7.02it/s, v_num=0, train_loss=2.750, val_exact_match=0.001]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 10838383838383838383838\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 106474798\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 108383838383838383838\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.69it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0010\n",
            "\n",
            "Epoch 1: 100% 79/79 [00:12<00:00,  6.19it/s, v_num=0, train_loss=2.750, val_exact_match=0.001]Epoch 1, global step 40: 'val_exact_match' was not in top 1\n",
            "Epoch 2:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.750, val_exact_match=0.001]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 811 plus 479?\n",
            "Sample answer: 1290\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1309 plus 7382?\n",
            "Sample answer: 8691\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 46537392619476996915 plus 31605985328814325604?\n",
            "Sample answer: 78143377948291322519\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 104631822224724704783299 plus 928845260284290372408479?\n",
            "Sample answer: 1033477082509015077191778\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 694116 plus 716493?\n",
            "Sample answer: 1410609\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 4109347298771359 plus 2519129461271661?\n",
            "Sample answer: 6628476760043020\n",
            "Epoch 2:  25% 20/79 [00:02<00:07,  7.59it/s, v_num=0, train_loss=2.760, val_exact_match=0.001]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 893923271339417030890774137 plus 599264386259827346444996887?\n",
            "Sample answer: 1493187657599244377335771024\n",
            "Epoch 2:  76% 60/79 [00:07<00:02,  7.59it/s, v_num=0, train_loss=2.260, val_exact_match=0.001]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8868054898 plus 5646751885?\n",
            "Sample answer: 14514806783\n",
            "Epoch 2: 100% 79/79 [00:10<00:00,  7.63it/s, v_num=0, train_loss=2.940, val_exact_match=0.001]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 108808080808080808021919\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 10647272727\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 108808080808080802383838\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.29it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0010\n",
            "\n",
            "Epoch 2: 100% 79/79 [00:12<00:00,  6.52it/s, v_num=0, train_loss=2.940, val_exact_match=0.001]Epoch 2, global step 60: 'val_exact_match' was not in top 1\n",
            "Epoch 3:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.940, val_exact_match=0.001]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 307375976740036 plus 148121011895816?\n",
            "Sample answer: 455496988635852\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7253159121801974067229408 plus 1697507175519366919147540?\n",
            "Sample answer: 8950666297321340986376948\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 641053307553968520 plus 671658750597020231?\n",
            "Sample answer: 1312712058150988751\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 70325 plus 57287?\n",
            "Sample answer: 127612\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 39921253453462131673651829385 plus 12655115413367730622186131579?\n",
            "Sample answer: 52576368866829862295837960964\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 6006506352738865998698387056 plus 6067893175692140313065398044?\n",
            "Sample answer: 12074399528431006311763785100\n",
            "Epoch 3:  25% 20/79 [00:02<00:07,  7.53it/s, v_num=0, train_loss=2.580, val_exact_match=0.001]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 388115 plus 380025?\n",
            "Sample answer: 768140\n",
            "Epoch 3:  76% 60/79 [00:07<00:02,  7.59it/s, v_num=0, train_loss=2.500, val_exact_match=0.001]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 307639 plus 610102?\n",
            "Sample answer: 917741\n",
            "Epoch 3: 100% 79/79 [00:10<00:00,  7.61it/s, v_num=0, train_loss=2.940, val_exact_match=0.001]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 108585858585858585858\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 106456767\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 1084545696969696969217\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.54it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0000\n",
            "\n",
            "Epoch 3: 100% 79/79 [00:11<00:00,  6.59it/s, v_num=0, train_loss=2.940, val_exact_match=0.000]Epoch 3, global step 80: 'val_exact_match' was not in top 1\n",
            "Epoch 4:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.940, val_exact_match=0.000]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 33392173165527231517941853 plus 29730658511508905867736584?\n",
            "Sample answer: 63122831677036137385678437\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 127959012692451179877473456 plus 902147910170495975042571378?\n",
            "Sample answer: 1030106922862947154920044834\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 3156 plus 1711?\n",
            "Sample answer: 4867\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 2088166771262879568312674567 plus 1312616206886269625285513658?\n",
            "Sample answer: 3400782978149149193598188225\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 61928174245 plus 60407402672?\n",
            "Sample answer: 122335576917\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 53 plus 34?\n",
            "Sample answer: 87\n",
            "Epoch 4:  25% 20/79 [00:02<00:07,  7.59it/s, v_num=0, train_loss=2.490, val_exact_match=0.000]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 9298833167254449701750 plus 7151573147127153914784?\n",
            "Sample answer: 16450406314381603616534\n",
            "Epoch 4:  76% 60/79 [00:07<00:02,  7.65it/s, v_num=0, train_loss=2.450, val_exact_match=0.000]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2456980 plus 5629452?\n",
            "Sample answer: 8086432\n",
            "Epoch 4: 100% 79/79 [00:10<00:00,  7.65it/s, v_num=0, train_loss=3.100, val_exact_match=0.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1086868686868686868\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 106456767\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 8868686868686869692\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.70it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 4: 100% 79/79 [00:11<00:00,  6.68it/s, v_num=0, train_loss=3.100, val_exact_match=0.002]Epoch 4, global step 100: 'val_exact_match' reached 0.00200 (best 0.00200), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=04-val_exact_match=0.0020.ckpt' as top 1\n",
            "Epoch 5:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=3.100, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 556794139600049186628 plus 139917401474343272108?\n",
            "Sample answer: 696711541074392458736\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 81039251 plus 64564323?\n",
            "Sample answer: 145603574\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 288 plus 800?\n",
            "Sample answer: 1088\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 5422063209219274 plus 5465446793519129?\n",
            "Sample answer: 10887510002738403\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 79635185281091821970 plus 99662506812818250083?\n",
            "Sample answer: 179297692093910072053\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 593431090605931772782897365792 plus 793449165309968889218794542862?\n",
            "Sample answer: 1386880255915900662001691908654\n",
            "Epoch 5:  25% 20/79 [00:02<00:07,  7.52it/s, v_num=0, train_loss=2.540, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 374955292 plus 956061995?\n",
            "Sample answer: 1331017287\n",
            "Epoch 5:  76% 60/79 [00:07<00:02,  7.59it/s, v_num=0, train_loss=2.770, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 7612429 plus 3554312?\n",
            "Sample answer: 11166741\n",
            "Epoch 5: 100% 79/79 [00:10<00:00,  7.61it/s, v_num=0, train_loss=2.880, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 129686868686875757575\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 78456767\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 8845292929292929292929\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.48it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0010\n",
            "\n",
            "Epoch 5: 100% 79/79 [00:12<00:00,  6.57it/s, v_num=0, train_loss=2.880, val_exact_match=0.001]Epoch 5, global step 120: 'val_exact_match' was not in top 1\n",
            "Epoch 6:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.880, val_exact_match=0.001]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 592331 plus 423668?\n",
            "Sample answer: 1015999\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 5681431406 plus 5990212370?\n",
            "Sample answer: 11671643776\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 11738618304621 plus 82886198363362?\n",
            "Sample answer: 94624816667983\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3609965 plus 2125505?\n",
            "Sample answer: 5735470\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 246048923476804204166045 plus 664333624811084933964536?\n",
            "Sample answer: 910382548287889138130581\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 86900288735229008765 plus 31630901405942664102?\n",
            "Sample answer: 118531190141171672867\n",
            "Epoch 6:  25% 20/79 [00:02<00:08,  7.28it/s, v_num=0, train_loss=2.390, val_exact_match=0.001]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 96374 plus 66498?\n",
            "Sample answer: 162872\n",
            "Epoch 6:  76% 60/79 [00:08<00:02,  7.27it/s, v_num=0, train_loss=2.740, val_exact_match=0.001]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 4748314304060 plus 8785429519612?\n",
            "Sample answer: 13533743823672\n",
            "Epoch 6: 100% 79/79 [00:10<00:00,  7.36it/s, v_num=0, train_loss=2.200, val_exact_match=0.001]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 149054545353535353535\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7845456712\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 595959593535353535352\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.46it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 6: 100% 79/79 [00:12<00:00,  6.38it/s, v_num=0, train_loss=2.200, val_exact_match=0.002]Epoch 6, global step 140: 'val_exact_match' was not in top 1\n",
            "Epoch 7:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.200, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 6429129903525466758403 plus 2477783945422528391403?\n",
            "Sample answer: 8906913848947995149806\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 10339959137046645978322873 plus 90773284887494003429175132?\n",
            "Sample answer: 101113244024540649407498005\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 9952107824271827 plus 6393638743911484?\n",
            "Sample answer: 16345746568183311\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 92360715026467659775138462082 plus 23665999081704192633436937445?\n",
            "Sample answer: 116026714108171852408575399527\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 410280700556107390320556 plus 970928120838618909123757?\n",
            "Sample answer: 1381208821394726299444313\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 368300507129 plus 915476979831?\n",
            "Sample answer: 1283777486960\n",
            "Epoch 7:  25% 20/79 [00:02<00:07,  7.54it/s, v_num=0, train_loss=2.300, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 82934654687740081848219 plus 24670040844576097837363?\n",
            "Sample answer: 107604695532316179685582\n",
            "Epoch 7:  76% 60/79 [00:08<00:02,  7.48it/s, v_num=0, train_loss=2.430, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 96832044246255013269810771525 plus 22875652201962435307670711110?\n",
            "Sample answer: 119707696448217448577481482635\n",
            "Epoch 7: 100% 79/79 [00:10<00:00,  7.53it/s, v_num=0, train_loss=2.450, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 14608082828282828282828\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909099713\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5959595959595959353535352\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.25it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0030\n",
            "\n",
            "Epoch 7: 100% 79/79 [00:12<00:00,  6.43it/s, v_num=0, train_loss=2.450, val_exact_match=0.003]Epoch 7, global step 160: 'val_exact_match' reached 0.00300 (best 0.00300), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=07-val_exact_match=0.0030.ckpt' as top 1\n",
            "Epoch 8:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.450, val_exact_match=0.003]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 546804805881792733027240896 plus 976998552498015926090418278?\n",
            "Sample answer: 1523803358379808659117659174\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 59349051169763 plus 46339330743232?\n",
            "Sample answer: 105688381912995\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6331455190615 plus 6385181798686?\n",
            "Sample answer: 12716636989301\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 896707765747853337345050281 plus 911671807416947634670157183?\n",
            "Sample answer: 1808379573164800972015207464\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 63418176228760869967 plus 90484473456033958244?\n",
            "Sample answer: 153902649684794828211\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 4232557668 plus 4103801478?\n",
            "Sample answer: 8336359146\n",
            "Epoch 8:  25% 20/79 [00:05<00:15,  3.86it/s, v_num=0, train_loss=2.500, val_exact_match=0.003]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 936694101228 plus 765128612070?\n",
            "Sample answer: 1701822713298\n",
            "Epoch 8:  76% 60/79 [00:10<00:03,  5.79it/s, v_num=0, train_loss=2.420, val_exact_match=0.003]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 554464616346191806474633 plus 539198580859695129417360?\n",
            "Sample answer: 1093663197205886935891993\n",
            "Epoch 8: 100% 79/79 [00:12<00:00,  6.13it/s, v_num=0, train_loss=2.130, val_exact_match=0.003]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1537474747445456767902\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79090997\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5747474454569696969692\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.12it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 8: 100% 79/79 [00:14<00:00,  5.34it/s, v_num=0, train_loss=2.130, val_exact_match=0.002]Epoch 8, global step 180: 'val_exact_match' was not in top 1\n",
            "Epoch 9:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.130, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 7255021677378831 plus 6862193125182940?\n",
            "Sample answer: 14117214802561771\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 85482166536166208 plus 34057885740341238?\n",
            "Sample answer: 119540052276507446\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 6054039371 plus 5173010454?\n",
            "Sample answer: 11227049825\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 7128357236153399564526591048 plus 6853603719861122375408439189?\n",
            "Sample answer: 13981960956014521939935030237\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 261 plus 334?\n",
            "Sample answer: 595\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 445532283 plus 366570563?\n",
            "Sample answer: 812102846\n",
            "Epoch 9:  25% 20/79 [00:03<00:09,  6.15it/s, v_num=0, train_loss=2.470, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 5029962216825433271 plus 6767687595880347806?\n",
            "Sample answer: 11797649812705781077\n",
            "Epoch 9:  76% 60/79 [00:08<00:02,  6.97it/s, v_num=0, train_loss=2.230, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 983377668648993028079 plus 826074224085913741322?\n",
            "Sample answer: 1809451892734906769401\n",
            "Epoch 9: 100% 79/79 [00:11<00:00,  7.12it/s, v_num=0, train_loss=2.880, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 149083838383838383838\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79459893\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5696868686868696969692\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.34it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 9: 100% 79/79 [00:12<00:00,  6.16it/s, v_num=0, train_loss=2.880, val_exact_match=0.002]Epoch 9, global step 200: 'val_exact_match' was not in top 1\n",
            "Epoch 10:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.880, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 418908340165138 plus 798480661911441?\n",
            "Sample answer: 1217389002076579\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 41878285856807221126187699 plus 76453235231059557792105903?\n",
            "Sample answer: 118331521087866778918293602\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 88193737479 plus 41845602616?\n",
            "Sample answer: 130039340095\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9985 plus 4969?\n",
            "Sample answer: 14954\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 144332652131482637258 plus 653295238717836278098?\n",
            "Sample answer: 797627890849318915356\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 6359899 plus 6783115?\n",
            "Sample answer: 13143014\n",
            "Epoch 10:  25% 20/79 [00:02<00:07,  7.58it/s, v_num=0, train_loss=2.370, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 409778086443431425145467680029 plus 471725608795942343311203201784?\n",
            "Sample answer: 881503695239373768456670881813\n",
            "Epoch 10:  76% 60/79 [00:07<00:02,  7.57it/s, v_num=0, train_loss=2.370, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 38043072 plus 57655811?\n",
            "Sample answer: 95698883\n",
            "Epoch 10: 100% 79/79 [00:10<00:00,  7.60it/s, v_num=0, train_loss=2.730, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1556868686868686868686868\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79454597\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5696868686868686869696969\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.34it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 10: 100% 79/79 [00:12<00:00,  6.51it/s, v_num=0, train_loss=2.730, val_exact_match=0.002]Epoch 10, global step 220: 'val_exact_match' was not in top 1\n",
            "Epoch 11:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.730, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 85916844278934033375791 plus 20133485028795691924549?\n",
            "Sample answer: 106050329307729725300340\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 41091 plus 59986?\n",
            "Sample answer: 101077\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 5544751332036264 plus 7344213944383244?\n",
            "Sample answer: 12888965276419508\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 6142608021836684165 plus 2648397369261359567?\n",
            "Sample answer: 8791005391098043732\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 411771905527006131 plus 145047751386953697?\n",
            "Sample answer: 556819656913959828\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 11 plus 74?\n",
            "Sample answer: 85\n",
            "Epoch 11:  25% 20/79 [00:02<00:07,  7.49it/s, v_num=0, train_loss=2.360, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 3676 plus 1007?\n",
            "Sample answer: 4683\n",
            "Epoch 11:  76% 60/79 [00:08<00:02,  7.49it/s, v_num=0, train_loss=2.520, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 2977479391513415017 plus 8185840665188935902?\n",
            "Sample answer: 11163320056702350919\n",
            "Epoch 11: 100% 79/79 [00:10<00:00,  7.53it/s, v_num=0, train_loss=2.340, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 153686868686835353593\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79456494\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 569683838383838383838\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.57it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0020\n",
            "\n",
            "Epoch 11: 100% 79/79 [00:12<00:00,  6.55it/s, v_num=0, train_loss=2.340, val_exact_match=0.002]Epoch 11, global step 240: 'val_exact_match' was not in top 1\n",
            "Epoch 12:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.340, val_exact_match=0.002]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 956295553568578346 plus 479457160569794308?\n",
            "Sample answer: 1435752714138372654\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1371671831757349048553606 plus 2003833102360689547904918?\n",
            "Sample answer: 3375504934118038596458524\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 151782376967475012518 plus 735664191637731390416?\n",
            "Sample answer: 887446568605206402934\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3341455729945 plus 1845831310336?\n",
            "Sample answer: 5187287040281\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 564085460 plus 535067922?\n",
            "Sample answer: 1099153382\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1377921490348086780147169497 plus 2256852289733117002498105758?\n",
            "Sample answer: 3634773780081203782645275255\n",
            "Epoch 12:  25% 20/79 [00:02<00:07,  7.40it/s, v_num=0, train_loss=2.210, val_exact_match=0.002]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 217513312700 plus 104688044575?\n",
            "Sample answer: 322201357275\n",
            "Epoch 12:  76% 60/79 [00:07<00:02,  7.56it/s, v_num=0, train_loss=2.400, val_exact_match=0.002]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 25907266612829574 plus 88728001570143418?\n",
            "Sample answer: 114635268182972992\n",
            "Epoch 12: 100% 79/79 [00:10<00:00,  7.58it/s, v_num=0, train_loss=2.490, val_exact_match=0.002]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15282828282828282828215\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79458694\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 56969696969696969696980\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.48it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0050\n",
            "\n",
            "Epoch 12: 100% 79/79 [00:12<00:00,  6.55it/s, v_num=0, train_loss=2.490, val_exact_match=0.005]Epoch 12, global step 260: 'val_exact_match' reached 0.00500 (best 0.00500), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=12-val_exact_match=0.0050.ckpt' as top 1\n",
            "Epoch 13:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.490, val_exact_match=0.005]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 986181809895 plus 544259229637?\n",
            "Sample answer: 1530441039532\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 763082140576615665202192790 plus 574184850694537952439047914?\n",
            "Sample answer: 1337266991271153617641240704\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 89011268177520218750012319939 plus 62911719384291070223898912824?\n",
            "Sample answer: 151922987561811288973911232763\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3554050414048 plus 1249890527980?\n",
            "Sample answer: 4803940942028\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2960130450817121288861253689 plus 9008338867819451607097043596?\n",
            "Sample answer: 11968469318636572895958297285\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 9916585964745344064 plus 4223735529798698110?\n",
            "Sample answer: 14140321494544042174\n",
            "Epoch 13:  25% 20/79 [00:03<00:09,  5.99it/s, v_num=0, train_loss=2.390, val_exact_match=0.005]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 76577130872294629170266 plus 42727806404694932952780?\n",
            "Sample answer: 119304937276989562123046\n",
            "Epoch 13:  76% 60/79 [00:08<00:02,  7.11it/s, v_num=0, train_loss=2.340, val_exact_match=0.005]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 5660378755 plus 7450281157?\n",
            "Sample answer: 13110659912\n",
            "Epoch 13: 100% 79/79 [00:10<00:00,  7.24it/s, v_num=0, train_loss=2.540, val_exact_match=0.005]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15302727276868353529517\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 84545199798\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 584567670707171717171718\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.38it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0030\n",
            "\n",
            "Epoch 13: 100% 79/79 [00:12<00:00,  6.26it/s, v_num=0, train_loss=2.540, val_exact_match=0.003]Epoch 13, global step 280: 'val_exact_match' was not in top 1\n",
            "Epoch 14:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.540, val_exact_match=0.003]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 116403496131216 plus 603365700889336?\n",
            "Sample answer: 719769197020552\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 7563413950732 plus 6639917817896?\n",
            "Sample answer: 14203331768628\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 1876619737017243 plus 9188781123497052?\n",
            "Sample answer: 11065400860514295\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 5202034776145930610014669356 plus 8578323180447515528074934615?\n",
            "Sample answer: 13780357956593446138089603971\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 106778439 plus 621373414?\n",
            "Sample answer: 728151853\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 65063157064980580387590541 plus 61269000706913776843636686?\n",
            "Sample answer: 126332157771894357231227227\n",
            "Epoch 14:  25% 20/79 [00:02<00:07,  7.53it/s, v_num=0, train_loss=2.460, val_exact_match=0.003]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 20247509865078292980420884993 plus 46577243034972288435832800442?\n",
            "Sample answer: 66824752900050581416253685435\n",
            "Epoch 14:  76% 60/79 [00:07<00:02,  7.62it/s, v_num=0, train_loss=2.240, val_exact_match=0.003]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 899665 plus 610326?\n",
            "Sample answer: 1509991\n",
            "Epoch 14: 100% 79/79 [00:10<00:00,  7.60it/s, v_num=0, train_loss=2.350, val_exact_match=0.003]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15307980806767673535215\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909079794\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 569352727282835353521919\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.32it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0070\n",
            "\n",
            "Epoch 14: 100% 79/79 [00:12<00:00,  6.51it/s, v_num=0, train_loss=2.350, val_exact_match=0.007]Epoch 14, global step 300: 'val_exact_match' reached 0.00700 (best 0.00700), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=14-val_exact_match=0.0070.ckpt' as top 1\n",
            "Epoch 15:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.350, val_exact_match=0.007]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 2061877473835569433903922041 plus 2718356448509660289435882403?\n",
            "Sample answer: 4780233922345229723339804444\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 61 plus 71?\n",
            "Sample answer: 132\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 27466246 plus 65246410?\n",
            "Sample answer: 92712656\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3137981599259844200 plus 2166083873403536780?\n",
            "Sample answer: 5304065472663380980\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 3027788 plus 5155291?\n",
            "Sample answer: 8183079\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 302762613239109277243144024 plus 272776418492539249882848258?\n",
            "Sample answer: 575539031731648527125992282\n",
            "Epoch 15:  25% 20/79 [00:05<00:16,  3.57it/s, v_num=0, train_loss=2.500, val_exact_match=0.007]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 28172482271030 plus 68886305676349?\n",
            "Sample answer: 97058787947379\n",
            "Epoch 15:  76% 60/79 [00:10<00:03,  5.47it/s, v_num=0, train_loss=2.150, val_exact_match=0.007]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 388980197 plus 951962057?\n",
            "Sample answer: 1340942254\n",
            "Epoch 15: 100% 79/79 [00:13<00:00,  5.84it/s, v_num=0, train_loss=2.610, val_exact_match=0.007]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15302727276868353538382\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909099798\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 586767070707171717171708\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.29it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0060\n",
            "\n",
            "Epoch 15: 100% 79/79 [00:15<00:00,  5.16it/s, v_num=0, train_loss=2.610, val_exact_match=0.006]Epoch 15, global step 320: 'val_exact_match' was not in top 1\n",
            "Epoch 16:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.610, val_exact_match=0.006]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 9893164929 plus 8692551359?\n",
            "Sample answer: 18585716288\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 708646894 plus 370940044?\n",
            "Sample answer: 1079586938\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 757428 plus 857649?\n",
            "Sample answer: 1615077\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 28791 plus 61748?\n",
            "Sample answer: 90539\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 170749941099745 plus 689685077673004?\n",
            "Sample answer: 860435018772749\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 1415933931147 plus 7140835129360?\n",
            "Sample answer: 8556769060507\n",
            "Epoch 16:  25% 20/79 [00:02<00:08,  7.01it/s, v_num=0, train_loss=2.290, val_exact_match=0.006]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 8376741307821801607749382 plus 3040903962798698670429837?\n",
            "Sample answer: 11417645270620500278179219\n",
            "Epoch 16:  76% 60/79 [00:08<00:02,  7.23it/s, v_num=0, train_loss=2.320, val_exact_match=0.006]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 220502665328 plus 171660510716?\n",
            "Sample answer: 392163176044\n",
            "Epoch 16: 100% 79/79 [00:10<00:00,  7.26it/s, v_num=0, train_loss=2.170, val_exact_match=0.006]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 153027272735353535295\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 79459498\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5696835353838353535298\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.44it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0060\n",
            "\n",
            "Epoch 16: 100% 79/79 [00:12<00:00,  6.30it/s, v_num=0, train_loss=2.170, val_exact_match=0.006]Epoch 16, global step 340: 'val_exact_match' was not in top 1\n",
            "Epoch 17:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.170, val_exact_match=0.006]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 5752254684547473657299747672 plus 7516578855110665869439667607?\n",
            "Sample answer: 13268833539658139526739415279\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 78576 plus 96362?\n",
            "Sample answer: 174938\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 8762357 plus 7937806?\n",
            "Sample answer: 16700163\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 391248 plus 239430?\n",
            "Sample answer: 630678\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 244269030609848391400 plus 128791557723817576525?\n",
            "Sample answer: 373060588333665967925\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3376688355376563833791322210 plus 2799576259149459419364237971?\n",
            "Sample answer: 6176264614526023253155560181\n",
            "Epoch 17:  25% 20/79 [00:02<00:08,  7.15it/s, v_num=0, train_loss=2.420, val_exact_match=0.006]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 4301597271634000665697 plus 4647278080498923319658?\n",
            "Sample answer: 8948875352132923985355\n",
            "Epoch 17:  76% 60/79 [00:08<00:02,  7.47it/s, v_num=0, train_loss=2.320, val_exact_match=0.006]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 8350338249904664136192958779 plus 5333135504436188815155825989?\n",
            "Sample answer: 13683473754340852951348784768\n",
            "Epoch 17: 100% 79/79 [00:10<00:00,  7.53it/s, v_num=0, train_loss=2.410, val_exact_match=0.006]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15302727683535353593902\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7945679798\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 58686875756868686835352\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.49it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0120\n",
            "\n",
            "Epoch 17: 100% 79/79 [00:12<00:00,  6.52it/s, v_num=0, train_loss=2.410, val_exact_match=0.012]Epoch 17, global step 360: 'val_exact_match' reached 0.01200 (best 0.01200), saving model to '/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt' as top 1\n",
            "Epoch 18:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.410, val_exact_match=0.012]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 808359316151005 plus 516846862717757?\n",
            "Sample answer: 1325206178868762\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 703863925617442835940451 plus 551964192863607504623675?\n",
            "Sample answer: 1255828118481050340564126\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 76849171351942814949992414 plus 88236110389793627893469513?\n",
            "Sample answer: 165085281741736442843461927\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 63411103196385573971780071 plus 22609580606863811181635939?\n",
            "Sample answer: 86020683803249385153416010\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 29309473306117532 plus 95355870354223994?\n",
            "Sample answer: 124665343660341526\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 10194555281472750246265 plus 49900460033241908789859?\n",
            "Sample answer: 60095015314714659036124\n",
            "Epoch 18:  25% 20/79 [00:03<00:11,  5.33it/s, v_num=0, train_loss=2.580, val_exact_match=0.012]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 87875353 plus 88028056?\n",
            "Sample answer: 175903409\n",
            "Epoch 18:  76% 60/79 [00:08<00:02,  6.70it/s, v_num=0, train_loss=2.280, val_exact_match=0.012]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 86446429496436 plus 35557074676231?\n",
            "Sample answer: 122003504172667\n",
            "Epoch 18: 100% 79/79 [00:11<00:00,  6.76it/s, v_num=0, train_loss=1.780, val_exact_match=0.012]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 15302727353535353529595\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909099796\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5696975357835936935354\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.25it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0090\n",
            "\n",
            "Epoch 18: 100% 79/79 [00:13<00:00,  5.86it/s, v_num=0, train_loss=1.780, val_exact_match=0.009]Epoch 18, global step 380: 'val_exact_match' was not in top 1\n",
            "Epoch 19:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=1.780, val_exact_match=0.009]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 86171006696 plus 39227292757?\n",
            "Sample answer: 125398299453\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 724079542074912153809134486230 plus 740669466133017626899519234994?\n",
            "Sample answer: 1464749008207929780708653721224\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 930 plus 824?\n",
            "Sample answer: 1754\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 3678834370945929671 plus 5223983299558101146?\n",
            "Sample answer: 8902817670504030817\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 566944 plus 286627?\n",
            "Sample answer: 853571\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 315744241133478697996032 plus 432981850766117204008544?\n",
            "Sample answer: 748726091899595902004576\n",
            "Epoch 19:  25% 20/79 [00:02<00:08,  7.07it/s, v_num=0, train_loss=2.240, val_exact_match=0.009]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 765383787826 plus 824626262735?\n",
            "Sample answer: 1590010050561\n",
            "Epoch 19:  76% 60/79 [00:08<00:02,  7.32it/s, v_num=0, train_loss=2.400, val_exact_match=0.009]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 469718127222802263259 plus 145542694110072361753?\n",
            "Sample answer: 615260821332874625012\n",
            "Epoch 19: 100% 79/79 [00:10<00:00,  7.36it/s, v_num=0, train_loss=2.590, val_exact_match=0.009]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1530272768595935353529595\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909179798\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 59865868659595959353535295\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.20it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0060\n",
            "\n",
            "Epoch 19: 100% 79/79 [00:12<00:00,  6.28it/s, v_num=0, train_loss=2.590, val_exact_match=0.006]Epoch 19, global step 400: 'val_exact_match' was not in top 1\n",
            "Epoch 20:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.590, val_exact_match=0.006]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 120520 plus 443279?\n",
            "Sample answer: 563799\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 863404073 plus 506370246?\n",
            "Sample answer: 1369774319\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 7785 plus 9294?\n",
            "Sample answer: 17079\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 90275382073415522 plus 97087293137972861?\n",
            "Sample answer: 187362675211388383\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 348 plus 818?\n",
            "Sample answer: 1166\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 21001862827073629229602179488 plus 39190023787186128972392381298?\n",
            "Sample answer: 60191886614259758201994560786\n",
            "Epoch 20:  25% 20/79 [00:02<00:07,  7.54it/s, v_num=0, train_loss=2.230, val_exact_match=0.006]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 54821242665130 plus 98546180100701?\n",
            "Sample answer: 153367422765831\n",
            "Epoch 20:  76% 60/79 [00:07<00:02,  7.59it/s, v_num=0, train_loss=2.390, val_exact_match=0.006]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 29651 plus 54550?\n",
            "Sample answer: 84201\n",
            "Epoch 20: 100% 79/79 [00:10<00:00,  7.54it/s, v_num=0, train_loss=2.160, val_exact_match=0.006]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1490707070717171735259598\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909099496\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 586963646707071717171789\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.24it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0100\n",
            "\n",
            "Epoch 20: 100% 79/79 [00:12<00:00,  6.43it/s, v_num=0, train_loss=2.160, val_exact_match=0.010]Epoch 20, global step 420: 'val_exact_match' was not in top 1\n",
            "Epoch 21:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.160, val_exact_match=0.010]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 934610009683684429 plus 468934884422144251?\n",
            "Sample answer: 1403544894105828680\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 1605005 plus 7035435?\n",
            "Sample answer: 8640440\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 22063561390569080042493167057 plus 37887943078165152163483021998?\n",
            "Sample answer: 59951504468734232205976189055\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 857117 plus 709479?\n",
            "Sample answer: 1566596\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 12828658896241706 plus 74782124994764465?\n",
            "Sample answer: 87610783891006171\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 121397518 plus 665269791?\n",
            "Sample answer: 786667309\n",
            "Epoch 21:  25% 20/79 [00:02<00:07,  7.55it/s, v_num=0, train_loss=2.170, val_exact_match=0.010]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 360046848 plus 268609868?\n",
            "Sample answer: 628656716\n",
            "Epoch 21:  76% 60/79 [00:08<00:02,  7.34it/s, v_num=0, train_loss=1.890, val_exact_match=0.010]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 919544918874981881598190 plus 306580032281935870887423?\n",
            "Sample answer: 1226124951156917752485613\n",
            "Epoch 21: 100% 79/79 [00:10<00:00,  7.36it/s, v_num=0, train_loss=2.510, val_exact_match=0.010]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1490707087576353585859594\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 884849796\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 5696378456868683535936790\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.17it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0110\n",
            "\n",
            "Epoch 21: 100% 79/79 [00:12<00:00,  6.27it/s, v_num=0, train_loss=2.510, val_exact_match=0.011]Epoch 21, global step 440: 'val_exact_match' was not in top 1\n",
            "Epoch 22:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.510, val_exact_match=0.011]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 3126717268759904 plus 4266785864239273?\n",
            "Sample answer: 7393503132999177\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 8124064657339078584301 plus 3833441356409654911765?\n",
            "Sample answer: 11957506013748733496066\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 66669500 plus 28028246?\n",
            "Sample answer: 94697746\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 55659873 plus 91424562?\n",
            "Sample answer: 147084435\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 321801499 plus 934754467?\n",
            "Sample answer: 1256555966\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 3653695 plus 2679655?\n",
            "Sample answer: 6333350\n",
            "Epoch 22:  25% 20/79 [00:02<00:07,  7.49it/s, v_num=0, train_loss=2.390, val_exact_match=0.011]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 10655304234568 plus 34632758890770?\n",
            "Sample answer: 45288063125338\n",
            "Epoch 22:  76% 60/79 [00:07<00:02,  7.56it/s, v_num=0, train_loss=2.070, val_exact_match=0.011]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 49981432240593362255842077311 plus 73130645175748912716994807194?\n",
            "Sample answer: 123112077416342274972836884505\n",
            "Epoch 22: 100% 79/79 [00:10<00:00,  7.55it/s, v_num=0, train_loss=2.290, val_exact_match=0.011]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 149070745767638383585989898\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 7909099898\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 56963574568356969353529589\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.07it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0080\n",
            "\n",
            "Epoch 22: 100% 79/79 [00:12<00:00,  6.37it/s, v_num=0, train_loss=2.290, val_exact_match=0.008]Epoch 22, global step 460: 'val_exact_match' was not in top 1\n",
            "Epoch 23:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.290, val_exact_match=0.008]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 75990558 plus 29915816?\n",
            "Sample answer: 105906374\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 31996692594788992290 plus 50108927519543605763?\n",
            "Sample answer: 82105620114332598053\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 27466246 plus 65246410?\n",
            "Sample answer: 92712656\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 268375501840850088537053 plus 278623414427591761075793?\n",
            "Sample answer: 546998916268441849612846\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 795825167356450632506037 plus 316436038334194424401506?\n",
            "Sample answer: 1112261205690645056907543\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 9955 plus 6618?\n",
            "Sample answer: 16573\n",
            "Epoch 23:  25% 20/79 [00:02<00:07,  7.58it/s, v_num=0, train_loss=2.170, val_exact_match=0.008]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 43402780615067731688 plus 53026958111975241835?\n",
            "Sample answer: 96429738727042973523\n",
            "Epoch 23:  76% 60/79 [00:07<00:02,  7.52it/s, v_num=0, train_loss=2.310, val_exact_match=0.008]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 523259720332846037357178756474 plus 853266455865637611633577402775?\n",
            "Sample answer: 1376526176198483648990756159249\n",
            "Epoch 23: 100% 79/79 [00:10<00:00,  7.53it/s, v_num=0, train_loss=2.270, val_exact_match=0.008]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 151517685935353580802\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 84849496\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 56963574568356935352\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.60it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0110\n",
            "\n",
            "Epoch 23: 100% 79/79 [00:12<00:00,  6.55it/s, v_num=0, train_loss=2.270, val_exact_match=0.011]Epoch 23, global step 480: 'val_exact_match' was not in top 1\n",
            "Epoch 24:   0% 0/79 [00:00<?, ?it/s, v_num=0, train_loss=2.270, val_exact_match=0.011]\n",
            "=== Training Batch 0 ===\n",
            "\n",
            "Sample question: What is 459224229250288002307805 plus 867212518883162133907442?\n",
            "Sample answer: 1326436748133450136215247\n",
            "\n",
            "=== Training Batch 1 ===\n",
            "\n",
            "Sample question: What is 33025241026857686417 plus 20428680014904037679?\n",
            "Sample answer: 53453921041761724096\n",
            "\n",
            "=== Training Batch 2 ===\n",
            "\n",
            "Sample question: What is 52505809197 plus 26611894782?\n",
            "Sample answer: 79117703979\n",
            "\n",
            "=== Training Batch 4 ===\n",
            "\n",
            "Sample question: What is 9432459 plus 2849490?\n",
            "Sample answer: 12281949\n",
            "\n",
            "=== Training Batch 8 ===\n",
            "\n",
            "Sample question: What is 2241176300532644268556987905 plus 7361413287458401291202322634?\n",
            "Sample answer: 9602589587991045559759310539\n",
            "\n",
            "=== Training Batch 16 ===\n",
            "\n",
            "Sample question: What is 7071 plus 6400?\n",
            "Sample answer: 13471\n",
            "Epoch 24:  25% 20/79 [00:02<00:07,  7.77it/s, v_num=0, train_loss=2.270, val_exact_match=0.011]\n",
            "=== Training Batch 32 ===\n",
            "\n",
            "Sample question: What is 7839450738 plus 2846223183?\n",
            "Sample answer: 10685673921\n",
            "Epoch 24:  76% 60/79 [00:07<00:02,  7.85it/s, v_num=0, train_loss=2.240, val_exact_match=0.011]\n",
            "=== Training Batch 64 ===\n",
            "\n",
            "Sample question: What is 9452188976013046064590 plus 9033950647453775027310?\n",
            "Sample answer: 18486139623466821091900\n",
            "Epoch 24: 100% 79/79 [00:10<00:00,  7.80it/s, v_num=0, train_loss=2.310, val_exact_match=0.011]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 69601364593533085983727 plus 81707529240011990054653?\n",
            "Sample correct answer: 151308893833545076038380\n",
            "Sample predicted answer: 1517173838353535858554\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 554595103 plus 245139793?\n",
            "Sample correct answer: 799734896\n",
            "Sample predicted answer: 84849494\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 469635718538349804510617 plus 106011592278011371566737?\n",
            "Sample correct answer: 575647310816361176077354\n",
            "Sample predicted answer: 56963590707171717176468\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "Validation DataLoader 0: 100% 4/4 [00:01<00:00,  2.54it/s]\u001b[A\n",
            "=== Validation Epoch End ===\n",
            "\n",
            "Validation Exact Match: 0.0110\n",
            "\n",
            "Epoch 24: 100% 79/79 [00:11<00:00,  6.73it/s, v_num=0, train_loss=2.310, val_exact_match=0.011]Epoch 24, global step 500: 'val_exact_match' was not in top 1\n",
            "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
            "Epoch 24: 100% 79/79 [00:11<00:00,  6.73it/s, v_num=0, train_loss=2.310, val_exact_match=0.011]\n",
            "\n",
            "Evaluating model...\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at /content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Testing DataLoader 0:   0% 0/4 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 108186072651720752137389603957 plus 431581415105588433720420034508?\n",
            "Sample correct answer: 539767487757309185857809638465\n",
            "Sample predicted answer: 534983568356767676767676767\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 362185513526374832687880221148 plus 810943577623366855931666750029?\n",
            "Sample correct answer: 1173129091149741688619546971177\n",
            "Sample predicted answer: 116845763566666680802959595\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 902443288327464287143511878604 plus 755298987598444491619976645158?\n",
            "Sample correct answer: 1657742275925908778763488523762\n",
            "Sample predicted answer: 1650869090696980939393939319\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 4/4 [00:01<00:00,  2.21it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 4/4 [00:01<00:00,  2.20it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/decimal_trained_on_30_digits_seed42/results.json\n",
            "Done!\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Error testing model: 'dict' object has no attribute 'model_name_or_path'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/generalization_runner.py\", line 231, in test_model_on_digit_length\n",
            "    result = evaluate_model(args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/evaluate.py\", line 106, in evaluate_model\n",
            "    model = ArithmeticTransformer.load_from_checkpoint(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/model_helpers.py\", line 125, in wrapper\n",
            "    return self.method(cls, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1581, in load_from_checkpoint\n",
            "    loaded = _load_from_checkpoint(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 91, in _load_from_checkpoint\n",
            "    model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py\", line 165, in _load_state\n",
            "    obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Cornell/CS5782/5782_Final_Project/code/model.py\", line 57, in __init__\n",
            "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name_or_path)\n",
            "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'model_name_or_path'\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Plot saved to ./generalized_results/generalization_results.png\n",
            "Figure(1000x600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "wr23lD-KDCED"
      },
      "outputs": [],
      "source": [
        "# !python code/experiment_runner.py --output_dir=./experiment_results --plot_only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate_generalization.py --base_dir=./generalized_results"
      ],
      "metadata": {
        "id": "IzoZZ1SeuOFI",
        "outputId": "49a440e9-d0d5-4056-f7ea-bca3d0ca72a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found models for orthographies: ['10ebased', 'words', 'underscore', 'character_fixed', 'character', '10based', 'decimal']\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "2025-05-06 08:48:44.886010: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-06 08:48:44.903419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746521324.923164   51864 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746521324.929098   51864 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-06 08:48:44.949400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 10e1 5 10e0 plus 5 10e1 8 10e0?\n",
            "Sample correct answer: 9 10e1 3 10e0\n",
            "Sample predicted answer: 9 10e1 3 10e0\n",
            "Exact match: True\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 10e1 4 10e0 plus 4 10e1 6 10e0?\n",
            "Sample correct answer: 6 10e1 0 10e0\n",
            "Sample predicted answer: 6 10e1 0 10e0\n",
            "Exact match: True\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 10e1 9 10e0 plus 3 10e1 3 10e0?\n",
            "Sample correct answer: 8 10e1 2 10e0\n",
            "Sample predicted answer: 8 10e1 2 10e0\n",
            "Exact match: True\n",
            "Max answer length: 14, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 10e1 5 10e0 plus 5 10e1 2 10e0?\n",
            "Sample correct answer: 1 10e2 3 10e1 7 10e0\n",
            "Sample predicted answer: 1 10e2 3 10e1 7 10e0\n",
            "Exact match: True\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 10e1 9 10e0 plus 2 10e1 8 10e0?\n",
            "Sample correct answer: 9 10e1 7 10e0\n",
            "Sample predicted answer: 9 10e1 7 10e0\n",
            "Exact match: True\n",
            "Max answer length: 14, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  3.12it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8260\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  3.10it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8259999752044678    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 10e4 6 10e3 4 10e2 3 10e1 4 10e0 plus 5 10e4 9 10e3 4 10e2 2 10e1 9 10e0?\n",
            "Sample correct answer: 9 10e4 5 10e3 8 10e2 6 10e1 3 10e0\n",
            "Sample predicted answer: 9 10e4 5 10e3 8 10e2 6 10e1 3 10e0\n",
            "Exact match: True\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 10e3 5 10e2 9 10e1 2 10e0 plus 3 10e4 1 10e3 5 10e2 7 10e1 1 10e0?\n",
            "Sample correct answer: 3 10e4 9 10e3 1 10e2 6 10e1 3 10e0\n",
            "Sample predicted answer: 1 10e5 1 10e4 3 10e3 1 10e2 6 10e1 3 10e0\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 10e4 1 10e3 3 10e2 0 10e1 3 10e0 plus 9 10e4 5 10e3 7 10e2 4 10e1 8 10e0?\n",
            "Sample correct answer: 1 10e5 8 10e4 7 10e3 0 10e2 5 10e1 1 10e0\n",
            "Sample predicted answer: 1 10e5 8 10e4 7 10e3 0 10e2 5 10e1 1 10e0\n",
            "Exact match: True\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 10e4 5 10e3 1 10e2 4 10e1 4 10e0 plus 5 10e4 5 10e3 1 10e2 0 10e1 8 10e0?\n",
            "Sample correct answer: 8 10e4 0 10e3 2 10e2 5 10e1 2 10e0\n",
            "Sample predicted answer: 8 10e4 0 10e3 2 10e2 5 10e1 2 10e0\n",
            "Exact match: True\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 10e4 5 10e3 5 10e2 9 10e1 7 10e0 plus 7 10e4 0 10e3 7 10e2 7 10e1 6 10e0?\n",
            "Sample correct answer: 1 10e5 3 10e4 6 10e3 3 10e2 7 10e1 3 10e0\n",
            "Sample predicted answer: 1 10e5 3 10e4 6 10e3 3 10e2 7 10e1 3 10e0\n",
            "Exact match: True\n",
            "Max answer length: 28, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:08<00:00,  1.90it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8290\n",
            "Testing DataLoader 0: 100% 16/16 [00:08<00:00,  1.90it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8289999961853027    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 10e9 1 10e8 8 10e7 4 10e6 0 10e5 6 10e4 4 10e3 9 10e2 0 10e1 0 10e0 plus 2 10e9 1 10e8 1 10e7 9 10e6 6 10e5 3 10e4 4 10e3 3 10e2 9 10e1 9 10e0?\n",
            "Sample correct answer: 1 10e10 1 10e9 3 10e8 0 10e7 3 10e6 6 10e5 9 10e4 9 10e3 2 10e2 9 10e1 9 10e0\n",
            "Sample predicted answer: 1 10e10 1 10e9 3 10e8 0 10e7 3 10e6 6 10e5 9 10e4 9 10e3 2 10e2 9 10e1 9 10e0\n",
            "Exact match: True\n",
            "Max answer length: 49, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 10e9 8 10e8 4 10e7 5 10e6 9 10e5 1 10e4 9 10e3 8 10e2 9 10e1 5 10e0 plus 4 10e9 4 10e8 7 10e7 4 10e6 6 10e5 2 10e4 0 10e3 8 10e2 9 10e1 4 10e0?\n",
            "Sample correct answer: 6 10e9 3 10e8 2 10e7 0 10e6 5 10e5 4 10e4 0 10e3 7 10e2 8 10e1 9 10e0\n",
            "Sample predicted answer: 6 10e9 3 10e8 2 10e7 0 10e6 5 10e5 4 10e4 0 10e3 7 10e2 8 10e1 9 10e0\n",
            "Exact match: True\n",
            "Max answer length: 48, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 10e9 0 10e8 8 10e7 5 10e6 3 10e5 2 10e4 0 10e3 8 10e2 2 10e1 8 10e0 plus 1 10e9 3 10e8 3 10e7 5 10e6 9 10e5 0 10e4 1 10e3 9 10e2 9 10e1 0 10e0?\n",
            "Sample correct answer: 4 10e9 4 10e8 2 10e7 1 10e6 2 10e5 2 10e4 2 10e3 8 10e2 1 10e1 8 10e0\n",
            "Sample predicted answer: 4 10e9 4 10e8 2 10e7 1 10e6 2 10e5 2 10e4 2 10e3 8 10e2 1 10e1 8 10e0\n",
            "Exact match: True\n",
            "Max answer length: 47, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 10e9 2 10e8 2 10e7 8 10e6 1 10e5 3 10e4 3 10e3 4 10e2 8 10e1 6 10e0 plus 1 10e9 5 10e8 5 10e7 7 10e6 7 10e5 2 10e4 4 10e3 6 10e2 9 10e1 8 10e0?\n",
            "Sample correct answer: 6 10e9 7 10e8 8 10e7 5 10e6 8 10e5 5 10e4 8 10e3 1 10e2 8 10e1 4 10e0\n",
            "Sample predicted answer: 6 10e9 7 10e8 8 10e7 5 10e6 8 10e5 5 10e4 8 10e3 1 10e2 8 10e1 4 10e0\n",
            "Exact match: True\n",
            "Max answer length: 48, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 10e9 2 10e8 1 10e7 5 10e6 4 10e5 0 10e4 0 10e3 0 10e2 9 10e1 1 10e0 plus 3 10e9 5 10e8 4 10e7 5 10e6 9 10e5 4 10e4 0 10e3 0 10e2 9 10e1 0 10e0?\n",
            "Sample correct answer: 1 10e10 1 10e9 7 10e8 6 10e7 1 10e6 3 10e5 4 10e4 0 10e3 1 10e2 8 10e1 1 10e0\n",
            "Sample predicted answer: 1 10e10 1 10e9 7 10e8 6 10e7 1 10e6 3 10e5 4 10e4 0 10e3 1 10e2 8 10e1 1 10e0\n",
            "Exact match: True\n",
            "Max answer length: 48, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:14<00:00,  1.09it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8400\n",
            "Testing DataLoader 0: 100% 16/16 [00:14<00:00,  1.09it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8399999737739563    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 10e13 2 10e12 9 10e11 8 10e10 0 10e9 4 10e8 3 10e7 8 10e6 3 10e5 5 10e4 5 10e3 5 10e2 1 10e1 3 10e0 plus 9 10e14 2 10e13 5 10e12 2 10e11 6 10e10 5 10e9 7 10e8 8 10e7 8 10e6 3 10e5 3 10e4 1 10e3 2 10e2 5 10e1 7 10e0?\n",
            "Sample correct answer: 9 10e14 8 10e13 8 10e12 2 10e11 4 10e10 6 10e9 2 10e8 2 10e7 6 10e6 6 10e5 8 10e4 6 10e3 7 10e2 7 10e1 0 10e0\n",
            "Sample predicted answer: 1 10e15 5 10e14 4 10e13 8 10e12 2 10e11 4 10e10 6 10e9 2 10e8 2 10e7 6 10e6 6 10e5 8 10e4 6 10e3 7 10e2 7 10e1 0 10e0\n",
            "Exact match: False\n",
            "Max answer length: 68, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 10e14 8 10e13 5 10e12 3 10e11 6 10e10 2 10e9 4 10e8 5 10e7 0 10e6 3 10e5 9 10e4 7 10e3 9 10e2 0 10e1 4 10e0 plus 2 10e12 4 10e11 2 10e10 8 10e9 2 10e8 8 10e7 4 10e6 1 10e5 9 10e4 9 10e3 3 10e2 9 10e1 5 10e0?\n",
            "Sample correct answer: 1 10e14 8 10e13 7 10e12 7 10e11 9 10e10 0 10e9 7 10e8 3 10e7 4 10e6 5 10e5 9 10e4 7 10e3 2 10e2 9 10e1 9 10e0\n",
            "Sample predicted answer: 4 10e12 7 10e11 9 10e10 0 10e9 7 10e8 3 10e7 4 10e6 5 10e5 9 10e4 7 10e3 2 10e2 9 10e1 9 10e0\n",
            "Exact match: False\n",
            "Max answer length: 68, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 10e13 1 10e12 0 10e11 7 10e10 8 10e9 1 10e8 2 10e7 2 10e6 3 10e5 7 10e4 9 10e3 3 10e2 8 10e1 5 10e0 plus 9 10e14 8 10e13 7 10e12 4 10e11 5 10e10 6 10e9 4 10e8 2 10e7 6 10e6 4 10e5 1 10e4 2 10e3 3 10e2 9 10e1 0 10e0?\n",
            "Sample correct answer: 1 10e15 0 10e14 1 10e13 8 10e12 5 10e11 3 10e10 4 10e9 5 10e8 4 10e7 8 10e6 7 10e5 9 10e4 1 10e3 7 10e2 7 10e1 5 10e0\n",
            "Sample predicted answer: 1 10e15 3 10e14 6 10e13 8 10e12 5 10e11 3 10e10 4 10e9 5 10e8 4 10e7 8 10e6 7 10e5 9 10e4 1 10e3 7 10e2 7 10e1 5 10e0\n",
            "Exact match: False\n",
            "Max answer length: 69, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 10e14 1 10e13 5 10e12 5 10e11 0 10e10 2 10e9 3 10e8 6 10e7 8 10e6 8 10e5 3 10e4 1 10e3 9 10e2 8 10e1 4 10e0 plus 1 10e13 3 10e12 8 10e11 1 10e10 2 10e9 0 10e8 6 10e7 8 10e6 1 10e5 5 10e4 9 10e3 3 10e2 7 10e1 9 10e0?\n",
            "Sample correct answer: 1 10e14 2 10e13 9 10e12 3 10e11 1 10e10 4 10e9 4 10e8 3 10e7 6 10e6 9 10e5 9 10e4 1 10e3 3 10e2 6 10e1 3 10e0\n",
            "Sample predicted answer: 2 10e14 2 10e13 9 10e12 3 10e11 1 10e10 4 10e9 4 10e8 3 10e7 6 10e6 9 10e5 9 10e4 1 10e3 3 10e2 6 10e1 3 10e0\n",
            "Exact match: False\n",
            "Max answer length: 70, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 10e14 6 10e13 6 10e12 2 10e11 8 10e10 7 10e9 4 10e8 5 10e7 4 10e6 3 10e5 8 10e4 9 10e3 2 10e2 4 10e1 6 10e0 plus 1 10e14 1 10e13 5 10e12 7 10e11 4 10e10 6 10e9 0 10e8 8 10e7 8 10e6 4 10e5 2 10e4 6 10e3 8 10e2 8 10e1 5 10e0?\n",
            "Sample correct answer: 2 10e14 8 10e13 2 10e12 0 10e11 3 10e10 3 10e9 5 10e8 4 10e7 2 10e6 8 10e5 1 10e4 6 10e3 1 10e2 3 10e1 1 10e0\n",
            "Sample predicted answer: 2 10e14 8 10e13 2 10e12 0 10e11 3 10e10 3 10e9 5 10e8 4 10e7 2 10e6 8 10e5 1 10e4 6 10e3 1 10e2 3 10e1 1 10e0\n",
            "Exact match: True\n",
            "Max answer length: 69, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:21<00:00,  1.32s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8260\n",
            "Testing DataLoader 0: 100% 16/16 [00:21<00:00,  1.32s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8259999752044678    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 10e19 0 10e18 1 10e17 6 10e16 9 10e15 2 10e14 5 10e13 3 10e12 2 10e11 7 10e10 5 10e9 9 10e8 6 10e7 0 10e6 3 10e5 3 10e4 7 10e3 9 10e2 0 10e1 1 10e0 plus 2 10e19 9 10e18 6 10e17 8 10e16 0 10e15 2 10e14 6 10e13 1 10e12 7 10e11 1 10e10 7 10e9 5 10e8 2 10e7 9 10e6 3 10e5 4 10e4 2 10e3 4 10e2 2 10e1 0 10e0?\n",
            "Sample correct answer: 7 10e19 9 10e18 8 10e17 4 10e16 9 10e15 5 10e14 1 10e13 4 10e12 9 10e11 9 10e10 3 10e9 4 10e8 8 10e7 9 10e6 6 10e5 8 10e4 0 10e3 3 10e2 2 10e1 1 10e0\n",
            "Sample predicted answer: 7 10e19 9 10e18 8 10e17 4 10e16 9 10e15 5 10e14 1 10e13 4 10e12 9 10e11 9 10e10 3 10e9 4 10e8 8 10e7 9 10e6 6 10e5 8 10e4 0 10e3 3 10e2 2 10e1 1 10e0\n",
            "Exact match: True\n",
            "Max answer length: 90, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 10e19 6 10e18 9 10e17 0 10e16 0 10e15 9 10e14 7 10e13 6 10e12 4 10e11 2 10e10 2 10e9 1 10e8 8 10e7 5 10e6 5 10e5 3 10e4 4 10e3 2 10e2 8 10e1 8 10e0 plus 5 10e19 9 10e18 2 10e17 1 10e16 7 10e15 4 10e14 1 10e13 8 10e12 3 10e11 6 10e10 4 10e9 3 10e8 2 10e7 7 10e6 7 10e5 6 10e4 7 10e3 4 10e2 0 10e1 4 10e0?\n",
            "Sample correct answer: 9 10e19 6 10e18 1 10e17 1 10e16 8 10e15 3 10e14 9 10e13 4 10e12 7 10e11 8 10e10 6 10e9 5 10e8 1 10e7 3 10e6 3 10e5 0 10e4 1 10e3 6 10e2 9 10e1 2 10e0\n",
            "Sample predicted answer: 9 10e19 6 10e18 1 10e17 1 10e16 8 10e15 3 10e14 9 10e13 4 10e12 7 10e11 8 10e10 6 10e9 5 10e8 1 10e7 3 10e6 3 10e5 0 10e4 1 10e3 6 10e2 9 10e1 2 10e0\n",
            "Exact match: True\n",
            "Max answer length: 89, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 10e19 3 10e18 6 10e17 5 10e16 2 10e15 2 10e14 3 10e13 8 10e12 0 10e11 9 10e10 0 10e9 6 10e8 4 10e7 2 10e6 2 10e5 3 10e4 9 10e3 5 10e2 5 10e1 9 10e0 plus 3 10e19 2 10e18 5 10e17 1 10e16 6 10e15 7 10e14 6 10e13 0 10e12 2 10e11 5 10e10 6 10e9 0 10e8 1 10e7 9 10e6 5 10e5 1 10e4 4 10e3 7 10e2 3 10e1 4 10e0?\n",
            "Sample correct answer: 4 10e19 6 10e18 1 10e17 6 10e16 8 10e15 9 10e14 9 10e13 8 10e12 3 10e11 4 10e10 6 10e9 6 10e8 6 10e7 1 10e6 7 10e5 5 10e4 4 10e3 2 10e2 9 10e1 3 10e0\n",
            "Sample predicted answer: 4 10e19 6 10e18 1 10e17 6 10e16 8 10e15 9 10e14 9 10e13 8 10e12 3 10e11 4 10e10 6 10e9 6 10e8 6 10e7 1 10e6 7 10e5 5 10e4 4 10e3 2 10e2 9 10e1 3 10e0\n",
            "Exact match: True\n",
            "Max answer length: 90, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 10e19 2 10e18 4 10e17 3 10e16 2 10e15 6 10e14 8 10e13 2 10e12 1 10e11 3 10e10 0 10e9 6 10e8 0 10e7 8 10e6 6 10e5 9 10e4 7 10e3 9 10e2 3 10e1 8 10e0 plus 7 10e19 9 10e18 7 10e17 1 10e16 8 10e15 0 10e14 9 10e13 8 10e12 2 10e11 5 10e10 3 10e9 3 10e8 4 10e7 2 10e6 8 10e5 6 10e4 4 10e3 9 10e2 4 10e1 7 10e0?\n",
            "Sample correct answer: 1 10e20 4 10e19 2 10e18 1 10e17 5 10e16 0 10e15 7 10e14 8 10e13 0 10e12 3 10e11 8 10e10 3 10e9 9 10e8 5 10e7 1 10e6 5 10e5 6 10e4 2 10e3 8 10e2 8 10e1 5 10e0\n",
            "Sample predicted answer: 1 10e20 4 10e19 2 10e18 1 10e17 5 10e16 0 10e15 7 10e14 8 10e13 0 10e12 3 10e11 8 10e10 3 10e9 9 10e8 5 10e7 1 10e6 5 10e5 6 10e4 2 10e3 8 10e2 8 10e1 5 10e0\n",
            "Exact match: True\n",
            "Max answer length: 90, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 10e19 9 10e18 8 10e17 7 10e16 2 10e15 1 10e14 5 10e13 5 10e12 8 10e11 0 10e10 5 10e9 6 10e8 8 10e7 9 10e6 3 10e5 2 10e4 7 10e3 5 10e2 4 10e1 1 10e0 plus 4 10e19 3 10e18 6 10e17 4 10e16 1 10e15 1 10e14 7 10e13 8 10e12 6 10e11 0 10e10 8 10e9 3 10e8 1 10e7 2 10e6 8 10e5 1 10e4 8 10e3 0 10e2 6 10e1 6 10e0?\n",
            "Sample correct answer: 1 10e20 3 10e19 3 10e18 5 10e17 1 10e16 3 10e15 3 10e14 3 10e13 4 10e12 4 10e11 1 10e10 4 10e9 0 10e8 0 10e7 2 10e6 1 10e5 4 10e4 5 10e3 6 10e2 0 10e1 7 10e0\n",
            "Sample predicted answer: 1 10e20 3 10e19 3 10e18 5 10e17 1 10e16 3 10e15 3 10e14 3 10e13 4 10e12 4 10e11 1 10e10 4 10e9 0 10e8 0 10e7 2 10e6 1 10e5 4 10e4 5 10e3 6 10e2 0 10e1 7 10e0\n",
            "Exact match: True\n",
            "Max answer length: 89, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:27<00:00,  1.71s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8400\n",
            "Testing DataLoader 0: 100% 16/16 [00:27<00:00,  1.71s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8399999737739563    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 10e24 2 10e23 8 10e22 7 10e21 1 10e20 1 10e19 8 10e18 3 10e17 2 10e16 5 10e15 1 10e14 0 10e13 1 10e12 8 10e11 4 10e10 9 10e9 9 10e8 4 10e7 4 10e6 5 10e5 4 10e4 2 10e3 2 10e2 1 10e1 4 10e0 plus 7 10e24 7 10e23 2 10e22 5 10e21 5 10e20 2 10e19 5 10e18 6 10e17 2 10e16 9 10e15 0 10e14 9 10e13 6 10e12 1 10e11 7 10e10 6 10e9 6 10e8 6 10e7 4 10e6 4 10e5 2 10e4 6 10e3 6 10e2 1 10e1 3 10e0?\n",
            "Sample correct answer: 1 10e25 6 10e24 0 10e23 1 10e22 2 10e21 6 10e20 4 10e19 3 10e18 9 10e17 5 10e16 4 10e15 1 10e14 9 10e13 8 10e12 0 10e11 2 10e10 6 10e9 6 10e8 0 10e7 8 10e6 9 10e5 6 10e4 8 10e3 8 10e2 2 10e1 7 10e0\n",
            "Sample predicted answer: 1 10e25 6 10e24 0 10e23 1 10e22 2 10e21 6 10e20 4 10e19 3 10e18 9 10e17 5 10e16 4 10e15 1 10e14 9 10e13 8 10e12 0 10e11 2 10e10 6 10e9 6 10e8 0 10e7 8 10e6 9 10e5 6 10e4 8 10e3 8 10e2 2 10e1 7 10e0\n",
            "Exact match: True\n",
            "Max answer length: 111, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2 10e24 6 10e23 7 10e22 6 10e21 5 10e20 3 10e19 1 10e18 8 10e17 1 10e16 7 10e15 7 10e14 7 10e13 0 10e12 3 10e11 4 10e10 6 10e9 8 10e8 0 10e7 8 10e6 3 10e5 0 10e4 6 10e3 3 10e2 1 10e1 1 10e0 plus 7 10e24 4 10e23 0 10e22 8 10e21 9 10e20 7 10e19 3 10e18 0 10e17 0 10e16 2 10e15 5 10e14 3 10e13 0 10e12 1 10e11 5 10e10 9 10e9 7 10e8 5 10e7 6 10e6 8 10e5 9 10e4 1 10e3 7 10e2 9 10e1 8 10e0?\n",
            "Sample correct answer: 1 10e25 0 10e24 0 10e23 8 10e22 5 10e21 5 10e20 0 10e19 4 10e18 8 10e17 2 10e16 0 10e15 3 10e14 0 10e13 0 10e12 5 10e11 0 10e10 6 10e9 5 10e8 6 10e7 5 10e6 1 10e5 9 10e4 8 10e3 1 10e2 0 10e1 9 10e0\n",
            "Sample predicted answer: 1 10e25 0 10e24 0 10e23 8 10e22 5 10e21 5 10e20 0 10e19 4 10e18 8 10e17 2 10e16 0 10e15 3 10e14 0 10e13 0 10e12 5 10e11 0 10e10 6 10e9 5 10e8 6 10e7 5 10e6 1 10e5 9 10e4 8 10e3 1 10e2 0 10e1 9 10e0\n",
            "Exact match: True\n",
            "Max answer length: 113, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 10e24 9 10e23 4 10e22 1 10e21 3 10e20 3 10e19 2 10e18 9 10e17 4 10e16 2 10e15 7 10e14 1 10e13 2 10e12 3 10e11 0 10e10 7 10e9 3 10e8 3 10e7 6 10e6 4 10e5 5 10e4 2 10e3 8 10e2 1 10e1 9 10e0 plus 8 10e24 5 10e23 1 10e22 6 10e21 7 10e20 6 10e19 6 10e18 6 10e17 4 10e16 6 10e15 5 10e14 9 10e13 3 10e12 2 10e11 3 10e10 9 10e9 9 10e8 9 10e7 5 10e6 0 10e5 5 10e4 2 10e3 2 10e2 1 10e1 5 10e0?\n",
            "Sample correct answer: 1 10e25 0 10e24 4 10e23 5 10e22 8 10e21 0 10e20 9 10e19 9 10e18 5 10e17 8 10e16 9 10e15 3 10e14 0 10e13 5 10e12 5 10e11 4 10e10 7 10e9 3 10e8 3 10e7 1 10e6 5 10e5 0 10e4 5 10e3 0 10e2 3 10e1 4 10e0\n",
            "Sample predicted answer: 1 10e25 0 10e24 4 10e23 5 10e22 8 10e21 0 10e20 9 10e19 9 10e18 5 10e17 8 10e16 9 10e15 3 10e14 0 10e13 5 10e12 5 10e11 4 10e10 7 10e9 3 10e8 3 10e7 1 10e6 5 10e5 0 10e4 5 10e3 0 10e2 3 10e1 4 10e0\n",
            "Exact match: True\n",
            "Max answer length: 111, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 10e24 7 10e23 0 10e22 3 10e21 7 10e20 6 10e19 5 10e18 5 10e17 1 10e16 7 10e15 8 10e14 8 10e13 9 10e12 6 10e11 3 10e10 4 10e9 9 10e8 6 10e7 9 10e6 8 10e5 6 10e4 5 10e3 5 10e2 4 10e1 4 10e0 plus 3 10e24 5 10e23 1 10e22 9 10e21 6 10e20 8 10e19 6 10e18 5 10e17 0 10e16 5 10e15 4 10e14 8 10e13 9 10e12 6 10e11 5 10e10 1 10e9 2 10e8 5 10e7 0 10e6 8 10e5 5 10e4 8 10e3 2 10e2 1 10e1 2 10e0?\n",
            "Sample correct answer: 1 10e25 2 10e24 2 10e23 2 10e22 3 10e21 4 10e20 5 10e19 2 10e18 0 10e17 2 10e16 3 10e15 3 10e14 7 10e13 9 10e12 2 10e11 8 10e10 6 10e9 2 10e8 2 10e7 0 10e6 7 10e5 2 10e4 3 10e3 7 10e2 5 10e1 6 10e0\n",
            "Sample predicted answer: 1 10e25 2 10e24 2 10e23 2 10e22 3 10e21 4 10e20 5 10e19 2 10e18 0 10e17 2 10e16 3 10e15 3 10e14 7 10e13 9 10e12 2 10e11 8 10e10 6 10e9 2 10e8 2 10e7 0 10e6 7 10e5 2 10e4 3 10e3 7 10e2 5 10e1 6 10e0\n",
            "Exact match: True\n",
            "Max answer length: 110, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 10e23 3 10e22 9 10e21 1 10e20 5 10e19 9 10e18 7 10e17 0 10e16 2 10e15 7 10e14 8 10e13 0 10e12 1 10e11 3 10e10 9 10e9 0 10e8 0 10e7 0 10e6 2 10e5 4 10e4 2 10e3 0 10e2 2 10e1 9 10e0 plus 8 10e24 0 10e23 1 10e22 6 10e21 4 10e20 7 10e19 7 10e18 4 10e17 3 10e16 4 10e15 6 10e14 9 10e13 4 10e12 5 10e11 3 10e10 1 10e9 5 10e8 7 10e7 0 10e6 8 10e5 9 10e4 2 10e3 8 10e2 9 10e1 7 10e0?\n",
            "Sample correct answer: 8 10e24 2 10e23 5 10e22 5 10e21 6 10e20 3 10e19 7 10e18 1 10e17 3 10e16 7 10e15 4 10e14 7 10e13 4 10e12 6 10e11 7 10e10 0 10e9 5 10e8 7 10e7 1 10e6 1 10e5 3 10e4 4 10e3 9 10e2 2 10e1 6 10e0\n",
            "Sample predicted answer: 1 10e25 0 10e24 7 10e23 5 10e22 5 10e21 6 10e20 3 10e19 7 10e18 1 10e17 3 10e16 7 10e15 4 10e14 7 10e13 4 10e12 6 10e11 7 10e10 0 10e9 5 10e8 7 10e7 1 10e6 1 10e5 3 10e4 4 10e3 9 10e2 2 10e1 6 10e0\n",
            "Exact match: False\n",
            "Max answer length: 111, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:33<00:00,  2.10s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8400\n",
            "Testing DataLoader 0: 100% 16/16 [00:33<00:00,  2.10s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8399999737739563    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10ebased model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10ebased_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=1.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 10e29 4 10e28 3 10e27 5 10e26 9 10e25 4 10e24 0 10e23 9 10e22 1 10e21 6 10e20 2 10e19 3 10e18 5 10e17 7 10e16 2 10e15 7 10e14 1 10e13 0 10e12 8 10e11 7 10e10 8 10e9 1 10e8 2 10e7 9 10e6 3 10e5 1 10e4 6 10e3 8 10e2 2 10e1 9 10e0 plus 1 10e28 8 10e27 9 10e26 7 10e25 1 10e24 3 10e23 6 10e22 6 10e21 9 10e20 5 10e19 3 10e18 6 10e17 3 10e16 4 10e15 5 10e14 4 10e13 7 10e12 6 10e11 3 10e10 9 10e9 8 10e8 0 10e7 2 10e6 9 10e5 7 10e4 2 10e3 9 10e2 7 10e1 3 10e0?\n",
            "Sample correct answer: 4 10e29 6 10e28 2 10e27 5 10e26 6 10e25 5 10e24 4 10e23 5 10e22 8 10e21 5 10e20 7 10e19 7 10e18 2 10e17 0 10e16 7 10e15 2 10e14 5 10e13 8 10e12 5 10e11 1 10e10 7 10e9 9 10e8 3 10e7 2 10e6 2 10e5 8 10e4 9 10e3 8 10e2 0 10e1 2 10e0\n",
            "Sample predicted answer: 6 10e29 7 10e28 2 10e27 5 10e26 6 10e25 5 10e24 4 10e23 5 10e22 8 10e21 5 10e20 7 10e19 7 10e18 2 10e17 0 10e16 7 10e15 2 10e14 5 10e13 8 10e12 5 10e11 1 10e10 7 10e9 9 10e8 3 10e7 2 10e6 2 10e5 8 10e4 9 10e3 8 10e2 0 10e1 2 10e0\n",
            "Exact match: False\n",
            "Max answer length: 133, Max generation length: 143\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 9 10e28 6 10e27 1 10e26 7 10e25 9 10e24 2 10e23 5 10e22 6 10e21 0 10e20 2 10e19 8 10e18 4 10e17 8 10e16 9 10e15 9 10e14 7 10e13 7 10e12 6 10e11 1 10e10 1 10e9 4 10e8 3 10e7 0 10e6 0 10e5 1 10e4 8 10e3 7 10e2 8 10e1 6 10e0 plus 6 10e29 6 10e28 0 10e27 0 10e26 4 10e25 0 10e24 4 10e23 2 10e22 4 10e21 4 10e20 6 10e19 0 10e18 3 10e17 3 10e16 7 10e15 1 10e14 2 10e13 1 10e12 3 10e11 0 10e10 1 10e9 5 10e8 5 10e7 6 10e6 9 10e5 3 10e4 8 10e3 8 10e2 3 10e1 6 10e0?\n",
            "Sample correct answer: 7 10e29 5 10e28 6 10e27 2 10e26 1 10e25 9 10e24 6 10e23 8 10e22 0 10e21 4 10e20 8 10e19 8 10e18 8 10e17 2 10e16 7 10e15 0 10e14 9 10e13 8 10e12 9 10e11 1 10e10 2 10e9 9 10e8 8 10e7 6 10e6 9 10e5 5 10e4 7 10e3 6 10e2 2 10e1 2 10e0\n",
            "Sample predicted answer: 1 10e29 5 10e28 6 10e27 2 10e26 1 10e25 9 10e24 6 10e23 8 10e22 0 10e21 4 10e20 8 10e19 8 10e18 8 10e17 2 10e16 7 10e15 0 10e14 9 10e13 8 10e12 9 10e11 1 10e10 2 10e9 9 10e8 8 10e7 6 10e6 9 10e5 5 10e4 7 10e3 6 10e2 2 10e1 2 10e0\n",
            "Exact match: False\n",
            "Max answer length: 130, Max generation length: 140\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 10e29 5 10e28 6 10e27 9 10e26 4 10e25 8 10e24 9 10e23 5 10e22 5 10e21 2 10e20 1 10e19 3 10e18 2 10e17 9 10e16 0 10e15 6 10e14 4 10e13 0 10e12 6 10e11 4 10e10 5 10e9 0 10e8 5 10e7 3 10e6 1 10e5 1 10e4 0 10e3 7 10e2 4 10e1 8 10e0 plus 2 10e29 7 10e28 5 10e27 0 10e26 9 10e25 7 10e24 5 10e23 9 10e22 3 10e21 2 10e20 4 10e19 3 10e18 0 10e17 0 10e16 9 10e15 8 10e14 3 10e13 9 10e12 0 10e11 8 10e10 5 10e9 7 10e8 9 10e7 9 10e6 1 10e5 5 10e4 1 10e3 5 10e2 1 10e1 4 10e0?\n",
            "Sample correct answer: 9 10e29 3 10e28 2 10e27 0 10e26 4 10e25 6 10e24 5 10e23 4 10e22 8 10e21 4 10e20 5 10e19 6 10e18 3 10e17 0 10e16 0 10e15 4 10e14 7 10e13 9 10e12 7 10e11 3 10e10 0 10e9 8 10e8 5 10e7 2 10e6 2 10e5 6 10e4 2 10e3 2 10e2 6 10e1 2 10e0\n",
            "Sample predicted answer: 9 10e29 3 10e28 2 10e27 0 10e26 4 10e25 6 10e24 5 10e23 4 10e22 8 10e21 4 10e20 5 10e19 6 10e18 3 10e17 0 10e16 0 10e15 4 10e14 7 10e13 9 10e12 7 10e11 3 10e10 0 10e9 8 10e8 5 10e7 2 10e6 2 10e5 6 10e4 2 10e3 2 10e2 6 10e1 2 10e0\n",
            "Exact match: True\n",
            "Max answer length: 132, Max generation length: 142\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 10e28 0 10e27 5 10e26 6 10e25 9 10e24 7 10e23 9 10e22 7 10e21 2 10e20 1 10e19 8 10e18 1 10e17 5 10e16 0 10e15 8 10e14 3 10e13 0 10e12 9 10e11 7 10e10 1 10e9 6 10e8 3 10e7 4 10e6 9 10e5 5 10e4 5 10e3 7 10e2 2 10e1 2 10e0 plus 9 10e29 4 10e28 9 10e27 2 10e26 1 10e25 8 10e24 2 10e23 8 10e22 9 10e21 5 10e20 2 10e19 8 10e18 0 10e17 5 10e16 5 10e15 6 10e14 3 10e13 0 10e12 4 10e11 7 10e10 3 10e9 6 10e8 4 10e7 6 10e6 7 10e5 9 10e4 1 10e3 4 10e2 9 10e1 7 10e0?\n",
            "Sample correct answer: 9 10e29 5 10e28 9 10e27 7 10e26 8 10e25 8 10e24 0 10e23 8 10e22 6 10e21 7 10e20 4 10e19 6 10e18 2 10e17 0 10e16 6 10e15 4 10e14 6 10e13 1 10e12 4 10e11 4 10e10 5 10e9 2 10e8 8 10e7 1 10e6 7 10e5 4 10e4 7 10e3 2 10e2 1 10e1 9 10e0\n",
            "Sample predicted answer: 1 10e30 1 10e29 8 10e28 9 10e27 7 10e26 8 10e25 8 10e24 0 10e23 8 10e22 6 10e21 7 10e20 4 10e19 6 10e18 2 10e17 0 10e16 6 10e15 4 10e14 6 10e13 1 10e12 4 10e11 4 10e10 5 10e9 2 10e8 8 10e7 1 10e6 7 10e5 4 10e4 7 10e3 2 10e2 1 10e1 9 10e0\n",
            "Exact match: False\n",
            "Max answer length: 132, Max generation length: 142\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 10e29 7 10e28 1 10e27 6 10e26 4 10e25 8 10e24 7 10e23 0 10e22 8 10e21 8 10e20 0 10e19 8 10e18 7 10e17 4 10e16 7 10e15 2 10e14 6 10e13 4 10e12 2 10e11 1 10e10 2 10e9 2 10e8 1 10e7 9 10e6 5 10e5 5 10e4 7 10e3 6 10e2 5 10e1 5 10e0 plus 7 10e29 4 10e28 2 10e27 2 10e26 3 10e25 1 10e24 1 10e23 0 10e22 5 10e21 1 10e20 4 10e19 9 10e18 3 10e17 8 10e16 3 10e15 5 10e14 6 10e13 0 10e12 8 10e11 7 10e10 2 10e9 3 10e8 5 10e7 1 10e6 8 10e5 9 10e4 0 10e3 4 10e2 6 10e1 1 10e0?\n",
            "Sample correct answer: 1 10e30 5 10e29 1 10e28 3 10e27 8 10e26 7 10e25 9 10e24 8 10e23 1 10e22 3 10e21 9 10e20 5 10e19 8 10e18 1 10e17 3 10e16 0 10e15 8 10e14 2 10e13 5 10e12 0 10e11 8 10e10 4 10e9 5 10e8 7 10e7 1 10e6 4 10e5 4 10e4 8 10e3 1 10e2 1 10e1 6 10e0\n",
            "Sample predicted answer: 1 10e30 5 10e29 1 10e28 3 10e27 8 10e26 7 10e25 9 10e24 8 10e23 1 10e22 3 10e21 9 10e20 5 10e19 8 10e18 1 10e17 3 10e16 0 10e15 8 10e14 2 10e13 5 10e12 0 10e11 8 10e10 4 10e9 5 10e8 7 10e7 1 10e6 4 10e5 4 10e4 8 10e3 1 10e2 1 10e1 6 10e0\n",
            "Exact match: True\n",
            "Max answer length: 131, Max generation length: 141\n",
            "Testing DataLoader 0: 100% 16/16 [00:40<00:00,  2.50s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8200\n",
            "Testing DataLoader 0: 100% 16/16 [00:40<00:00,  2.50s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8199999928474426    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is thirty-five plus fifty-eight?\n",
            "Sample correct answer: ninety-three\n",
            "Sample predicted answer: ninety-three\n",
            "Exact match: True\n",
            "Max answer length: 10, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is fourteen plus forty-six?\n",
            "Sample correct answer: sixty\n",
            "Sample predicted answer: sixty\n",
            "Exact match: True\n",
            "Max answer length: 8, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is forty-nine plus thirty-three?\n",
            "Sample correct answer: eighty-two\n",
            "Sample predicted answer: eighty-two\n",
            "Exact match: True\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is eighty-five plus fifty-two?\n",
            "Sample correct answer: one hundred and thirty-seven\n",
            "Sample predicted answer: one hundred and thirty-seven\n",
            "Exact match: True\n",
            "Max answer length: 10, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is sixty-nine plus twenty-eight?\n",
            "Sample correct answer: ninety-seven\n",
            "Sample predicted answer: ninety-seven\n",
            "Exact match: True\n",
            "Max answer length: 9, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  5.09it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8290\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  5.08it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8289999961853027    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is thirty-six thousand, four hundred and thirty-four plus fifty-nine thousand, four hundred and twenty-nine?\n",
            "Sample correct answer: ninety-five thousand, eight hundred and sixty-three\n",
            "Sample predicted answer: ninety-five thousand, eight hundred and sixty-three\n",
            "Exact match: True\n",
            "Max answer length: 18, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is seven thousand, five hundred and ninety-two plus thirty-one thousand, five hundred and seventy-one?\n",
            "Sample correct answer: thirty-nine thousand, one hundred and sixty-three\n",
            "Sample predicted answer: forty-nine thousand, one hundred and sixty-three\n",
            "Exact match: False\n",
            "Max answer length: 20, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is ninety-one thousand, three hundred and three plus ninety-five thousand, seven hundred and forty-eight?\n",
            "Sample correct answer: one hundred and eighty-seven thousand and fifty-one\n",
            "Sample predicted answer: one hundred and eighty-seven thousand and fifty-one\n",
            "Exact match: True\n",
            "Max answer length: 19, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is twenty-five thousand, one hundred and forty-four plus fifty-five thousand, one hundred and eight?\n",
            "Sample correct answer: eighty thousand, two hundred and fifty-two\n",
            "Sample predicted answer: eighty thousand, two hundred and fifty-two\n",
            "Exact match: True\n",
            "Max answer length: 19, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is sixty-five thousand, five hundred and ninety-seven plus seventy thousand, seven hundred and seventy-six?\n",
            "Sample correct answer: one hundred and thirty-six thousand, three hundred and seventy-three\n",
            "Sample predicted answer: one hundred and thirty-six thousand, three hundred and seventy-three\n",
            "Exact match: True\n",
            "Max answer length: 18, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:06<00:00,  2.63it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.7990\n",
            "Testing DataLoader 0: 100% 16/16 [00:06<00:00,  2.63it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7990000247955322    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is nine billion, one hundred and eighty-four million, sixty-four thousand, nine hundred plus two billion, one hundred and nineteen million, six hundred and thirty-four thousand, three hundred and ninety-nine?\n",
            "Sample correct answer: eleven billion, three hundred and three million, six hundred and ninety-nine thousand, two hundred and ninety-nine\n",
            "Sample predicted answer: eleven billion, three hundred and three million, six hundred and ninety-nine thousand, two hundred and ninety-nine\n",
            "Exact match: True\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is one billion, eight hundred and forty-five million, nine hundred and nineteen thousand, eight hundred and ninety-five plus four billion, four hundred and seventy-four million, six hundred and twenty thousand, eight hundred and ninety-four?\n",
            "Sample correct answer: six billion, three hundred and twenty million, five hundred and forty thousand, seven hundred and eighty-nine\n",
            "Sample predicted answer: six billion, three hundred and twenty million, five hundred and forty thousand, seven hundred and eighty-nine\n",
            "Exact match: True\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is three billion, eighty-five million, three hundred and twenty thousand, eight hundred and twenty-eight plus one billion, three hundred and thirty-five million, nine hundred and one thousand, nine hundred and ninety?\n",
            "Sample correct answer: four billion, four hundred and twenty-one million, two hundred and twenty-two thousand, eight hundred and eighteen\n",
            "Sample predicted answer: four billion, four hundred and twenty-one million, two hundred and twenty-two thousand, eight hundred and eighteen\n",
            "Exact match: True\n",
            "Max answer length: 34, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is five billion, two hundred and twenty-eight million, one hundred and thirty-three thousand, four hundred and eighty-six plus one billion, five hundred and fifty-seven million, seven hundred and twenty-four thousand, six hundred and ninety-eight?\n",
            "Sample correct answer: six billion, seven hundred and eighty-five million, eight hundred and fifty-eight thousand, one hundred and eighty-four\n",
            "Sample predicted answer: six billion, seven hundred and eighty-five million, eight hundred and fifty-eight thousand, one hundred and eighty-four\n",
            "Exact match: True\n",
            "Max answer length: 31, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is eight billion, two hundred and fifteen million, four hundred thousand and ninety-one plus three billion, five hundred and forty-five million, nine hundred and forty thousand and ninety?\n",
            "Sample correct answer: eleven billion, seven hundred and sixty-one million, three hundred and forty thousand, one hundred and eighty-one\n",
            "Sample predicted answer: eleven billion, seven hundred and sixty-one million, three hundred and forty thousand and eighty-one\n",
            "Exact match: False\n",
            "Max answer length: 34, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:10<00:00,  1.56it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.7050\n",
            "Testing DataLoader 0: 100% 16/16 [00:10<00:00,  1.56it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7049999833106995    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is sixty-two trillion, nine hundred and eighty billion, four hundred and thirty-eight million, three hundred and fifty-five thousand, five hundred and thirteen plus nine hundred and twenty-five trillion, two hundred and sixty-five billion, seven hundred and eighty-eight million, three hundred and thirty-one thousand, two hundred and fifty-seven?\n",
            "Sample correct answer: nine hundred and eighty-eight trillion, two hundred and forty-six billion, two hundred and twenty-six million, six hundred and eighty-six thousand, seven hundred and seventy\n",
            "Sample predicted answer: one quadrillion, eighty-eight trillion, two hundred and forty-six billion, two hundred and twenty-six million, six hundred and eighty-six thousand, seven hundred and seventy\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is one hundred and eighty-five trillion, three hundred and sixty-two billion, four hundred and fifty million, three hundred and ninety-seven thousand, nine hundred and four plus two trillion, four hundred and twenty-eight billion, two hundred and eighty-four million, one hundred and ninety-nine thousand, three hundred and ninety-five?\n",
            "Sample correct answer: one hundred and eighty-seven trillion, seven hundred and ninety billion, seven hundred and thirty-four million, five hundred and ninety-seven thousand, two hundred and ninety-nine\n",
            "Sample predicted answer: three hundred and eighty-seven trillion, seven hundred and ninety billion, seven hundred and thirty-four million, five hundred and ninety-seven thousand, two hundred and ninety-nine\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is thirty-one trillion, seventy-eight billion, one hundred and twenty-two million, three hundred and seventy-nine thousand, three hundred and eighty-five plus nine hundred and eighty-seven trillion, four hundred and fifty-six billion, four hundred and twenty-six million, four hundred and twelve thousand, three hundred and ninety?\n",
            "Sample correct answer: one quadrillion, eighteen trillion, five hundred and thirty-four billion, five hundred and forty-eight million, seven hundred and ninety-one thousand, seven hundred and seventy-five\n",
            "Sample predicted answer: one quadrillion, nineteen trillion, five hundred and thirty-four billion, five hundred and forty-eight million, seven hundred and ninety-one thousand, seven hundred and seventy-five\n",
            "Exact match: False\n",
            "Max answer length: 50, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is one hundred and fifteen trillion, five hundred and two billion, three hundred and sixty-eight million, eight hundred and thirty-one thousand, nine hundred and eighty-four plus thirteen trillion, eight hundred and twelve billion, sixty-eight million, one hundred and fifty-nine thousand, three hundred and seventy-nine?\n",
            "Sample correct answer: one hundred and twenty-nine trillion, three hundred and fourteen billion, four hundred and thirty-six million, nine hundred and ninety-one thousand, three hundred and sixty-three\n",
            "Sample predicted answer: two hundred and twenty-nine trillion, three hundred and fourteen billion, four hundred and thirty-six million, nine hundred and ninety-one thousand, three hundred and sixty-three\n",
            "Exact match: False\n",
            "Max answer length: 49, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is one hundred and sixty-six trillion, two hundred and eighty-seven billion, four hundred and fifty-four million, three hundred and eighty-nine thousand, two hundred and forty-six plus one hundred and fifteen trillion, seven hundred and forty-six billion, eighty-eight million, four hundred and twenty-six thousand, eight hundred and eighty-five?\n",
            "Sample correct answer: two hundred and eighty-two trillion, thirty-three billion, five hundred and forty-two million, eight hundred and sixteen thousand, one hundred and thirty-one\n",
            "Sample predicted answer: two hundred and eighty-two trillion, thirty-three billion, five hundred and forty-two million, eight hundred and sixteen thousand, one hundred and thirty-one\n",
            "Exact match: True\n",
            "Max answer length: 52, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:15<00:00,  1.04it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.6610\n",
            "Testing DataLoader 0: 100% 16/16 [00:15<00:00,  1.04it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6610000133514404    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is fifty quintillion, one hundred and sixty-nine quadrillion, two hundred and fifty-three trillion, two hundred and seventy-five billion, nine hundred and sixty million, three hundred and thirty-seven thousand, nine hundred and one plus twenty-nine quintillion, six hundred and eighty quadrillion, two hundred and sixty-one trillion, seven hundred and seventeen billion, five hundred and twenty-nine million, three hundred and forty-two thousand, four hundred and twenty?\n",
            "Sample correct answer: seventy-nine quintillion, eight hundred and forty-nine quadrillion, five hundred and fourteen trillion, nine hundred and ninety-three billion, four hundred and eighty-nine million, six hundred and eighty thousand, three hundred and twenty-one\n",
            "Sample predicted answer: seventy-nine quintillion, eight hundred and forty-nine quadrillion, five hundred and fifteen trillion, nine hundred and ninety-three billion, four hundred and eighty-nine million, six hundred and eighty thousand, three hundred and twenty-one\n",
            "Exact match: False\n",
            "Max answer length: 72, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is thirty-six quintillion, nine hundred quadrillion, nine hundred and seventy-six trillion, four hundred and twenty-two billion, one hundred and eighty-five million, five hundred and thirty-four thousand, two hundred and eighty-eight plus fifty-nine quintillion, two hundred and seventeen quadrillion, four hundred and eighteen trillion, three hundred and sixty-four billion, three hundred and twenty-seven million, seven hundred and sixty-seven thousand, four hundred and four?\n",
            "Sample correct answer: ninety-six quintillion, one hundred and eighteen quadrillion, three hundred and ninety-four trillion, seven hundred and eighty-six billion, five hundred and thirteen million, three hundred and one thousand, six hundred and ninety-two\n",
            "Sample predicted answer: ninety-six quintillion, one hundred and eighteen quadrillion, three hundred and ninety-four trillion, seven hundred and eighty-six billion, five hundred and thirteen million, three hundred and one thousand, six hundred and ninety-two\n",
            "Exact match: True\n",
            "Max answer length: 67, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is thirteen quintillion, six hundred and fifty-two quadrillion, two hundred and thirty-eight trillion, ninety billion, six hundred and forty-two million, two hundred and thirty-nine thousand, five hundred and fifty-nine plus thirty-two quintillion, five hundred and sixteen quadrillion, seven hundred and sixty trillion, two hundred and fifty-six billion, nineteen million, five hundred and fourteen thousand, seven hundred and thirty-four?\n",
            "Sample correct answer: forty-six quintillion, one hundred and sixty-eight quadrillion, nine hundred and ninety-eight trillion, three hundred and forty-six billion, six hundred and sixty-one million, seven hundred and fifty-four thousand, two hundred and ninety-three\n",
            "Sample predicted answer: forty-six quintillion, one hundred and sixty-nine quadrillion, nine hundred and ninety-eight trillion, three hundred and forty-six billion, six hundred and sixty-one million, seven hundred and fifty-four thousand, two hundred and ninety-three\n",
            "Exact match: False\n",
            "Max answer length: 67, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is sixty-two quintillion, four hundred and thirty-two quadrillion, six hundred and eighty-two trillion, one hundred and thirty billion, six hundred and eight million, six hundred and ninety-seven thousand, nine hundred and thirty-eight plus seventy-nine quintillion, seven hundred and eighteen quadrillion, ninety-eight trillion, two hundred and fifty-three billion, three hundred and forty-two million, eight hundred and sixty-four thousand, nine hundred and forty-seven?\n",
            "Sample correct answer: one hundred and forty-two quintillion, one hundred and fifty quadrillion, seven hundred and eighty trillion, three hundred and eighty-three billion, nine hundred and fifty-one million, five hundred and sixty-two thousand, eight hundred and eighty-five\n",
            "Sample predicted answer: one hundred and forty-two quintillion, one hundred and fifty quadrillion, seven hundred and eighty trillion, three hundred and eighty-three billion, nine hundred and fifty-one million, five hundred and sixty-two thousand, eight hundred and eighty-five\n",
            "Exact match: True\n",
            "Max answer length: 67, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is eighty-nine quintillion, eight hundred and seventy-two quadrillion, one hundred and fifty-five trillion, eight hundred and five billion, six hundred and eighty-nine million, three hundred and twenty-seven thousand, five hundred and forty-one plus forty-three quintillion, six hundred and forty-one quadrillion, one hundred and seventy-eight trillion, six hundred and eight billion, three hundred and twelve million, eight hundred and eighteen thousand and sixty-six?\n",
            "Sample correct answer: one hundred and thirty-three quintillion, five hundred and thirteen quadrillion, three hundred and thirty-four trillion, four hundred and fourteen billion, two million, one hundred and forty-five thousand, six hundred and seven\n",
            "Sample predicted answer: one hundred and thirty-three quintillion, five hundred and thirteen quadrillion, three hundred and thirty-four trillion, four hundred and fourteen billion, one million, one hundred and forty-five thousand, six hundred and seven\n",
            "Exact match: False\n",
            "Max answer length: 68, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:20<00:00,  1.31s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.6450\n",
            "Testing DataLoader 0: 100% 16/16 [00:20<00:00,  1.31s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6449999809265137    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is eight septillion, two hundred and eighty-seven sextillion, one hundred and eighteen quintillion, three hundred and twenty-five quadrillion, one hundred and one trillion, eight hundred and forty-nine billion, nine hundred and forty-four million, five hundred and forty-two thousand, two hundred and fourteen plus seven septillion, seven hundred and twenty-five sextillion, five hundred and twenty-five quintillion, six hundred and twenty-nine quadrillion, ninety-six trillion, one hundred and seventy-six billion, six hundred and sixty-four million, four hundred and twenty-six thousand, six hundred and thirteen?\n",
            "Sample correct answer: sixteen septillion, twelve sextillion, six hundred and forty-three quintillion, nine hundred and fifty-four quadrillion, one hundred and ninety-eight trillion, twenty-six billion, six hundred and eight million, nine hundred and sixty-eight thousand, eight hundred and twenty-seven\n",
            "Sample predicted answer: sixteen septillion, twelve sextillion, six hundred and forty-three quintillion, nine hundred and fifty-four quadrillion, one hundred and ninety-eight trillion, twenty-six billion, six hundred and eight million, nine hundred and sixty-eight thousand, eight hundred and twenty-seven\n",
            "Exact match: True\n",
            "Max answer length: 85, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is two septillion, six hundred and seventy-six sextillion, five hundred and thirty-one quintillion, eight hundred and seventeen quadrillion, seven hundred and seventy trillion, three hundred and forty-six billion, eight hundred and eight million, three hundred and six thousand, three hundred and eleven plus seven septillion, four hundred and eight sextillion, nine hundred and seventy-three quintillion, two quadrillion, five hundred and thirty trillion, one hundred and fifty-nine billion, seven hundred and fifty-six million, eight hundred and ninety-one thousand, seven hundred and ninety-eight?\n",
            "Sample correct answer: ten septillion, eighty-five sextillion, five hundred and four quintillion, eight hundred and twenty quadrillion, three hundred trillion, five hundred and six billion, five hundred and sixty-five million, one hundred and ninety-eight thousand, one hundred and nine\n",
            "Sample predicted answer: ten septillion, eighty-five sextillion, five hundred and four quintillion, eight hundred and ten quadrillion, three hundred and one trillion, five hundred and six billion, five hundred and sixty-five million, one hundred and ninety-eight thousand, one hundred and nine\n",
            "Exact match: False\n",
            "Max answer length: 85, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is one septillion, nine hundred and forty-one sextillion, three hundred and thirty-two quintillion, nine hundred and forty-two quadrillion, seven hundred and twelve trillion, three hundred and seven billion, three hundred and thirty-six million, four hundred and fifty-two thousand, eight hundred and nineteen plus eight septillion, five hundred and sixteen sextillion, seven hundred and sixty-six quintillion, six hundred and forty-six quadrillion, five hundred and ninety-three trillion, two hundred and thirty-nine billion, nine hundred and ninety-five million, fifty-two thousand, two hundred and fifteen?\n",
            "Sample correct answer: ten septillion, four hundred and fifty-eight sextillion, ninety-nine quintillion, five hundred and eighty-nine quadrillion, three hundred and five trillion, five hundred and forty-seven billion, three hundred and thirty-one million, five hundred and five thousand and thirty-four\n",
            "Sample predicted answer: ten septillion, four hundred and fifty-eight sextillion, ninety-nine quintillion, five hundred and eighty-nine quadrillion, three hundred and five trillion, five hundred and forty-seven billion, three hundred and thirty-one million, five hundred and five thousand and thirty-four\n",
            "Exact match: True\n",
            "Max answer length: 86, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is eight septillion, seven hundred and three sextillion, seven hundred and sixty-five quintillion, five hundred and seventeen quadrillion, eight hundred and eighty-nine trillion, six hundred and thirty-four billion, nine hundred and sixty-nine million, eight hundred and sixty-five thousand, five hundred and forty-four plus three septillion, five hundred and nineteen sextillion, six hundred and eighty-six quintillion, five hundred and five quadrillion, four hundred and eighty-nine trillion, six hundred and fifty-one billion, two hundred and fifty million, eight hundred and fifty-eight thousand, two hundred and twelve?\n",
            "Sample correct answer: twelve septillion, two hundred and twenty-three sextillion, four hundred and fifty-two quintillion, twenty-three quadrillion, three hundred and seventy-nine trillion, two hundred and eighty-six billion, two hundred and twenty million, seven hundred and twenty-three thousand, seven hundred and fifty-six\n",
            "Sample predicted answer: twelve septillion, two hundred and twenty-three sextillion, four hundred and fifty-two quintillion, twenty-three quadrillion, three hundred and seventy-nine trillion, two hundred and eighty-six billion, two hundred and twenty million, seven hundred and twenty-three thousand, seven hundred and fifty-six\n",
            "Exact match: True\n",
            "Max answer length: 87, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is two hundred and thirty-nine sextillion, one hundred and fifty-nine quintillion, seven hundred and two quadrillion, seven hundred and eighty trillion, one hundred and thirty-nine billion, two hundred and forty-two thousand and twenty-nine plus eight septillion, sixteen sextillion, four hundred and seventy-seven quintillion, four hundred and thirty-four quadrillion, six hundred and ninety-four trillion, five hundred and thirty-one billion, five hundred and seventy million, eight hundred and ninety-two thousand, eight hundred and ninety-seven?\n",
            "Sample correct answer: eight septillion, two hundred and fifty-five sextillion, six hundred and thirty-seven quintillion, one hundred and thirty-seven quadrillion, four hundred and seventy-four trillion, six hundred and seventy billion, five hundred and seventy-one million, one hundred and thirty-four thousand, nine hundred and twenty-six\n",
            "Sample predicted answer: ten septillion, fifty-five sextillion, six hundred and thirty-seven quintillion, one hundred and thirty-seven quadrillion, four hundred and seventy-four trillion, six hundred and seventy billion, five hundred and fifty-one million, one hundred and thirty-four thousand, nine hundred and twenty-six\n",
            "Exact match: False\n",
            "Max answer length: 87, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:26<00:00,  1.67s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.5130\n",
            "Testing DataLoader 0: 100% 16/16 [00:26<00:00,  1.67s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5130000114440918    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing words model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/words_trained_on_30_digits_seed42/model-epoch=13-val_exact_match=0.7870.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is four hundred and forty-three octillion, five hundred and ninety-four septillion, ninety-one sextillion, six hundred and twenty-three quintillion, five hundred and seventy-two quadrillion, seven hundred and ten trillion, eight hundred and seventy-eight billion, one hundred and twenty-nine million, three hundred and sixteen thousand, eight hundred and twenty-nine plus eighteen octillion, nine hundred and seventy-one septillion, three hundred and sixty-six sextillion, nine hundred and fifty-three quintillion, six hundred and thirty-four quadrillion, five hundred and forty-seven trillion, six hundred and thirty-nine billion, eight hundred and two million, nine hundred and seventy-two thousand, nine hundred and seventy-three?\n",
            "Sample correct answer: four hundred and sixty-two octillion, five hundred and sixty-five septillion, four hundred and fifty-eight sextillion, five hundred and seventy-seven quintillion, two hundred and seven quadrillion, two hundred and fifty-eight trillion, five hundred and seventeen billion, nine hundred and thirty-two million, two hundred and eighty-nine thousand, eight hundred and two\n",
            "Sample predicted answer: fifty-two octillion, five hundred and sixty-five septillion, four hundred and fifty-eight sextillion, five hundred and seventy-seven quintillion, two hundred and seven quadrillion, two hundred and fifty-eight trillion, five hundred and seventeen billion, nine hundred and thirty-two million, two hundred and eighty-nine thousand, eight hundred and two\n",
            "Exact match: False\n",
            "Max answer length: 104, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is ninety-six octillion, one hundred and seventy-nine septillion, two hundred and fifty-six sextillion, twenty-eight quintillion, four hundred and eighty-nine quadrillion, nine hundred and seventy-seven trillion, six hundred and eleven billion, four hundred and thirty million, eighteen thousand, seven hundred and eighty-six plus six hundred and sixty octillion, forty septillion, four hundred and twenty-four sextillion, four hundred and sixty quintillion, three hundred and thirty-seven quadrillion, one hundred and twenty-one trillion, three hundred and one billion, five hundred and fifty-six million, nine hundred and thirty-eight thousand, eight hundred and thirty-six?\n",
            "Sample correct answer: seven hundred and fifty-six octillion, two hundred and nineteen septillion, six hundred and eighty sextillion, four hundred and eighty-eight quintillion, eight hundred and twenty-seven quadrillion, ninety-eight trillion, nine hundred and twelve billion, nine hundred and eighty-six million, nine hundred and fifty-seven thousand, six hundred and twenty-two\n",
            "Sample predicted answer: one nonillion, six hundred and fifty-six octillion, two hundred and nineteen septillion, six hundred and eighty sextillion, four hundred and eighty-eight quintillion, eight hundred and twenty-seven quadrillion, ninety-eight trillion, nine hundred and thirteen billion, nine hundred and eighty-six million, nine hundred and fifty-seven thousand, six hundred and twenty-two\n",
            "Exact match: False\n",
            "Max answer length: 107, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is six hundred and fifty-six octillion, nine hundred and forty-eight septillion, nine hundred and fifty-five sextillion, two hundred and thirteen quintillion, two hundred and ninety quadrillion, six hundred and forty trillion, six hundred and forty-five billion, fifty-three million, one hundred and ten thousand, seven hundred and forty-eight plus two hundred and seventy-five octillion, ninety-seven septillion, five hundred and ninety-three sextillion, two hundred and forty-three quintillion, nine quadrillion, eight hundred and thirty-nine trillion, eighty-five billion, seven hundred and ninety-nine million, one hundred and fifty-one thousand, five hundred and fourteen?\n",
            "Sample correct answer: nine hundred and thirty-two octillion, forty-six septillion, five hundred and forty-eight sextillion, four hundred and fifty-six quintillion, three hundred quadrillion, four hundred and seventy-nine trillion, seven hundred and thirty billion, eight hundred and fifty-two million, two hundred and sixty-two thousand, two hundred and sixty-two\n",
            "Sample predicted answer: nine hundred and thirty-two octillion, forty-six septillion, five hundred and forty-eight sextillion, four hundred and fifty-six quintillion, three hundred quadrillion, four hundred and seventy-nine trillion, seven hundred and thirty billion, eight hundred and fifty-two million, two hundred and sixty-two thousand, two hundred and sixty-two\n",
            "Exact match: True\n",
            "Max answer length: 107, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is ten octillion, five hundred and sixty-nine septillion, seven hundred and ninety-seven sextillion, two hundred and eighteen quintillion, one hundred and fifty quadrillion, eight hundred and thirty trillion, nine hundred and seventy-one billion, six hundred and thirty-four million, nine hundred and fifty-five thousand, seven hundred and twenty-two plus nine hundred and forty-nine octillion, two hundred and eighteen septillion, two hundred and eighty-nine sextillion, five hundred and twenty-eight quintillion, fifty-five quadrillion, six hundred and thirty trillion, four hundred and seventy-three billion, six hundred and forty-six million, seven hundred and ninety-one thousand, four hundred and ninety-seven?\n",
            "Sample correct answer: nine hundred and fifty-nine octillion, seven hundred and eighty-eight septillion, eighty-six sextillion, seven hundred and forty-six quintillion, two hundred and six quadrillion, four hundred and sixty-one trillion, four hundred and forty-five billion, two hundred and eighty-one million, seven hundred and forty-seven thousand, two hundred and nineteen\n",
            "Sample predicted answer: one nonillion, eighty-nine octillion, seven hundred and eighty-eight septillion, eighty-six sextillion, seven hundred and forty-six quintillion, two hundred and six quadrillion, four hundred and sixty-one trillion, four hundred and forty-five billion, two hundred and eighty-one million, seven hundred and forty-seven thousand, two hundred and nineteen\n",
            "Exact match: False\n",
            "Max answer length: 106, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is seven hundred and seventy-one octillion, six hundred and forty-eight septillion, seven hundred and eight sextillion, eight hundred and eight quintillion, seven hundred and forty-seven quadrillion, two hundred and sixty-four trillion, two hundred and twelve billion, two hundred and nineteen million, five hundred and fifty-seven thousand, six hundred and fifty-five plus seven hundred and forty-two octillion, two hundred and thirty-one septillion, one hundred and five sextillion, one hundred and forty-nine quintillion, three hundred and eighty-three quadrillion, five hundred and sixty trillion, eight hundred and seventy-two billion, three hundred and fifty-one million, eight hundred and ninety thousand, four hundred and sixty-one?\n",
            "Sample correct answer: one nonillion, five hundred and thirteen octillion, eight hundred and seventy-nine septillion, eight hundred and thirteen sextillion, nine hundred and fifty-eight quintillion, one hundred and thirty quadrillion, eight hundred and twenty-five trillion, eighty-four billion, five hundred and seventy-one million, four hundred and forty-eight thousand, one hundred and sixteen\n",
            "Sample predicted answer: one nonillion, five hundred and thirteen octillion, eight hundred and seventy-nine septillion, eight hundred and thirteen sextillion, nine hundred and fifty-eight quintillion, one hundred and thirty quadrillion, eight hundred and twenty-five trillion, eighty-four billion, five hundred and seventy-one million, four hundred and forty-eight thousand, one hundred and sixteen\n",
            "Exact match: True\n",
            "Max answer length: 105, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.02s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.4690\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.02s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4690000116825104    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3_5 plus 5_8?\n",
            "Sample correct answer: 9_3\n",
            "Sample predicted answer: 9_3\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1_4 plus 4_6?\n",
            "Sample correct answer: 6_0\n",
            "Sample predicted answer: 6_0\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4_9 plus 3_3?\n",
            "Sample correct answer: 8_2\n",
            "Sample predicted answer: 8_2\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8_5 plus 5_2?\n",
            "Sample correct answer: 1_3_7\n",
            "Sample predicted answer: 1_3_7\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6_9 plus 2_8?\n",
            "Sample correct answer: 9_7\n",
            "Sample predicted answer: 9_7\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  7.51it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.7940\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  7.48it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7940000295639038    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3_6_4_3_4 plus 5_9_4_2_9?\n",
            "Sample correct answer: 9_5_8_6_3\n",
            "Sample predicted answer: 9_5_8_6_3\n",
            "Exact match: True\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 7_5_9_2 plus 3_1_5_7_1?\n",
            "Sample correct answer: 3_9_1_6_3\n",
            "Sample predicted answer: 1_0_7_5_8_3\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9_1_3_0_3 plus 9_5_7_4_8?\n",
            "Sample correct answer: 1_8_7_0_5_1\n",
            "Sample predicted answer: 1_8_7_0_5_1\n",
            "Exact match: True\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 2_5_1_4_4 plus 5_5_1_0_8?\n",
            "Sample correct answer: 8_0_2_5_2\n",
            "Sample predicted answer: 8_0_2_5_2\n",
            "Exact match: True\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6_5_5_9_7 plus 7_0_7_7_6?\n",
            "Sample correct answer: 1_3_6_3_7_3\n",
            "Sample predicted answer: 1_3_6_3_7_3\n",
            "Exact match: True\n",
            "Max answer length: 12, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.17it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.5640\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.16it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5640000104904175    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9_1_8_4_0_6_4_9_0_0 plus 2_1_1_9_6_3_4_3_9_9?\n",
            "Sample correct answer: 1_1_3_0_3_6_9_9_2_9_9\n",
            "Sample predicted answer: 1_1_3_0_3_7_0_4_9\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1_8_4_5_9_1_9_8_9_5 plus 4_4_7_4_6_2_0_8_9_4?\n",
            "Sample correct answer: 6_3_2_0_5_4_0_7_8_9\n",
            "Sample predicted answer: 6_3_2_0_5_4_9_8_9_9\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3_0_8_5_3_2_0_8_2_8 plus 1_3_3_5_9_0_1_9_9_0?\n",
            "Sample correct answer: 4_4_2_1_2_2_2_8_1_8\n",
            "Sample predicted answer: 4_4_2_1_2_2_2_8\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5_2_2_8_1_3_3_4_8_6 plus 1_5_5_7_7_2_4_6_9_8?\n",
            "Sample correct answer: 6_7_8_5_8_5_8_1_8_4\n",
            "Sample predicted answer: 6_7_8_5_8_6_9_4_4_4\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8_2_1_5_4_0_0_0_9_1 plus 3_5_4_5_9_4_0_0_9_0?\n",
            "Sample correct answer: 1_1_7_6_1_3_4_0_1_8_1\n",
            "Sample predicted answer: 1_1_7_6_1_3_4_0_1\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:06<00:00,  2.37it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:06<00:00,  2.36it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6_2_9_8_0_4_3_8_3_5_5_5_1_3 plus 9_2_5_2_6_5_7_8_8_3_3_1_2_5_7?\n",
            "Sample correct answer: 9_8_8_2_4_6_2_2_6_6_8_6_7_7_0\n",
            "Sample predicted answer: 1_5_5_5_0_7_0_3_3_3_9_9_9_0_0\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1_8_5_3_6_2_4_5_0_3_9_7_9_0_4 plus 2_4_2_8_2_8_4_1_9_9_3_9_5?\n",
            "Sample correct answer: 1_8_7_7_9_0_7_3_4_5_9_7_2_9_9\n",
            "Sample predicted answer: 4_2_8_2_9_1_4_2_2_2_0_9_9_9\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3_1_0_7_8_1_2_2_3_7_9_3_8_5 plus 9_8_7_4_5_6_4_2_6_4_1_2_3_9_0?\n",
            "Sample correct answer: 1_0_1_8_5_3_4_5_4_8_7_9_1_7_7_5\n",
            "Sample predicted answer: 1_2_9_8_2_3_7_6_7_3_3_5_5\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1_1_5_5_0_2_3_6_8_8_3_1_9_8_4 plus 1_3_8_1_2_0_6_8_1_5_9_3_7_9?\n",
            "Sample correct answer: 1_2_9_3_1_4_4_3_6_9_9_1_3_6_3\n",
            "Sample predicted answer: 2_5_3_6_2_3_9_9_3_3_3_3_3\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 1_6_6_2_8_7_4_5_4_3_8_9_2_4_6 plus 1_1_5_7_4_6_0_8_8_4_2_6_8_8_5?\n",
            "Sample correct answer: 2_8_2_0_3_3_5_4_2_8_1_6_1_3_1\n",
            "Sample predicted answer: 2_8_2_0_3_3_4_9_9_9_9_9_9_1_1\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:09<00:00,  1.64it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:09<00:00,  1.64it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 5_0_1_6_9_2_5_3_2_7_5_9_6_0_3_3_7_9_0_1 plus 2_9_6_8_0_2_6_1_7_1_7_5_2_9_3_4_2_4_2_0?\n",
            "Sample correct answer: 7_9_8_4_9_5_1_4_9_9_3_4_8_9_6_8_0_3_2_1\n",
            "Sample predicted answer: 7_9_8_4_9_5_7_7_7_7_7_7_7_1_1_1_1\n",
            "Exact match: False\n",
            "Max answer length: 42, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3_6_9_0_0_9_7_6_4_2_2_1_8_5_5_3_4_2_8_8 plus 5_9_2_1_7_4_1_8_3_6_4_3_2_7_7_6_7_4_0_4?\n",
            "Sample correct answer: 9_6_1_1_8_3_9_4_7_8_6_5_1_3_3_0_1_6_9_2\n",
            "Sample predicted answer: 9_6_1_1_8_3_0_1_0_1_0_1_9_1_1_1_2_2_2_2\n",
            "Exact match: False\n",
            "Max answer length: 42, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1_3_6_5_2_2_3_8_0_9_0_6_4_2_2_3_9_5_5_9 plus 3_2_5_1_6_7_6_0_2_5_6_0_1_9_5_1_4_7_3_4?\n",
            "Sample correct answer: 4_6_1_6_8_9_9_8_3_4_6_6_6_1_7_5_4_2_9_3\n",
            "Sample predicted answer: 4_6_1_6_8_9_3_3_3_3_3_3_3_3_3_3_3_3_3_3\n",
            "Exact match: False\n",
            "Max answer length: 42, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 6_2_4_3_2_6_8_2_1_3_0_6_0_8_6_9_7_9_3_8 plus 7_9_7_1_8_0_9_8_2_5_3_3_4_2_8_6_4_9_4_7?\n",
            "Sample correct answer: 1_4_2_1_5_0_7_8_0_3_8_3_9_5_1_5_6_2_8_8_5\n",
            "Sample predicted answer: 1_4_2_1_5_0_7_8_8_3_3_3_3_3_3_3_3_3_5_5_5\n",
            "Exact match: False\n",
            "Max answer length: 42, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8_9_8_7_2_1_5_5_8_0_5_6_8_9_3_2_7_5_4_1 plus 4_3_6_4_1_1_7_8_6_0_8_3_1_2_8_1_8_0_6_6?\n",
            "Sample correct answer: 1_3_3_5_1_3_3_3_4_4_1_4_0_0_2_1_4_5_6_0_7\n",
            "Sample predicted answer: 1_3_3_5_1_3_3_3_3_3_3_3_9_3_9_9_9_7_7\n",
            "Exact match: False\n",
            "Max answer length: 42, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:12<00:00,  1.26it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:12<00:00,  1.26it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8_2_8_7_1_1_8_3_2_5_1_0_1_8_4_9_9_4_4_5_4_2_2_1_4 plus 7_7_2_5_5_2_5_6_2_9_0_9_6_1_7_6_6_6_4_4_2_6_6_1_3?\n",
            "Sample correct answer: 1_6_0_1_2_6_4_3_9_5_4_1_9_8_0_2_6_6_0_8_9_6_8_8_2_7\n",
            "Sample predicted answer: 1_6_0_1_2_6_4_3_3_9_9_9_9_9_9_9_9_9_9_9_9_7_7_7_7_7\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2_6_7_6_5_3_1_8_1_7_7_7_0_3_4_6_8_0_8_3_0_6_3_1_1 plus 7_4_0_8_9_7_3_0_0_2_5_3_0_1_5_9_7_5_6_8_9_1_7_9_8?\n",
            "Sample correct answer: 1_0_0_8_5_5_0_4_8_2_0_3_0_0_5_0_6_5_6_5_1_9_8_1_0_9\n",
            "Sample predicted answer: 1_0_0_8_5_5_0_1_0_7_7_7_7_7_7_7_7_7_7_9_9_9_9_9_9_9\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1_9_4_1_3_3_2_9_4_2_7_1_2_3_0_7_3_3_6_4_5_2_8_1_9 plus 8_5_1_6_7_6_6_6_4_6_5_9_3_2_3_9_9_9_5_0_5_2_2_1_5?\n",
            "Sample correct answer: 1_0_4_5_8_0_9_9_5_8_9_3_0_5_5_4_7_3_3_1_5_0_5_0_3_4\n",
            "Sample predicted answer: 1_0_4_6_8_1_9_9_7_7_7_7_7_7_7_7_7_7_7_7_4_4_4_4\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8_7_0_3_7_6_5_5_1_7_8_8_9_6_3_4_9_6_9_8_6_5_5_4_4 plus 3_5_1_9_6_8_6_5_0_5_4_8_9_6_5_1_2_5_0_8_5_8_2_1_2?\n",
            "Sample correct answer: 1_2_2_2_3_4_5_2_0_2_3_3_7_9_2_8_6_2_2_0_7_2_3_7_5_6\n",
            "Sample predicted answer: 1_2_2_2_3_4_5_3_3_7_3_7_3_7_3_7_3_7_7_7_7_6_6_6_6\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 2_3_9_1_5_9_7_0_2_7_8_0_1_3_9_0_0_0_2_4_2_0_2_9 plus 8_0_1_6_4_7_7_4_3_4_6_9_4_5_3_1_5_7_0_8_9_2_8_9_7?\n",
            "Sample correct answer: 8_2_5_5_6_3_7_1_3_7_4_7_4_6_7_0_5_7_1_1_3_4_9_2_6\n",
            "Sample predicted answer: 1_0_4_0_7_0_7_7_2_2_2_2_2_2_2_2_2_2_2_6_6_6_6_6\n",
            "Exact match: False\n",
            "Max answer length: 52, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:15<00:00,  1.02it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:15<00:00,  1.02it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing underscore model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/underscore_trained_on_30_digits_seed42/model-epoch=24-val_exact_match=0.1460.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 4_4_3_5_9_4_0_9_1_6_2_3_5_7_2_7_1_0_8_7_8_1_2_9_3_1_6_8_2_9 plus 1_8_9_7_1_3_6_6_9_5_3_6_3_4_5_4_7_6_3_9_8_0_2_9_7_2_9_7_3?\n",
            "Sample correct answer: 4_6_2_5_6_5_4_5_8_5_7_7_2_0_7_2_5_8_5_1_7_9_3_2_2_8_9_8_0_2\n",
            "Sample predicted answer: 6_3_3_3_0_7_7_7_9_9_9_9_9_9_9_9_9_9_9_9_9_9_9_2_2_2_2_2\n",
            "Exact match: False\n",
            "Max answer length: 62, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 9_6_1_7_9_2_5_6_0_2_8_4_8_9_9_7_7_6_1_1_4_3_0_0_1_8_7_8_6 plus 6_6_0_0_4_0_4_2_4_4_6_0_3_3_7_1_2_1_3_0_1_5_5_6_9_3_8_8_3_6?\n",
            "Sample correct answer: 7_5_6_2_1_9_6_8_0_4_8_8_8_2_7_0_9_8_9_1_2_9_8_6_9_5_7_6_2_2\n",
            "Sample predicted answer: 1_6_2_1_8_3_2_5_3_9_3_3_3_3_3_9_3_3_3_3_3_3_3_3_3_3_3_3_1_2_2\n",
            "Exact match: False\n",
            "Max answer length: 62, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 6_5_6_9_4_8_9_5_5_2_1_3_2_9_0_6_4_0_6_4_5_0_5_3_1_1_0_7_4_8 plus 2_7_5_0_9_7_5_9_3_2_4_3_0_0_9_8_3_9_0_8_5_7_9_9_1_5_1_5_1_4?\n",
            "Sample correct answer: 9_3_2_0_4_6_5_4_8_4_5_6_3_0_0_4_7_9_7_3_0_8_5_2_2_6_2_2_6_2\n",
            "Sample predicted answer: 9_3_2_0_4_6_5_3_7_7_7_7_3_7_3_7_3_7_3_3_3_3_3_3_3_3_2_2_2_2\n",
            "Exact match: False\n",
            "Max answer length: 62, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1_0_5_6_9_7_9_7_2_1_8_1_5_0_8_3_0_9_7_1_6_3_4_9_5_5_7_2_2 plus 9_4_9_2_1_8_2_8_9_5_2_8_0_5_5_6_3_0_4_7_3_6_4_6_7_9_1_4_9_7?\n",
            "Sample correct answer: 9_5_9_7_8_8_0_8_6_7_4_6_2_0_6_4_6_1_4_4_5_2_8_1_7_4_7_2_1_9\n",
            "Sample predicted answer: 1_0_5_4_9_1_6_2_2_2_2_2_2_2_9_3_9_9_9_9_9_9_9_9_9_9_9_9_9_9_9\n",
            "Exact match: False\n",
            "Max answer length: 62, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 7_7_1_6_4_8_7_0_8_8_0_8_7_4_7_2_6_4_2_1_2_2_1_9_5_5_7_6_5_5 plus 7_4_2_2_3_1_1_0_5_1_4_9_3_8_3_5_6_0_8_7_2_3_5_1_8_9_0_4_6_1?\n",
            "Sample correct answer: 1_5_1_3_8_7_9_8_1_3_9_5_8_1_3_0_8_2_5_0_8_4_5_7_1_4_4_8_1_1_6\n",
            "Sample predicted answer: 1_5_1_4_8_8_9_9_7_7_7_3_7_3_7_3_3_3_3_9_3_9_9_9_9_9_3_6_6\n",
            "Exact match: False\n",
            "Max answer length: 62, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:18<00:00,  1.18s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:18<00:00,  1.18s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 5 plus 5 8?\n",
            "Sample correct answer: 9 3\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 4 plus 4 6?\n",
            "Sample correct answer: 6 0\n",
            "Sample predicted answer: 1 4 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 9 plus 3 3?\n",
            "Sample correct answer: 8 2\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 5 plus 5 2?\n",
            "Sample correct answer: 1 3 7\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 9 plus 2 8?\n",
            "Sample correct answer: 9 7\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.96s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0010\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.96s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0010000000474974513  \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 6 4 3 4 plus 5 9 4 2 9?\n",
            "Sample correct answer: 9 5 8 6 3\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 0 7 5 9 2 plus 3 1 5 7 1?\n",
            "Sample correct answer: 3 9 1 6 3\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 1 3 0 3 plus 9 5 7 4 8?\n",
            "Sample correct answer: 1 8 7 0 5 1\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 5 1 4 4 plus 5 5 1 0 8?\n",
            "Sample correct answer: 8 0 2 5 2\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 5 5 9 7 plus 7 0 7 7 6?\n",
            "Sample correct answer: 1 3 6 3 7 3\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.96s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.97s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 1 8 4 0 6 4 9 0 0 plus 2 1 1 9 6 3 4 3 9 9?\n",
            "Sample correct answer: 1 1 3 0 3 6 9 9 2 9 9\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 8 4 5 9 1 9 8 9 5 plus 4 4 7 4 6 2 0 8 9 4?\n",
            "Sample correct answer: 6 3 2 0 5 4 0 7 8 9\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 0 8 5 3 2 0 8 2 8 plus 1 3 3 5 9 0 1 9 9 0?\n",
            "Sample correct answer: 4 4 2 1 2 2 2 8 1 8\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 2 2 8 1 3 3 4 8 6 plus 1 5 5 7 7 2 4 6 9 8?\n",
            "Sample correct answer: 6 7 8 5 8 5 8 1 8 4\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 2 1 5 4 0 0 0 9 1 plus 3 5 4 5 9 4 0 0 9 0?\n",
            "Sample correct answer: 1 1 7 6 1 3 4 0 1 8 1\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.98s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.98s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 0 6 2 9 8 0 4 3 8 3 5 5 5 1 3 plus 9 2 5 2 6 5 7 8 8 3 3 1 2 5 7?\n",
            "Sample correct answer: 9 8 8 2 4 6 2 2 6 6 8 6 7 7 0\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 8 5 3 6 2 4 5 0 3 9 7 9 0 4 plus 0 0 2 4 2 8 2 8 4 1 9 9 3 9 5?\n",
            "Sample correct answer: 1 8 7 7 9 0 7 3 4 5 9 7 2 9 9\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 0 3 1 0 7 8 1 2 2 3 7 9 3 8 5 plus 9 8 7 4 5 6 4 2 6 4 1 2 3 9 0?\n",
            "Sample correct answer: 1 0 1 8 5 3 4 5 4 8 7 9 1 7 7 5\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 1 5 5 0 2 3 6 8 8 3 1 9 8 4 plus 0 1 3 8 1 2 0 6 8 1 5 9 3 7 9?\n",
            "Sample correct answer: 1 2 9 3 1 4 4 3 6 9 9 1 3 6 3\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 6 6 2 8 7 4 5 4 3 8 9 2 4 6 plus 1 1 5 7 4 6 0 8 8 4 2 6 8 8 5?\n",
            "Sample correct answer: 2 8 2 0 3 3 5 4 2 8 1 6 1 3 1\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.98s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.98s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 0 1 6 9 2 5 3 2 7 5 9 6 0 3 3 7 9 0 1 plus 2 9 6 8 0 2 6 1 7 1 7 5 2 9 3 4 2 4 2 0?\n",
            "Sample correct answer: 7 9 8 4 9 5 1 4 9 9 3 4 8 9 6 8 0 3 2 1\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 6 9 0 0 9 7 6 4 2 2 1 8 5 5 3 4 2 8 8 plus 5 9 2 1 7 4 1 8 3 6 4 3 2 7 7 6 7 4 0 4?\n",
            "Sample correct answer: 9 6 1 1 8 3 9 4 7 8 6 5 1 3 3 0 1 6 9 2\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 3 6 5 2 2 3 8 0 9 0 6 4 2 2 3 9 5 5 9 plus 3 2 5 1 6 7 6 0 2 5 6 0 1 9 5 1 4 7 3 4?\n",
            "Sample correct answer: 4 6 1 6 8 9 9 8 3 4 6 6 6 1 7 5 4 2 9 3\n",
            "Sample predicted answer: 1\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 2 4 3 2 6 8 2 1 3 0 6 0 8 6 9 7 9 3 8 plus 7 9 7 1 8 0 9 8 2 5 3 3 4 2 8 6 4 9 4 7?\n",
            "Sample correct answer: 1 4 2 1 5 0 7 8 0 3 8 3 9 5 1 5 6 2 8 8 5\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 9 8 7 2 1 5 5 8 0 5 6 8 9 3 2 7 5 4 1 plus 4 3 6 4 1 1 7 8 6 0 8 3 1 2 8 1 8 0 6 6?\n",
            "Sample correct answer: 1 3 3 5 1 3 3 3 4 4 1 4 0 0 2 1 4 5 6 0 7\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.99s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:31<00:00,  1.99s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 2 8 7 1 1 8 3 2 5 1 0 1 8 4 9 9 4 4 5 4 2 2 1 4 plus 7 7 2 5 5 2 5 6 2 9 0 9 6 1 7 6 6 6 4 4 2 6 6 1 3?\n",
            "Sample correct answer: 1 6 0 1 2 6 4 3 9 5 4 1 9 8 0 2 6 6 0 8 9 6 8 8 2 7\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2 6 7 6 5 3 1 8 1 7 7 7 0 3 4 6 8 0 8 3 0 6 3 1 1 plus 7 4 0 8 9 7 3 0 0 2 5 3 0 1 5 9 7 5 6 8 9 1 7 9 8?\n",
            "Sample correct answer: 1 0 0 8 5 5 0 4 8 2 0 3 0 0 5 0 6 5 6 5 1 9 8 1 0 9\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 9 4 1 3 3 2 9 4 2 7 1 2 3 0 7 3 3 6 4 5 2 8 1 9 plus 8 5 1 6 7 6 6 6 4 6 5 9 3 2 3 9 9 9 5 0 5 2 2 1 5?\n",
            "Sample correct answer: 1 0 4 5 8 0 9 9 5 8 9 3 0 5 5 4 7 3 3 1 5 0 5 0 3 4\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 7 0 3 7 6 5 5 1 7 8 8 9 6 3 4 9 6 9 8 6 5 5 4 4 plus 3 5 1 9 6 8 6 5 0 5 4 8 9 6 5 1 2 5 0 8 5 8 2 1 2?\n",
            "Sample correct answer: 1 2 2 2 3 4 5 2 0 2 3 3 7 9 2 8 6 2 2 0 7 2 3 7 5 6\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 0 2 3 9 1 5 9 7 0 2 7 8 0 1 3 9 0 0 0 2 4 2 0 2 9 plus 8 0 1 6 4 7 7 4 3 4 6 9 4 5 3 1 5 7 0 8 9 2 8 9 7?\n",
            "Sample correct answer: 8 2 5 5 6 3 7 1 3 7 4 7 4 6 7 0 5 7 1 1 3 4 9 2 6\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.00s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.00s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character_fixed model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_fixed_trained_on_30_digits_seed42/model-epoch=00-val_exact_match=0.0000.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 4 3 5 9 4 0 9 1 6 2 3 5 7 2 7 1 0 8 7 8 1 2 9 3 1 6 8 2 9 plus 0 1 8 9 7 1 3 6 6 9 5 3 6 3 4 5 4 7 6 3 9 8 0 2 9 7 2 9 7 3?\n",
            "Sample correct answer: 4 6 2 5 6 5 4 5 8 5 7 7 2 0 7 2 5 8 5 1 7 9 3 2 2 8 9 8 0 2\n",
            "Sample predicted answer: 1\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 0 9 6 1 7 9 2 5 6 0 2 8 4 8 9 9 7 7 6 1 1 4 3 0 0 1 8 7 8 6 plus 6 6 0 0 4 0 4 2 4 4 6 0 3 3 7 1 2 1 3 0 1 5 5 6 9 3 8 8 3 6?\n",
            "Sample correct answer: 7 5 6 2 1 9 6 8 0 4 8 8 8 2 7 0 9 8 9 1 2 9 8 6 9 5 7 6 2 2\n",
            "Sample predicted answer: \n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 5 6 9 4 8 9 5 5 2 1 3 2 9 0 6 4 0 6 4 5 0 5 3 1 1 0 7 4 8 plus 2 7 5 0 9 7 5 9 3 2 4 3 0 0 9 8 3 9 0 8 5 7 9 9 1 5 1 5 1 4?\n",
            "Sample correct answer: 9 3 2 0 4 6 5 4 8 4 5 6 3 0 0 4 7 9 7 3 0 8 5 2 2 6 2 2 6 2\n",
            "Sample predicted answer: 1\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 0 1 0 5 6 9 7 9 7 2 1 8 1 5 0 8 3 0 9 7 1 6 3 4 9 5 5 7 2 2 plus 9 4 9 2 1 8 2 8 9 5 2 8 0 5 5 6 3 0 4 7 3 6 4 6 7 9 1 4 9 7?\n",
            "Sample correct answer: 9 5 9 7 8 8 0 8 6 7 4 6 2 0 6 4 6 1 4 4 5 2 8 1 7 4 7 2 1 9\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 7 1 6 4 8 7 0 8 8 0 8 7 4 7 2 6 4 2 1 2 2 1 9 5 5 7 6 5 5 plus 7 4 2 2 3 1 1 0 5 1 4 9 3 8 3 5 6 0 8 7 2 3 5 1 8 9 0 4 6 1?\n",
            "Sample correct answer: 1 5 1 3 8 7 9 8 1 3 9 5 8 1 3 0 8 2 5 0 8 4 5 7 1 4 4 8 1 1 6\n",
            "Sample predicted answer: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 32, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.02s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:32<00:00,  2.02s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 5 plus 5 8?\n",
            "Sample correct answer: 9 3\n",
            "Sample predicted answer: 9 3\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 4 plus 4 6?\n",
            "Sample correct answer: 6 0\n",
            "Sample predicted answer: 6 0\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 9 plus 3 3?\n",
            "Sample correct answer: 8 2\n",
            "Sample predicted answer: 8 2\n",
            "Exact match: True\n",
            "Max answer length: 5, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 5 plus 5 2?\n",
            "Sample correct answer: 1 3 7\n",
            "Sample predicted answer: 1 3 7\n",
            "Exact match: True\n",
            "Max answer length: 6, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 9 plus 2 8?\n",
            "Sample correct answer: 9 7\n",
            "Sample predicted answer: 9 7\n",
            "Exact match: True\n",
            "Max answer length: 5, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  7.94it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.7140\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  7.91it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7139999866485596    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 6 4 3 4 plus 5 9 4 2 9?\n",
            "Sample correct answer: 9 5 8 6 3\n",
            "Sample predicted answer: 9 5 8 7 3\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 5 9 2 plus 3 1 5 7 1?\n",
            "Sample correct answer: 3 9 1 6 3\n",
            "Sample predicted answer: 1 0 6 5 5 3\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 1 3 0 3 plus 9 5 7 4 8?\n",
            "Sample correct answer: 1 8 7 0 5 1\n",
            "Sample predicted answer: 1 8 6 0 4 1\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 5 1 4 4 plus 5 5 1 0 8?\n",
            "Sample correct answer: 8 0 2 5 2\n",
            "Sample predicted answer: 7 0 2 4 2\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 5 5 9 7 plus 7 0 7 7 6?\n",
            "Sample correct answer: 1 3 6 3 7 3\n",
            "Sample predicted answer: 1 3 6 4 6 4\n",
            "Exact match: False\n",
            "Max answer length: 10, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.92it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0790\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.91it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07900000363588333   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 1 8 4 0 6 4 9 0 0 plus 2 1 1 9 6 3 4 3 9 9?\n",
            "Sample correct answer: 1 1 3 0 3 6 9 9 2 9 9\n",
            "Sample predicted answer: 1 1 2 9 4 2 8 8 8 0 9 9\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 8 4 5 9 1 9 8 9 5 plus 4 4 7 4 6 2 0 8 9 4?\n",
            "Sample correct answer: 6 3 2 0 5 4 0 7 8 9\n",
            "Sample predicted answer: 6 3 2 9 9 1 1 1 8 8 8\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 0 8 5 3 2 0 8 2 8 plus 1 3 3 5 9 0 1 9 9 0?\n",
            "Sample correct answer: 4 4 2 1 2 2 2 8 1 8\n",
            "Sample predicted answer: 4 4 1 5 5 2 2 2 8 8\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 2 2 8 1 3 3 4 8 6 plus 1 5 5 7 7 2 4 6 9 8?\n",
            "Sample correct answer: 6 7 8 5 8 5 8 1 8 4\n",
            "Sample predicted answer: 6 7 8 0 0 0 0 7 7 5 5\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 2 1 5 4 0 0 0 9 1 plus 3 5 4 5 9 4 0 0 9 0?\n",
            "Sample correct answer: 1 1 7 6 1 3 4 0 1 8 1\n",
            "Sample predicted answer: 1 1 7 5 7 4 4 4 0 0 1 1\n",
            "Exact match: False\n",
            "Max answer length: 15, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  2.78it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  2.78it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 2 9 8 0 4 3 8 3 5 5 5 1 3 plus 9 2 5 2 6 5 7 8 8 3 3 1 2 5 7?\n",
            "Sample correct answer: 9 8 8 2 4 6 2 2 6 6 8 6 7 7 0\n",
            "Sample predicted answer: 1 5 5 5 2 2 2 2 2 2 2 2 2 2 8 8 8\n",
            "Exact match: False\n",
            "Max answer length: 21, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 8 5 3 6 2 4 5 0 3 9 7 9 0 4 plus 2 4 2 8 2 8 4 1 9 9 3 9 5?\n",
            "Sample correct answer: 1 8 7 7 9 0 7 3 4 5 9 7 2 9 9\n",
            "Sample predicted answer: 4 2 7 7 7 7 9 9 9 9 9 9 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 21, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 1 0 7 8 1 2 2 3 7 9 3 8 5 plus 9 8 7 4 5 6 4 2 6 4 1 2 3 9 0?\n",
            "Sample correct answer: 1 0 1 8 5 3 4 5 4 8 7 9 1 7 7 5\n",
            "Sample predicted answer: 1 3 0 7 2 2 2 2 2 2 2 2 5 5 5 5\n",
            "Exact match: False\n",
            "Max answer length: 21, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 1 5 5 0 2 3 6 8 8 3 1 9 8 4 plus 1 3 8 1 2 0 6 8 1 5 9 3 7 9?\n",
            "Sample correct answer: 1 2 9 3 1 4 4 3 6 9 9 1 3 6 3\n",
            "Sample predicted answer: 2 4 4 6 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            "Exact match: False\n",
            "Max answer length: 22, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 6 6 2 8 7 4 5 4 3 8 9 2 4 6 plus 1 1 5 7 4 6 0 8 8 4 2 6 8 8 5?\n",
            "Sample correct answer: 2 8 2 0 3 3 5 4 2 8 1 6 1 3 1\n",
            "Sample predicted answer: 2 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 21, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:07<00:00,  2.15it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:07<00:00,  2.14it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 0 1 6 9 2 5 3 2 7 5 9 6 0 3 3 7 9 0 1 plus 2 9 6 8 0 2 6 1 7 1 7 5 2 9 3 4 2 4 2 0?\n",
            "Sample correct answer: 7 9 8 4 9 5 1 4 9 9 3 4 8 9 6 8 0 3 2 1\n",
            "Sample predicted answer: 7 9 7 4 4 4 4 4 4 3 3 3 3 3 3 3 3 1 1 1 1 1 1\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 6 9 0 0 9 7 6 4 2 2 1 8 5 5 3 4 2 8 8 plus 5 9 2 1 7 4 1 8 3 6 4 3 2 7 7 6 7 4 0 4?\n",
            "Sample correct answer: 9 6 1 1 8 3 9 4 7 8 6 5 1 3 3 0 1 6 9 2\n",
            "Sample predicted answer: 9 5 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n",
            "Exact match: False\n",
            "Max answer length: 26, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 3 6 5 2 2 3 8 0 9 0 6 4 2 2 3 9 5 5 9 plus 3 2 5 1 6 7 6 0 2 5 6 0 1 9 5 1 4 7 3 4?\n",
            "Sample correct answer: 4 6 1 6 8 9 9 8 3 4 6 6 6 1 7 5 4 2 9 3\n",
            "Sample predicted answer: 4 5 0 8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 2 4 3 2 6 8 2 1 3 0 6 0 8 6 9 7 9 3 8 plus 7 9 7 1 8 0 9 8 2 5 3 3 4 2 8 6 4 9 4 7?\n",
            "Sample correct answer: 1 4 2 1 5 0 7 8 0 3 8 3 9 5 1 5 6 2 8 8 5\n",
            "Sample predicted answer: 1 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 9 8 7 2 1 5 5 8 0 5 6 8 9 3 2 7 5 4 1 plus 4 3 6 4 1 1 7 8 6 0 8 3 1 2 8 1 8 0 6 6?\n",
            "Sample correct answer: 1 3 3 5 1 3 3 3 4 4 1 4 0 0 2 1 4 5 6 0 7\n",
            "Sample predicted answer: 1 3 3 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 7 7 7 7 7 7\n",
            "Exact match: False\n",
            "Max answer length: 27, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:09<00:00,  1.67it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:09<00:00,  1.67it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 2 8 7 1 1 8 3 2 5 1 0 1 8 4 9 9 4 4 5 4 2 2 1 4 plus 7 7 2 5 5 2 5 6 2 9 0 9 6 1 7 6 6 6 4 4 2 6 6 1 3?\n",
            "Sample correct answer: 1 6 0 1 2 6 4 3 9 5 4 1 9 8 0 2 6 6 0 8 9 6 8 8 2 7\n",
            "Sample predicted answer: 1 6 0 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 33, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2 6 7 6 5 3 1 8 1 7 7 7 0 3 4 6 8 0 8 3 0 6 3 1 1 plus 7 4 0 8 9 7 3 0 0 2 5 3 0 1 5 9 7 5 6 8 9 1 7 9 8?\n",
            "Sample correct answer: 1 0 0 8 5 5 0 4 8 2 0 3 0 0 5 0 6 5 6 5 1 9 8 1 0 9\n",
            "Sample predicted answer: 1 0 0 7 7 6 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 9 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 35, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 9 4 1 3 3 2 9 4 2 7 1 2 3 0 7 3 3 6 4 5 2 8 1 9 plus 8 5 1 6 7 6 6 6 4 6 5 9 3 2 3 9 9 9 5 0 5 2 2 1 5?\n",
            "Sample correct answer: 1 0 4 5 8 0 9 9 5 8 9 3 0 5 5 4 7 3 3 1 5 0 5 0 3 4\n",
            "Sample predicted answer: 1 0 4 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 6 6 6 2 2 4 4 4 4 4 4\n",
            "Exact match: False\n",
            "Max answer length: 33, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 7 0 3 7 6 5 5 1 7 8 8 9 6 3 4 9 6 9 8 6 5 5 4 4 plus 3 5 1 9 6 8 6 5 0 5 4 8 9 6 5 1 2 5 0 8 5 8 2 1 2?\n",
            "Sample correct answer: 1 2 2 2 3 4 5 2 0 2 3 3 7 9 2 8 6 2 2 0 7 2 3 7 5 6\n",
            "Sample predicted answer: 1 2 2 1 1 1 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 34, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 3 9 1 5 9 7 0 2 7 8 0 1 3 9 0 0 0 2 4 2 0 2 9 plus 8 0 1 6 4 7 7 4 3 4 6 9 4 5 3 1 5 7 0 8 9 2 8 9 7?\n",
            "Sample correct answer: 8 2 5 5 6 3 7 1 3 7 4 7 4 6 7 0 5 7 1 1 3 4 9 2 6\n",
            "Sample predicted answer: 1 0 3 1 0 8 8 8 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 6 6\n",
            "Exact match: False\n",
            "Max answer length: 33, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:12<00:00,  1.30it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:12<00:00,  1.30it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing character model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/character_trained_on_30_digits_seed42/model-epoch=22-val_exact_match=0.0630.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 4 3 5 9 4 0 9 1 6 2 3 5 7 2 7 1 0 8 7 8 1 2 9 3 1 6 8 2 9 plus 1 8 9 7 1 3 6 6 9 5 3 6 3 4 5 4 7 6 3 9 8 0 2 9 7 2 9 7 3?\n",
            "Sample correct answer: 4 6 2 5 6 5 4 5 8 5 7 7 2 0 7 2 5 8 5 1 7 9 3 2 2 8 9 8 0 2\n",
            "Sample predicted answer: 6 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 40, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 9 6 1 7 9 2 5 6 0 2 8 4 8 9 9 7 7 6 1 1 4 3 0 0 1 8 7 8 6 plus 6 6 0 0 4 0 4 2 4 4 6 0 3 3 7 1 2 1 3 0 1 5 5 6 9 3 8 8 3 6?\n",
            "Sample correct answer: 7 5 6 2 1 9 6 8 0 4 8 8 8 2 7 0 9 8 9 1 2 9 8 6 9 5 7 6 2 2\n",
            "Sample predicted answer: 1 6 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n",
            "Exact match: False\n",
            "Max answer length: 38, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 5 6 9 4 8 9 5 5 2 1 3 2 9 0 6 4 0 6 4 5 0 5 3 1 1 0 7 4 8 plus 2 7 5 0 9 7 5 9 3 2 4 3 0 0 9 8 3 9 0 8 5 7 9 9 1 5 1 5 1 4?\n",
            "Sample correct answer: 9 3 2 0 4 6 5 4 8 4 5 6 3 0 0 4 7 9 7 3 0 8 5 2 2 6 2 2 6 2\n",
            "Sample predicted answer: 9 3 1 9 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 2 2 2 2 2 2\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 0 5 6 9 7 9 7 2 1 8 1 5 0 8 3 0 9 7 1 6 3 4 9 5 5 7 2 2 plus 9 4 9 2 1 8 2 8 9 5 2 8 0 5 5 6 3 0 4 7 3 6 4 6 7 9 1 4 9 7?\n",
            "Sample correct answer: 9 5 9 7 8 8 0 8 6 7 4 6 2 0 6 4 6 1 4 4 5 2 8 1 7 4 7 2 1 9\n",
            "Sample predicted answer: 1 0 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 9 9 9\n",
            "Exact match: False\n",
            "Max answer length: 39, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 7 1 6 4 8 7 0 8 8 0 8 7 4 7 2 6 4 2 1 2 2 1 9 5 5 7 6 5 5 plus 7 4 2 2 3 1 1 0 5 1 4 9 3 8 3 5 6 0 8 7 2 3 5 1 8 9 0 4 6 1?\n",
            "Sample correct answer: 1 5 1 3 8 7 9 8 1 3 9 5 8 1 3 0 8 2 5 0 8 4 5 7 1 4 4 8 1 1 6\n",
            "Sample predicted answer: 1 5 1 3 4 9 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 6 6\n",
            "Exact match: False\n",
            "Max answer length: 38, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:14<00:00,  1.07it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:14<00:00,  1.07it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 10 5 plus 5 10 8?\n",
            "Sample correct answer: 9 10 3\n",
            "Sample predicted answer: 9 10 3\n",
            "Exact match: True\n",
            "Max answer length: 8, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 10 4 plus 4 10 6?\n",
            "Sample correct answer: 6 10 0\n",
            "Sample predicted answer: 6 10 0\n",
            "Exact match: True\n",
            "Max answer length: 8, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 4 10 9 plus 3 10 3?\n",
            "Sample correct answer: 8 10 2\n",
            "Sample predicted answer: 8 10 2\n",
            "Exact match: True\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 10 5 plus 5 10 2?\n",
            "Sample correct answer: 1 100 3 10 7\n",
            "Sample predicted answer: 1 100 3 10 7\n",
            "Exact match: True\n",
            "Max answer length: 8, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 10 9 plus 2 10 8?\n",
            "Sample correct answer: 9 10 7\n",
            "Sample predicted answer: 9 10 7\n",
            "Exact match: True\n",
            "Max answer length: 7, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.54it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8230\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.53it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8230000138282776    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 3 10000 6 1000 4 100 3 10 4 plus 5 10000 9 1000 4 100 2 10 9?\n",
            "Sample correct answer: 9 10000 5 1000 8 100 6 10 3\n",
            "Sample predicted answer: 9 10000 5 1000 8 100 6 10 3\n",
            "Exact match: True\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 7 1000 5 100 9 10 2 plus 3 10000 1 1000 5 100 7 10 1?\n",
            "Sample correct answer: 3 10000 9 1000 1 100 6 10 3\n",
            "Sample predicted answer: 1 100000 0 10000 3 1000 1 100 6 10 3\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 9 10000 1 1000 3 100 0 10 3 plus 9 10000 5 1000 7 100 4 10 8?\n",
            "Sample correct answer: 1 100000 8 10000 7 1000 0 100 5 10 1\n",
            "Sample predicted answer: 1 100000 8 10000 7 1000 0 100 5 10 1\n",
            "Exact match: True\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 2 10000 5 1000 1 100 4 10 4 plus 5 10000 5 1000 1 100 0 10 8?\n",
            "Sample correct answer: 8 10000 0 1000 2 100 5 10 2\n",
            "Sample predicted answer: 8 10000 0 1000 2 100 5 10 2\n",
            "Exact match: True\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 6 10000 5 1000 5 100 9 10 7 plus 7 10000 0 1000 7 100 7 10 6?\n",
            "Sample correct answer: 1 100000 3 10000 6 1000 3 100 7 10 3\n",
            "Sample predicted answer: 1 100000 3 10000 6 1000 3 100 7 10 3\n",
            "Exact match: True\n",
            "Max answer length: 17, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  3.15it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8290\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  3.15it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8289999961853027    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9 1000000000 1 100000000 8 10000000 4 1000000 0 100000 6 10000 4 1000 9 100 0 10 0 plus 2 1000000000 1 100000000 1 10000000 9 1000000 6 100000 3 10000 4 1000 3 100 9 10 9?\n",
            "Sample correct answer: 1 10000000000 1 1000000000 3 100000000 0 10000000 3 1000000 6 100000 9 10000 9 1000 2 100 9 10 9\n",
            "Sample predicted answer: 1 10000000000 1 1000000000 3 100000000 0 10000000 3 1000000 6 100000 9 10000 9 1000 2 100 9 10 9\n",
            "Exact match: True\n",
            "Max answer length: 36, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 1000000000 8 100000000 4 10000000 5 1000000 9 100000 1 10000 9 1000 8 100 9 10 5 plus 4 1000000000 4 100000000 7 10000000 4 1000000 6 100000 2 10000 0 1000 8 100 9 10 4?\n",
            "Sample correct answer: 6 1000000000 3 100000000 2 10000000 0 1000000 5 100000 4 10000 0 1000 7 100 8 10 9\n",
            "Sample predicted answer: 6 1000000000 3 100000000 2 10000000 0 1000000 5 100000 4 10000 0 1000 7 100 8 10 9\n",
            "Exact match: True\n",
            "Max answer length: 35, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 1000000000 0 100000000 8 10000000 5 1000000 3 100000 2 10000 0 1000 8 100 2 10 8 plus 1 1000000000 3 100000000 3 10000000 5 1000000 9 100000 0 10000 1 1000 9 100 9 10 0?\n",
            "Sample correct answer: 4 1000000000 4 100000000 2 10000000 1 1000000 2 100000 2 10000 2 1000 8 100 1 10 8\n",
            "Sample predicted answer: 4 1000000000 4 100000000 2 10000000 1 1000000 2 100000 2 10000 2 1000 8 100 1 10 8\n",
            "Exact match: True\n",
            "Max answer length: 34, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5 1000000000 2 100000000 2 10000000 8 1000000 1 100000 3 10000 3 1000 4 100 8 10 6 plus 1 1000000000 5 100000000 5 10000000 7 1000000 7 100000 2 10000 4 1000 6 100 9 10 8?\n",
            "Sample correct answer: 6 1000000000 7 100000000 8 10000000 5 1000000 8 100000 5 10000 8 1000 1 100 8 10 4\n",
            "Sample predicted answer: 6 1000000000 7 100000000 8 10000000 5 1000000 8 100000 5 10000 8 1000 1 100 8 10 4\n",
            "Exact match: True\n",
            "Max answer length: 35, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 1000000000 2 100000000 1 10000000 5 1000000 4 100000 0 10000 0 1000 0 100 9 10 1 plus 3 1000000000 5 100000000 4 10000000 5 1000000 9 100000 4 10000 0 1000 0 100 9 10 0?\n",
            "Sample correct answer: 1 10000000000 1 1000000000 7 100000000 6 10000000 1 1000000 3 100000 4 10000 0 1000 1 100 8 10 1\n",
            "Sample predicted answer: 1 10000000000 1 1000000000 7 100000000 6 10000000 1 1000000 3 100000 4 10000 0 1000 1 100 8 10 1\n",
            "Exact match: True\n",
            "Max answer length: 35, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:10<00:00,  1.49it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8380\n",
            "Testing DataLoader 0: 100% 16/16 [00:10<00:00,  1.49it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8379999995231628    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 6 10000000000000 2 1000000000000 9 100000000000 8 10000000000 0 1000000000 4 100000000 3 10000000 8 1000000 3 100000 5 10000 5 1000 5 100 1 10 3 plus 9 100000000000000 2 10000000000000 5 1000000000000 2 100000000000 6 10000000000 5 1000000000 7 100000000 8 10000000 8 1000000 3 100000 3 10000 1 1000 2 100 5 10 7?\n",
            "Sample correct answer: 9 100000000000000 8 10000000000000 8 1000000000000 2 100000000000 4 10000000000 6 1000000000 2 100000000 2 10000000 6 1000000 6 100000 8 10000 6 1000 7 100 7 10 0\n",
            "Sample predicted answer: 1 1000000000000000 5 100000000000000 4 10000000000000 8 1000000000000 2 100000000000 4 10000000000 6 1000000000 2 100000000 2 10000000 6 1000000 6 100000 8 10000 6 1000 7 100 7 10 0\n",
            "Exact match: False\n",
            "Max answer length: 59, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1 100000000000000 8 10000000000000 5 1000000000000 3 100000000000 6 10000000000 2 1000000000 4 100000000 5 10000000 0 1000000 3 100000 9 10000 7 1000 9 100 0 10 4 plus 2 1000000000000 4 100000000000 2 10000000000 8 1000000000 2 100000000 8 10000000 4 1000000 1 100000 9 10000 9 1000 3 100 9 10 5?\n",
            "Sample correct answer: 1 100000000000000 8 10000000000000 7 1000000000000 7 100000000000 9 10000000000 0 1000000000 7 100000000 3 10000000 4 1000000 5 100000 9 10000 7 1000 2 100 9 10 9\n",
            "Sample predicted answer: 4 100000000000000 0 10000000000000 8 1000000000000 7 100000000000 9 10000000000 0 1000000000 7 100000000 3 10000000 4 1000000 5 100000 9 10000 7 1000 2 100 9 10 9\n",
            "Exact match: False\n",
            "Max answer length: 59, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3 10000000000000 1 1000000000000 0 100000000000 7 10000000000 8 1000000000 1 100000000 2 10000000 2 1000000 3 100000 7 10000 9 1000 3 100 8 10 5 plus 9 100000000000000 8 10000000000000 7 1000000000000 4 100000000000 5 10000000000 6 1000000000 4 100000000 2 10000000 6 1000000 4 100000 1 10000 2 1000 3 100 9 10 0?\n",
            "Sample correct answer: 1 1000000000000000 0 100000000000000 1 10000000000000 8 1000000000000 5 100000000000 3 10000000000 4 1000000000 5 100000000 4 10000000 8 1000000 7 100000 9 10000 1 1000 7 100 7 10 5\n",
            "Sample predicted answer: 1 1000000000000000 3 100000000000000 6 10000000000000 8 1000000000000 5 100000000000 3 10000000000 4 1000000000 5 100000000 4 10000000 8 1000000 7 100000 9 10000 1 1000 7 100 7 10 5\n",
            "Exact match: False\n",
            "Max answer length: 60, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 100000000000000 1 10000000000000 5 1000000000000 5 100000000000 0 10000000000 2 1000000000 3 100000000 6 10000000 8 1000000 8 100000 3 10000 1 1000 9 100 8 10 4 plus 1 10000000000000 3 1000000000000 8 100000000000 1 10000000000 2 1000000000 0 100000000 6 10000000 8 1000000 1 100000 5 10000 9 1000 3 100 7 10 9?\n",
            "Sample correct answer: 1 100000000000000 2 10000000000000 9 1000000000000 3 100000000000 1 10000000000 4 1000000000 4 100000000 3 10000000 6 1000000 9 100000 9 10000 1 1000 3 100 6 10 3\n",
            "Sample predicted answer: 2 100000000000000 2 10000000000000 9 1000000000000 3 100000000000 1 10000000000 4 1000000000 4 100000000 3 10000000 6 1000000 9 100000 9 10000 1 1000 3 100 6 10 3\n",
            "Exact match: False\n",
            "Max answer length: 61, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 1 100000000000000 6 10000000000000 6 1000000000000 2 100000000000 8 10000000000 7 1000000000 4 100000000 5 10000000 4 1000000 3 100000 8 10000 9 1000 2 100 4 10 6 plus 1 100000000000000 1 10000000000000 5 1000000000000 7 100000000000 4 10000000000 6 1000000000 0 100000000 8 10000000 8 1000000 4 100000 2 10000 6 1000 8 100 8 10 5?\n",
            "Sample correct answer: 2 100000000000000 8 10000000000000 2 1000000000000 0 100000000000 3 10000000000 3 1000000000 5 100000000 4 10000000 2 1000000 8 100000 1 10000 6 1000 1 100 3 10 1\n",
            "Sample predicted answer: 2 100000000000000 8 10000000000000 2 1000000000000 0 100000000000 3 10000000000 3 1000000000 5 100000000 4 10000000 2 1000000 8 100000 1 10000 6 1000 1 100 3 10 1\n",
            "Exact match: True\n",
            "Max answer length: 60, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:18<00:00,  1.15s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8210\n",
            "Testing DataLoader 0: 100% 16/16 [00:18<00:00,  1.15s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8209999799728394    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 5 10000000000000000000 0 1000000000000000000 1 100000000000000000 6 10000000000000000 9 1000000000000000 2 100000000000000 5 10000000000000 3 1000000000000 2 100000000000 7 10000000000 5 1000000000 9 100000000 6 10000000 0 1000000 3 100000 3 10000 7 1000 9 100 0 10 1 plus 2 10000000000000000000 9 1000000000000000000 6 100000000000000000 8 10000000000000000 0 1000000000000000 2 100000000000000 6 10000000000000 1 1000000000000 7 100000000000 1 10000000000 7 1000000000 5 100000000 2 10000000 9 1000000 3 100000 4 10000 2 1000 4 100 2 10 0?\n",
            "Sample correct answer: 7 10000000000000000000 9 1000000000000000000 8 100000000000000000 4 10000000000000000 9 1000000000000000 5 100000000000000 1 10000000000000 4 1000000000000 9 100000000000 9 10000000000 3 1000000000 4 100000000 8 10000000 9 1000000 6 100000 8 10000 0 1000 3 100 2 10 1\n",
            "Sample predicted answer: 7 10000000000000000000 9 1000000000000000000 8 100000000000000000 4 10000000000000000 9 1000000000000000 5 100000000000000 1 10000000000000 4 1000000000000 9 100000000000 9 10000000000 3 1000000000 4 100000000 8 10000000 9 1000000 6 100000 8 10000 0 1000 3 100 2 10 1\n",
            "Exact match: True\n",
            "Max answer length: 92, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 3 10000000000000000000 6 1000000000000000000 9 100000000000000000 0 10000000000000000 0 1000000000000000 9 100000000000000 7 10000000000000 6 1000000000000 4 100000000000 2 10000000000 2 1000000000 1 100000000 8 10000000 5 1000000 5 100000 3 10000 4 1000 2 100 8 10 8 plus 5 10000000000000000000 9 1000000000000000000 2 100000000000000000 1 10000000000000000 7 1000000000000000 4 100000000000000 1 10000000000000 8 1000000000000 3 100000000000 6 10000000000 4 1000000000 3 100000000 2 10000000 7 1000000 7 100000 6 10000 7 1000 4 100 0 10 4?\n",
            "Sample correct answer: 9 10000000000000000000 6 1000000000000000000 1 100000000000000000 1 10000000000000000 8 1000000000000000 3 100000000000000 9 10000000000000 4 1000000000000 7 100000000000 8 10000000000 6 1000000000 5 100000000 1 10000000 3 1000000 3 100000 0 10000 1 1000 6 100 9 10 2\n",
            "Sample predicted answer: 9 10000000000000000000 6 1000000000000000000 1 100000000000000000 1 10000000000000000 8 1000000000000000 3 100000000000000 9 10000000000000 4 1000000000000 7 100000000000 8 10000000000 6 1000000000 5 100000000 1 10000000 3 1000000 3 100000 0 10000 1 1000 6 100 9 10 2\n",
            "Exact match: True\n",
            "Max answer length: 91, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 10000000000000000000 3 1000000000000000000 6 100000000000000000 5 10000000000000000 2 1000000000000000 2 100000000000000 3 10000000000000 8 1000000000000 0 100000000000 9 10000000000 0 1000000000 6 100000000 4 10000000 2 1000000 2 100000 3 10000 9 1000 5 100 5 10 9 plus 3 10000000000000000000 2 1000000000000000000 5 100000000000000000 1 10000000000000000 6 1000000000000000 7 100000000000000 6 10000000000000 0 1000000000000 2 100000000000 5 10000000000 6 1000000000 0 100000000 1 10000000 9 1000000 5 100000 1 10000 4 1000 7 100 3 10 4?\n",
            "Sample correct answer: 4 10000000000000000000 6 1000000000000000000 1 100000000000000000 6 10000000000000000 8 1000000000000000 9 100000000000000 9 10000000000000 8 1000000000000 3 100000000000 4 10000000000 6 1000000000 6 100000000 6 10000000 1 1000000 7 100000 5 10000 4 1000 2 100 9 10 3\n",
            "Sample predicted answer: 4 10000000000000000000 6 1000000000000000000 1 100000000000000000 6 10000000000000000 8 1000000000000000 9 100000000000000 9 10000000000000 8 1000000000000 3 100000000000 4 10000000000 6 1000000000 6 100000000 6 10000000 1 1000000 7 100000 5 10000 4 1000 2 100 9 10 3\n",
            "Exact match: True\n",
            "Max answer length: 92, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 6 10000000000000000000 2 1000000000000000000 4 100000000000000000 3 10000000000000000 2 1000000000000000 6 100000000000000 8 10000000000000 2 1000000000000 1 100000000000 3 10000000000 0 1000000000 6 100000000 0 10000000 8 1000000 6 100000 9 10000 7 1000 9 100 3 10 8 plus 7 10000000000000000000 9 1000000000000000000 7 100000000000000000 1 10000000000000000 8 1000000000000000 0 100000000000000 9 10000000000000 8 1000000000000 2 100000000000 5 10000000000 3 1000000000 3 100000000 4 10000000 2 1000000 8 100000 6 10000 4 1000 9 100 4 10 7?\n",
            "Sample correct answer: 1 100000000000000000000 4 10000000000000000000 2 1000000000000000000 1 100000000000000000 5 10000000000000000 0 1000000000000000 7 100000000000000 8 10000000000000 0 1000000000000 3 100000000000 8 10000000000 3 1000000000 9 100000000 5 10000000 1 1000000 5 100000 6 10000 2 1000 8 100 8 10 5\n",
            "Sample predicted answer: 1 100000000000000000000 4 10000000000000000000 2 1000000000000000000 1 100000000000000000 5 10000000000000000 0 1000000000000000 7 100000000000000 8 10000000000000 0 1000000000000 3 100000000000 8 10000000000 3 1000000000 9 100000000 5 10000000 1 1000000 5 100000 6 10000 2 1000 8 100 8 10 5\n",
            "Exact match: True\n",
            "Max answer length: 92, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8 10000000000000000000 9 1000000000000000000 8 100000000000000000 7 10000000000000000 2 1000000000000000 1 100000000000000 5 10000000000000 5 1000000000000 8 100000000000 0 10000000000 5 1000000000 6 100000000 8 10000000 9 1000000 3 100000 2 10000 7 1000 5 100 4 10 1 plus 4 10000000000000000000 3 1000000000000000000 6 100000000000000000 4 10000000000000000 1 1000000000000000 1 100000000000000 7 10000000000000 8 1000000000000 6 100000000000 0 10000000000 8 1000000000 3 100000000 1 10000000 2 1000000 8 100000 1 10000 8 1000 0 100 6 10 6?\n",
            "Sample correct answer: 1 100000000000000000000 3 10000000000000000000 3 1000000000000000000 5 100000000000000000 1 10000000000000000 3 1000000000000000 3 100000000000000 3 10000000000000 4 1000000000000 4 100000000000 1 10000000000 4 1000000000 0 100000000 0 10000000 2 1000000 1 100000 4 10000 5 1000 6 100 0 10 7\n",
            "Sample predicted answer: 1 100000000000000000000 3 10000000000000000000 3 1000000000000000000 5 100000000000000000 1 10000000000000000 3 1000000000000000 3 100000000000000 3 10000000000000 4 1000000000000 4 100000000000 1 10000000000 4 1000000000 0 100000000 0 10000000 2 1000000 1 100000 4 10000 5 1000 6 100 0 10 7\n",
            "Exact match: True\n",
            "Max answer length: 91, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:27<00:00,  1.74s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8400\n",
            "Testing DataLoader 0: 100% 16/16 [00:27<00:00,  1.74s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8399999737739563    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8 1000000000000000000000000 2 100000000000000000000000 8 10000000000000000000000 7 1000000000000000000000 1 100000000000000000000 1 10000000000000000000 8 1000000000000000000 3 100000000000000000 2 10000000000000000 5 1000000000000000 1 100000000000000 0 10000000000000 1 1000000000000 8 100000000000 4 10000000000 9 1000000000 9 100000000 4 10000000 4 1000000 5 100000 4 10000 2 1000 2 100 1 10 4 plus 7 1000000000000000000000000 7 100000000000000000000000 2 10000000000000000000000 5 1000000000000000000000 5 100000000000000000000 2 10000000000000000000 5 1000000000000000000 6 100000000000000000 2 10000000000000000 9 1000000000000000 0 100000000000000 9 10000000000000 6 1000000000000 1 100000000000 7 10000000000 6 1000000000 6 100000000 6 10000000 4 1000000 4 100000 2 10000 6 1000 6 100 1 10 3?\n",
            "Sample correct answer: 1 10000000000000000000000000 6 1000000000000000000000000 0 100000000000000000000000 1 10000000000000000000000 2 1000000000000000000000 6 100000000000000000000 4 10000000000000000000 3 1000000000000000000 9 100000000000000000 5 10000000000000000 4 1000000000000000 1 100000000000000 9 10000000000000 8 1000000000000 0 100000000000 2 10000000000 6 1000000000 6 100000000 0 10000000 8 1000000 9 100000 6 10000 8 1000 8 100 2 10 7\n",
            "Sample predicted answer: 1 10000000000000000000000000 6 1000000000000000000000000 0 100000000000000000000000 1 10000000000000000000000 2 1000000000000000000000 6 100000000000000000000 4 10000000000000000000 3 1000000000000000000 9 100000000000000000 5 10000000000000000 4 1000000000000000 1 100000000000000 9 10000000000000 8 1000000000000 0 100000000000 2 10000000000 6 1000000000 6 100000000 0 10000000 8 1000000 9 100000 6 10000 8 1000 8 100 2 10 7\n",
            "Exact match: True\n",
            "Max answer length: 130, Max generation length: 140\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2 1000000000000000000000000 6 100000000000000000000000 7 10000000000000000000000 6 1000000000000000000000 5 100000000000000000000 3 10000000000000000000 1 1000000000000000000 8 100000000000000000 1 10000000000000000 7 1000000000000000 7 100000000000000 7 10000000000000 0 1000000000000 3 100000000000 4 10000000000 6 1000000000 8 100000000 0 10000000 8 1000000 3 100000 0 10000 6 1000 3 100 1 10 1 plus 7 1000000000000000000000000 4 100000000000000000000000 0 10000000000000000000000 8 1000000000000000000000 9 100000000000000000000 7 10000000000000000000 3 1000000000000000000 0 100000000000000000 0 10000000000000000 2 1000000000000000 5 100000000000000 3 10000000000000 0 1000000000000 1 100000000000 5 10000000000 9 1000000000 7 100000000 5 10000000 6 1000000 8 100000 9 10000 1 1000 7 100 9 10 8?\n",
            "Sample correct answer: 1 10000000000000000000000000 0 1000000000000000000000000 0 100000000000000000000000 8 10000000000000000000000 5 1000000000000000000000 5 100000000000000000000 0 10000000000000000000 4 1000000000000000000 8 100000000000000000 2 10000000000000000 0 1000000000000000 3 100000000000000 0 10000000000000 0 1000000000000 5 100000000000 0 10000000000 6 1000000000 5 100000000 6 10000000 5 1000000 1 100000 9 10000 8 1000 1 100 0 10 9\n",
            "Sample predicted answer: 1 10000000000000000000000000 0 1000000000000000000000000 0 100000000000000000000000 8 10000000000000000000000 5 1000000000000000000000 5 100000000000000000000 0 10000000000000000000 4 1000000000000000000 8 100000000000000000 2 10000000000000000 0 1000000000000000 3 100000000000000 0 10000000000000 0 1000000000000 5 100000000000 0 10000000000 6 1000000000 5 100000000 6 10000000 5 1000000 1 100000 9 10000 8 1000 1 100 0 10 9\n",
            "Exact match: True\n",
            "Max answer length: 132, Max generation length: 142\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1 1000000000000000000000000 9 100000000000000000000000 4 10000000000000000000000 1 1000000000000000000000 3 100000000000000000000 3 10000000000000000000 2 1000000000000000000 9 100000000000000000 4 10000000000000000 2 1000000000000000 7 100000000000000 1 10000000000000 2 1000000000000 3 100000000000 0 10000000000 7 1000000000 3 100000000 3 10000000 6 1000000 4 100000 5 10000 2 1000 8 100 1 10 9 plus 8 1000000000000000000000000 5 100000000000000000000000 1 10000000000000000000000 6 1000000000000000000000 7 100000000000000000000 6 10000000000000000000 6 1000000000000000000 6 100000000000000000 4 10000000000000000 6 1000000000000000 5 100000000000000 9 10000000000000 3 1000000000000 2 100000000000 3 10000000000 9 1000000000 9 100000000 9 10000000 5 1000000 0 100000 5 10000 2 1000 2 100 1 10 5?\n",
            "Sample correct answer: 1 10000000000000000000000000 0 1000000000000000000000000 4 100000000000000000000000 5 10000000000000000000000 8 1000000000000000000000 0 100000000000000000000 9 10000000000000000000 9 1000000000000000000 5 100000000000000000 8 10000000000000000 9 1000000000000000 3 100000000000000 0 10000000000000 5 1000000000000 5 100000000000 4 10000000000 7 1000000000 3 100000000 3 10000000 1 1000000 5 100000 0 10000 5 1000 0 100 3 10 4\n",
            "Sample predicted answer: 1 10000000000000000000000000 0 1000000000000000000000000 4 100000000000000000000000 5 10000000000000000000000 8 1000000000000000000000 0 100000000000000000000 9 10000000000000000000 9 1000000000000000000 5 100000000000000000 8 10000000000000000 9 1000000000000000 3 100000000000000 0 10000000000000 5 1000000000000 5 100000000000 4 10000000000 7 1000000000 3 100000000 3 10000000 1 1000000 5 100000 0 10000 5 1000 0 100 3 10 4\n",
            "Exact match: True\n",
            "Max answer length: 130, Max generation length: 140\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8 1000000000000000000000000 7 100000000000000000000000 0 10000000000000000000000 3 1000000000000000000000 7 100000000000000000000 6 10000000000000000000 5 1000000000000000000 5 100000000000000000 1 10000000000000000 7 1000000000000000 8 100000000000000 8 10000000000000 9 1000000000000 6 100000000000 3 10000000000 4 1000000000 9 100000000 6 10000000 9 1000000 8 100000 6 10000 5 1000 5 100 4 10 4 plus 3 1000000000000000000000000 5 100000000000000000000000 1 10000000000000000000000 9 1000000000000000000000 6 100000000000000000000 8 10000000000000000000 6 1000000000000000000 5 100000000000000000 0 10000000000000000 5 1000000000000000 4 100000000000000 8 10000000000000 9 1000000000000 6 100000000000 5 10000000000 1 1000000000 2 100000000 5 10000000 0 1000000 8 100000 5 10000 8 1000 2 100 1 10 2?\n",
            "Sample correct answer: 1 10000000000000000000000000 2 1000000000000000000000000 2 100000000000000000000000 2 10000000000000000000000 3 1000000000000000000000 4 100000000000000000000 5 10000000000000000000 2 1000000000000000000 0 100000000000000000 2 10000000000000000 3 1000000000000000 3 100000000000000 7 10000000000000 9 1000000000000 2 100000000000 8 10000000000 6 1000000000 2 100000000 2 10000000 0 1000000 7 100000 2 10000 3 1000 7 100 5 10 6\n",
            "Sample predicted answer: 1 10000000000000000000000000 2 1000000000000000000000000 2 100000000000000000000000 2 10000000000000000000000 3 1000000000000000000000 4 100000000000000000000 5 10000000000000000000 2 1000000000000000000 0 100000000000000000 2 10000000000000000 3 1000000000000000 3 100000000000000 7 10000000000000 9 1000000000000 2 100000000000 8 10000000000 6 1000000000 2 100000000 2 10000000 0 1000000 7 100000 2 10000 3 1000 7 100 5 10 6\n",
            "Exact match: True\n",
            "Max answer length: 129, Max generation length: 139\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 2 100000000000000000000000 3 10000000000000000000000 9 1000000000000000000000 1 100000000000000000000 5 10000000000000000000 9 1000000000000000000 7 100000000000000000 0 10000000000000000 2 1000000000000000 7 100000000000000 8 10000000000000 0 1000000000000 1 100000000000 3 10000000000 9 1000000000 0 100000000 0 10000000 0 1000000 2 100000 4 10000 2 1000 0 100 2 10 9 plus 8 1000000000000000000000000 0 100000000000000000000000 1 10000000000000000000000 6 1000000000000000000000 4 100000000000000000000 7 10000000000000000000 7 1000000000000000000 4 100000000000000000 3 10000000000000000 4 1000000000000000 6 100000000000000 9 10000000000000 4 1000000000000 5 100000000000 3 10000000000 1 1000000000 5 100000000 7 10000000 0 1000000 8 100000 9 10000 2 1000 8 100 9 10 7?\n",
            "Sample correct answer: 8 1000000000000000000000000 2 100000000000000000000000 5 10000000000000000000000 5 1000000000000000000000 6 100000000000000000000 3 10000000000000000000 7 1000000000000000000 1 100000000000000000 3 10000000000000000 7 1000000000000000 4 100000000000000 7 10000000000000 4 1000000000000 6 100000000000 7 10000000000 0 1000000000 5 100000000 7 10000000 1 1000000 1 100000 3 10000 4 1000 9 100 2 10 6\n",
            "Sample predicted answer: 1 10000000000000000000000000 0 1000000000000000000000000 0 100000000000000000000000 5 10000000000000000000000 5 1000000000000000000000 6 100000000000000000000 3 10000000000000000000 7 1000000000000000000 1 100000000000000000 3 10000000000000000 7 1000000000000000 4 100000000000000 7 10000000000000 4 1000000000000 6 100000000000 7 10000000000 0 1000000000 5 100000000 7 10000000 1 1000000 1 100000 3 10000 4 1000 9 100 2 10 6\n",
            "Exact match: False\n",
            "Max answer length: 130, Max generation length: 140\n",
            "Testing DataLoader 0: 100% 16/16 [00:39<00:00,  2.48s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8330\n",
            "Testing DataLoader 0: 100% 16/16 [00:39<00:00,  2.48s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8330000042915344    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing 10based model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/10based_trained_on_30_digits_seed42/model-epoch=19-val_exact_match=0.9940.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 4 100000000000000000000000000000 4 10000000000000000000000000000 3 1000000000000000000000000000 5 100000000000000000000000000 9 10000000000000000000000000 4 1000000000000000000000000 0 100000000000000000000000 9 10000000000000000000000 1 1000000000000000000000 6 100000000000000000000 2 10000000000000000000 3 1000000000000000000 5 100000000000000000 7 10000000000000000 2 1000000000000000 7 100000000000000 1 10000000000000 0 1000000000000 8 100000000000 7 10000000000 8 1000000000 1 100000000 2 10000000 9 1000000 3 100000 1 10000 6 1000 8 100 2 10 9 plus 1 10000000000000000000000000000 8 1000000000000000000000000000 9 100000000000000000000000000 7 10000000000000000000000000 1 1000000000000000000000000 3 100000000000000000000000 6 10000000000000000000000 6 1000000000000000000000 9 100000000000000000000 5 10000000000000000000 3 1000000000000000000 6 100000000000000000 3 10000000000000000 4 1000000000000000 5 100000000000000 4 10000000000000 7 1000000000000 6 100000000000 3 10000000000 9 1000000000 8 100000000 0 10000000 2 1000000 9 100000 7 10000 2 1000 9 100 7 10 3?\n",
            "Sample correct answer: 4 100000000000000000000000000000 6 10000000000000000000000000000 2 1000000000000000000000000000 5 100000000000000000000000000 6 10000000000000000000000000 5 1000000000000000000000000 4 100000000000000000000000 5 10000000000000000000000 8 1000000000000000000000 5 100000000000000000000 7 10000000000000000000 7 1000000000000000000 2 100000000000000000 0 10000000000000000 7 1000000000000000 2 100000000000000 5 10000000000000 8 1000000000000 5 100000000000 1 10000000000 7 1000000000 9 100000000 3 10000000 2 1000000 2 100000 8 10000 9 1000 8 100 0 10 2\n",
            "Sample predicted answer: 6 100000000000000000000000000000 3 10000000000000000000000000000 2 1000000000000000000000000000 5 100000000000000000000000000 6 10000000000000000000000000 5 1000000000000000000000000 4 100000000000000000000000 5 10000000000000000000000 8 1000000000000000000000 5 100000000000000000000 7 10000000000000000000 7 1000000000000000000 2 100000000000000000 0 10000000000000000 7 1000000000000000 2 100000000000000 5 10000000000000 8 1000000000000 5 100000000000 1 10000000000 7 1000000000 9 100000000 3 10000000 2 1000000 2 100000 8 10000 9 1000 8 100 0 10 2\n",
            "Exact match: False\n",
            "Max answer length: 175, Max generation length: 185\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 9 10000000000000000000000000000 6 1000000000000000000000000000 1 100000000000000000000000000 7 10000000000000000000000000 9 1000000000000000000000000 2 100000000000000000000000 5 10000000000000000000000 6 1000000000000000000000 0 100000000000000000000 2 10000000000000000000 8 1000000000000000000 4 100000000000000000 8 10000000000000000 9 1000000000000000 9 100000000000000 7 10000000000000 7 1000000000000 6 100000000000 1 10000000000 1 1000000000 4 100000000 3 10000000 0 1000000 0 100000 1 10000 8 1000 7 100 8 10 6 plus 6 100000000000000000000000000000 6 10000000000000000000000000000 0 1000000000000000000000000000 0 100000000000000000000000000 4 10000000000000000000000000 0 1000000000000000000000000 4 100000000000000000000000 2 10000000000000000000000 4 1000000000000000000000 4 100000000000000000000 6 10000000000000000000 0 1000000000000000000 3 100000000000000000 3 10000000000000000 7 1000000000000000 1 100000000000000 2 10000000000000 1 1000000000000 3 100000000000 0 10000000000 1 1000000000 5 100000000 5 10000000 6 1000000 9 100000 3 10000 8 1000 8 100 3 10 6?\n",
            "Sample correct answer: 7 100000000000000000000000000000 5 10000000000000000000000000000 6 1000000000000000000000000000 2 100000000000000000000000000 1 10000000000000000000000000 9 1000000000000000000000000 6 100000000000000000000000 8 10000000000000000000000 0 1000000000000000000000 4 100000000000000000000 8 10000000000000000000 8 1000000000000000000 8 100000000000000000 2 10000000000000000 7 1000000000000000 0 100000000000000 9 10000000000000 8 1000000000000 9 100000000000 1 10000000000 2 1000000000 9 100000000 8 10000000 6 1000000 9 100000 5 10000 7 1000 6 100 2 10 2\n",
            "Sample predicted answer: 1 1000000000000000000000000000000 5 100000000000000000000000000000 5 10000000000000000000000000000 6 1000000000000000000000000000 2 100000000000000000000000000 1 10000000000000000000000000 9 1000000000000000000000000 6 100000000000000000000000 8 10000000000000000000000 0 1000000000000000000000 4 100000000000000000000 8 10000000000000000000 8 1000000000000000000 8 100000000000000000 2 10000000000000000 7 1000000000000000 0 100000000000000 9 10000000000000 8 1000000000000 9 100000000000 1 10000000000 2 1000000000 9 100000000 8 10000000 6 1000000 9 100000 5 10000 7 1000 6 100 2 10 2\n",
            "Exact match: False\n",
            "Max answer length: 172, Max generation length: 182\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 6 100000000000000000000000000000 5 10000000000000000000000000000 6 1000000000000000000000000000 9 100000000000000000000000000 4 10000000000000000000000000 8 1000000000000000000000000 9 100000000000000000000000 5 10000000000000000000000 5 1000000000000000000000 2 100000000000000000000 1 10000000000000000000 3 1000000000000000000 2 100000000000000000 9 10000000000000000 0 1000000000000000 6 100000000000000 4 10000000000000 0 1000000000000 6 100000000000 4 10000000000 5 1000000000 0 100000000 5 10000000 3 1000000 1 100000 1 10000 0 1000 7 100 4 10 8 plus 2 100000000000000000000000000000 7 10000000000000000000000000000 5 1000000000000000000000000000 0 100000000000000000000000000 9 10000000000000000000000000 7 1000000000000000000000000 5 100000000000000000000000 9 10000000000000000000000 3 1000000000000000000000 2 100000000000000000000 4 10000000000000000000 3 1000000000000000000 0 100000000000000000 0 10000000000000000 9 1000000000000000 8 100000000000000 3 10000000000000 9 1000000000000 0 100000000000 8 10000000000 5 1000000000 7 100000000 9 10000000 9 1000000 1 100000 5 10000 1 1000 5 100 1 10 4?\n",
            "Sample correct answer: 9 100000000000000000000000000000 3 10000000000000000000000000000 2 1000000000000000000000000000 0 100000000000000000000000000 4 10000000000000000000000000 6 1000000000000000000000000 5 100000000000000000000000 4 10000000000000000000000 8 1000000000000000000000 4 100000000000000000000 5 10000000000000000000 6 1000000000000000000 3 100000000000000000 0 10000000000000000 0 1000000000000000 4 100000000000000 7 10000000000000 9 1000000000000 7 100000000000 3 10000000000 0 1000000000 8 100000000 5 10000000 2 1000000 2 100000 6 10000 2 1000 2 100 6 10 2\n",
            "Sample predicted answer: 9 100000000000000000000000000000 3 10000000000000000000000000000 2 1000000000000000000000000000 0 100000000000000000000000000 4 10000000000000000000000000 6 1000000000000000000000000 5 100000000000000000000000 4 10000000000000000000000 8 1000000000000000000000 4 100000000000000000000 5 10000000000000000000 6 1000000000000000000 3 100000000000000000 0 10000000000000000 0 1000000000000000 4 100000000000000 7 10000000000000 9 1000000000000 7 100000000000 3 10000000000 0 1000000000 8 100000000 5 10000000 2 1000000 2 100000 6 10000 2 1000 2 100 6 10 2\n",
            "Exact match: True\n",
            "Max answer length: 174, Max generation length: 184\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 1 10000000000000000000000000000 0 1000000000000000000000000000 5 100000000000000000000000000 6 10000000000000000000000000 9 1000000000000000000000000 7 100000000000000000000000 9 10000000000000000000000 7 1000000000000000000000 2 100000000000000000000 1 10000000000000000000 8 1000000000000000000 1 100000000000000000 5 10000000000000000 0 1000000000000000 8 100000000000000 3 10000000000000 0 1000000000000 9 100000000000 7 10000000000 1 1000000000 6 100000000 3 10000000 4 1000000 9 100000 5 10000 5 1000 7 100 2 10 2 plus 9 100000000000000000000000000000 4 10000000000000000000000000000 9 1000000000000000000000000000 2 100000000000000000000000000 1 10000000000000000000000000 8 1000000000000000000000000 2 100000000000000000000000 8 10000000000000000000000 9 1000000000000000000000 5 100000000000000000000 2 10000000000000000000 8 1000000000000000000 0 100000000000000000 5 10000000000000000 5 1000000000000000 6 100000000000000 3 10000000000000 0 1000000000000 4 100000000000 7 10000000000 3 1000000000 6 100000000 4 10000000 6 1000000 7 100000 9 10000 1 1000 4 100 9 10 7?\n",
            "Sample correct answer: 9 100000000000000000000000000000 5 10000000000000000000000000000 9 1000000000000000000000000000 7 100000000000000000000000000 8 10000000000000000000000000 8 1000000000000000000000000 0 100000000000000000000000 8 10000000000000000000000 6 1000000000000000000000 7 100000000000000000000 4 10000000000000000000 6 1000000000000000000 2 100000000000000000 0 10000000000000000 6 1000000000000000 4 100000000000000 6 10000000000000 1 1000000000000 4 100000000000 4 10000000000 5 1000000000 2 100000000 8 10000000 1 1000000 7 100000 4 10000 7 1000 2 100 1 10 9\n",
            "Sample predicted answer: 1 1000000000000000000000000000000 0 100000000000000000000000000000 2 10000000000000000000000000000 9 1000000000000000000000000000 7 100000000000000000000000000 8 10000000000000000000000000 8 1000000000000000000000000 0 100000000000000000000000 8 10000000000000000000000 6 1000000000000000000000 7 100000000000000000000 4 10000000000000000000 6 1000000000000000000 2 100000000000000000 0 10000000000000000 6 1000000000000000 4 100000000000000 6 10000000000000 1 1000000000000 4 100000000000 4 10000000000 5 1000000000 2 100000000 8 10000000 1 1000000 7 100000 4 10000 7 1000 2 100 1 10 9\n",
            "Exact match: False\n",
            "Max answer length: 174, Max generation length: 184\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 7 100000000000000000000000000000 7 10000000000000000000000000000 1 1000000000000000000000000000 6 100000000000000000000000000 4 10000000000000000000000000 8 1000000000000000000000000 7 100000000000000000000000 0 10000000000000000000000 8 1000000000000000000000 8 100000000000000000000 0 10000000000000000000 8 1000000000000000000 7 100000000000000000 4 10000000000000000 7 1000000000000000 2 100000000000000 6 10000000000000 4 1000000000000 2 100000000000 1 10000000000 2 1000000000 2 100000000 1 10000000 9 1000000 5 100000 5 10000 7 1000 6 100 5 10 5 plus 7 100000000000000000000000000000 4 10000000000000000000000000000 2 1000000000000000000000000000 2 100000000000000000000000000 3 10000000000000000000000000 1 1000000000000000000000000 1 100000000000000000000000 0 10000000000000000000000 5 1000000000000000000000 1 100000000000000000000 4 10000000000000000000 9 1000000000000000000 3 100000000000000000 8 10000000000000000 3 1000000000000000 5 100000000000000 6 10000000000000 0 1000000000000 8 100000000000 7 10000000000 2 1000000000 3 100000000 5 10000000 1 1000000 8 100000 9 10000 0 1000 4 100 6 10 1?\n",
            "Sample correct answer: 1 1000000000000000000000000000000 5 100000000000000000000000000000 1 10000000000000000000000000000 3 1000000000000000000000000000 8 100000000000000000000000000 7 10000000000000000000000000 9 1000000000000000000000000 8 100000000000000000000000 1 10000000000000000000000 3 1000000000000000000000 9 100000000000000000000 5 10000000000000000000 8 1000000000000000000 1 100000000000000000 3 10000000000000000 0 1000000000000000 8 100000000000000 2 10000000000000 5 1000000000000 0 100000000000 8 10000000000 4 1000000000 5 100000000 7 10000000 1 1000000 4 100000 4 10000 8 1000 1 100 1 10 6\n",
            "Sample predicted answer: 1 1000000000000000000000000000000 5 100000000000000000000000000000 1 10000000000000000000000000000 3 1000000000000000000000000000 8 100000000000000000000000000 7 10000000000000000000000000 9 1000000000000000000000000 8 100000000000000000000000 1 10000000000000000000000 3 1000000000000000000000 9 100000000000000000000 5 10000000000000000000 8 1000000000000000000 1 100000000000000000 3 10000000000000000 0 1000000000000000 8 100000000000000 2 10000000000000 5 1000000000000 0 100000000000 8 10000000000 4 1000000000 5 100000000 7 10000000 1 1000000 4 100000 4 10000 8 1000 1 100 1 10 6\n",
            "Exact match: True\n",
            "Max answer length: 173, Max generation length: 183\n",
            "Testing DataLoader 0: 100% 16/16 [00:52<00:00,  3.28s/it]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.8100\n",
            "Testing DataLoader 0: 100% 16/16 [00:52<00:00,  3.28s/it]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8100000023841858    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 2 digits\n",
            "Testing model on 2 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 2 to 2\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 35 plus 58?\n",
            "Sample correct answer: 93\n",
            "Sample predicted answer: 88\n",
            "Exact match: False\n",
            "Max answer length: 3, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 14 plus 46?\n",
            "Sample correct answer: 60\n",
            "Sample predicted answer: 57\n",
            "Exact match: False\n",
            "Max answer length: 3, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 49 plus 33?\n",
            "Sample correct answer: 82\n",
            "Sample predicted answer: 82\n",
            "Exact match: True\n",
            "Max answer length: 3, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 85 plus 52?\n",
            "Sample correct answer: 137\n",
            "Sample predicted answer: 139\n",
            "Exact match: False\n",
            "Max answer length: 3, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 69 plus 28?\n",
            "Sample correct answer: 97\n",
            "Sample predicted answer: 98\n",
            "Exact match: False\n",
            "Max answer length: 3, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:01<00:00, 12.97it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0970\n",
            "Testing DataLoader 0: 100% 16/16 [00:01<00:00, 12.89it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.09700000286102295   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 5 digits\n",
            "Testing model on 5 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 5 to 5\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 36434 plus 59429?\n",
            "Sample correct answer: 95863\n",
            "Sample predicted answer: 96454\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 7592 plus 31571?\n",
            "Sample correct answer: 39163\n",
            "Sample predicted answer: 106157\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 91303 plus 95748?\n",
            "Sample correct answer: 187051\n",
            "Sample predicted answer: 185757\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 25144 plus 55108?\n",
            "Sample correct answer: 80252\n",
            "Sample predicted answer: 76145\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 65597 plus 70776?\n",
            "Sample correct answer: 136373\n",
            "Sample predicted answer: 1385676\n",
            "Exact match: False\n",
            "Max answer length: 4, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:01<00:00,  8.78it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0030\n",
            "Testing DataLoader 0: 100% 16/16 [00:01<00:00,  8.74it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.003000000026077032   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 10 digits\n",
            "Testing model on 10 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 10 to 10\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 9184064900 plus 2119634399?\n",
            "Sample correct answer: 11303699299\n",
            "Sample predicted answer: 112262634999\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 1845919895 plus 4474620894?\n",
            "Sample correct answer: 6320540789\n",
            "Sample predicted answer: 5747471993\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 3085320828 plus 1335901990?\n",
            "Sample correct answer: 4421222818\n",
            "Sample predicted answer: 43485292828\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 5228133486 plus 1557724698?\n",
            "Sample correct answer: 6785858184\n",
            "Sample predicted answer: 68484293585\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 8215400091 plus 3545940090?\n",
            "Sample correct answer: 11761340181\n",
            "Sample predicted answer: 12565959900\n",
            "Exact match: False\n",
            "Max answer length: 7, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  6.55it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:02<00:00,  6.53it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 15 digits\n",
            "Testing model on 15 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 15 to 15\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 62980438355513 plus 925265788331257?\n",
            "Sample correct answer: 988246226686770\n",
            "Sample predicted answer: 156292968383968\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 185362450397904 plus 2428284199395?\n",
            "Sample correct answer: 187790734597299\n",
            "Sample predicted answer: 498457645453997\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 31078122379385 plus 987456426412390?\n",
            "Sample correct answer: 1018534548791775\n",
            "Sample predicted answer: 1270494568683585\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 115502368831984 plus 13812068159379?\n",
            "Sample correct answer: 129314436991363\n",
            "Sample predicted answer: 3845968683535\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 166287454389246 plus 115746088426885?\n",
            "Sample correct answer: 282033542816131\n",
            "Sample predicted answer: 356296835696934\n",
            "Exact match: False\n",
            "Max answer length: 9, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.83it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:03<00:00,  4.82it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 20 digits\n",
            "Testing model on 20 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 20 to 20\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 50169253275960337901 plus 29680261717529342420?\n",
            "Sample correct answer: 79849514993489680321\n",
            "Sample predicted answer: 7949535272768683593\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 36900976422185534288 plus 59217418364327767404?\n",
            "Sample correct answer: 96118394786513301692\n",
            "Sample predicted answer: 970707171717171717\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 13652238090642239559 plus 32516760256019514734?\n",
            "Sample correct answer: 46168998346661754293\n",
            "Sample predicted answer: 525272780808021919\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 62432682130608697938 plus 79718098253342864947?\n",
            "Sample correct answer: 142150780383951562885\n",
            "Sample predicted answer: 146252727272735353593\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 89872155805689327541 plus 43641178608312818066?\n",
            "Sample correct answer: 133513334414002145607\n",
            "Sample predicted answer: 1345869080806969352\n",
            "Exact match: False\n",
            "Max answer length: 12, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.78it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.78it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 25 digits\n",
            "Testing model on 25 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 25 to 25\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 8287118325101849944542214 plus 7725525629096176664426613?\n",
            "Sample correct answer: 16012643954198026608968827\n",
            "Sample predicted answer: 1622727272727276868686868\n",
            "Exact match: False\n",
            "Max answer length: 14, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 2676531817770346808306311 plus 7408973002530159756891798?\n",
            "Sample correct answer: 10085504820300506565198109\n",
            "Sample predicted answer: 10247635666666808029519\n",
            "Exact match: False\n",
            "Max answer length: 14, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 1941332942712307336452819 plus 8516766646593239995052215?\n",
            "Sample correct answer: 10458099589305547331505034\n",
            "Sample predicted answer: 103069808080806969696980\n",
            "Exact match: False\n",
            "Max answer length: 14, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 8703765517889634969865544 plus 3519686505489651250858212?\n",
            "Sample correct answer: 12223452023379286220723756\n",
            "Sample predicted answer: 12458635359890696969802\n",
            "Exact match: False\n",
            "Max answer length: 14, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 239159702780139000242029 plus 8016477434694531570892897?\n",
            "Sample correct answer: 8255637137474670571134926\n",
            "Sample predicted answer: 10249880808080808080220\n",
            "Exact match: False\n",
            "Max answer length: 14, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.27it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:04<00:00,  3.27it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "\n",
            "Testing decimal model trained on 30 digits (seed 42) on 30 digits\n",
            "Testing model on 30 digits (balanced=False)\n",
            "Using checkpoint: ./generalized_results/decimal_trained_on_30_digits_seed42/model-epoch=17-val_exact_match=0.0120.ckpt\n",
            "Seed set to 42\n",
            "Creating test dataset with digits 30 to 30\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Testing DataLoader 0:   0% 0/16 [00:00<?, ?it/s]\n",
            "=== Validation Batch 0 ===\n",
            "\n",
            "Sample question: What is 443594091623572710878129316829 plus 18971366953634547639802972973?\n",
            "Sample correct answer: 462565458577207258517932289802\n",
            "Sample predicted answer: 6343434353567670707171717\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 1 ===\n",
            "\n",
            "Sample question: What is 96179256028489977611430018786 plus 660040424460337121301556938836?\n",
            "Sample correct answer: 756219680488827098912986957622\n",
            "Sample predicted answer: 1632980525252595959595951919\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 2 ===\n",
            "\n",
            "Sample question: What is 656948955213290640645053110748 plus 275097593243009839085799151514?\n",
            "Sample correct answer: 932046548456300479730852262262\n",
            "Sample predicted answer: 96970745764566359595959595\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 4 ===\n",
            "\n",
            "Sample question: What is 10569797218150830971634955722 plus 949218289528055630473646791497?\n",
            "Sample correct answer: 959788086746206461445281747219\n",
            "Sample predicted answer: 1084576767680808080228282828\n",
            "Exact match: False\n",
            "Max answer length: 16, Max generation length: 128\n",
            "\n",
            "=== Validation Batch 8 ===\n",
            "\n",
            "Sample question: What is 771648708808747264212219557655 plus 742231105149383560872351890461?\n",
            "Sample correct answer: 1513879813958130825084571448116\n",
            "Sample predicted answer: 153080828283539393939393939\n",
            "Exact match: False\n",
            "Max answer length: 17, Max generation length: 128\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  2.99it/s]\n",
            "=== Test Epoch End ===\n",
            "\n",
            "Test Exact Match: 0.0000\n",
            "Testing DataLoader 0: 100% 16/16 [00:05<00:00,  2.99it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m    test_exact_match     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Results saved to ./generalized_results/generalization_results.json\n",
            "Plot saved to ./generalized_results/generalization_results.png\n",
            "Figure(1000x600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLLAYeVfDCED",
        "outputId": "c45f57e2-fc44-4a4d-91f7-6d7fe9e62dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot not found. Run experiments first.\n"
          ]
        }
      ],
      "source": [
        "# Display the plot in the notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Load and display the plot\n",
        "if os.path.exists('./experiment_results/figure1_reproduction.png'):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    img = plt.imread('./experiment_results/figure1_reproduction.png')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Also download the plot\n",
        "    files.download('./experiment_results/figure1_reproduction.png')\n",
        "    files.download('./experiment_results/figure1_reproduction.pdf')\n",
        "else:\n",
        "    print(\"Plot not found. Run experiments first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fevwBwtWDCED",
        "outputId": "7476e26a-23af-42b9-99c2-c0cbf6ab2e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"10ebased\": {\n",
            "    \"2\": {\n",
            "      \"runs\": {\n",
            "        \"42\": 0.8059999942779541\n",
            "      },\n",
            "      \"mean\": 0.8059999942779541,\n",
            "      \"ci_lower\": 0.8059999942779541,\n",
            "      \"ci_upper\": 0.8059999942779541\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists('./experiment_results/experiment_results.json'):\n",
        "    with open('./experiment_results/experiment_results.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(json.dumps(results, indent=2))\n",
        "else:\n",
        "    print(\"Results not found. Run experiments first.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}